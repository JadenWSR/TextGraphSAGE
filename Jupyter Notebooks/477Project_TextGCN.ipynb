{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armancohan/cpsc477-internal/blob/main/hw3/part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH2YExQa16Gd"
      },
      "source": [
        "# CPSC 477/577 Project Spring 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk0Ng1L3puxz"
      },
      "source": [
        "## Name and NetID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtwa1W61px8Z"
      },
      "source": [
        "Group Member: Shurui Wang; Lang Ding; Weiyi You\n",
        "\n",
        "NetID: wy257"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jfNL_UxjyA-A",
        "outputId": "556cefc5-c3fe-49e7-e4e8-935445d2c925",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install klepto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BmaY5OKzqnP",
        "outputId": "7acc9c99-a2f9-471e-dcff-5c5b30c89fa0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting klepto\n",
            "  Downloading klepto-0.2.5-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.4 (from klepto)\n",
            "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Collecting dill>=0.3.8 (from klepto)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pox, dill, klepto\n",
            "Successfully installed dill-0.3.8 klepto-0.2.5 pox-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# util.py\n",
        "from collections import OrderedDict\n",
        "import datetime\n",
        "import klepto\n",
        "from os.path import dirname, abspath, join, expanduser, isfile, exists\n",
        "from os import environ, makedirs, getcwd\n",
        "import pytz\n",
        "import re\n",
        "from socket import gethostname\n",
        "\n",
        "\n",
        "\n",
        "def get_root_path():\n",
        "    return getcwd()  # 获取并返回当前工作目录\n",
        "\n",
        "def get_data_path():\n",
        "    return join(get_root_path(), 'drive/My Drive/CPSC_577_FP')\n",
        "\n",
        "\n",
        "def get_corpus_path():\n",
        "    return join(get_data_path())\n",
        "\n",
        "\n",
        "def get_save_path():\n",
        "    return join(get_root_path(), 'drive/My Drive/CPSC_577_FP/save')\n",
        "\n",
        "\n",
        "def load(filepath, print_msg=True):\n",
        "    fp = proc_filepath(filepath)\n",
        "    if isfile(fp):\n",
        "        return load_klepto(fp, print_msg)\n",
        "    elif print_msg:\n",
        "        print('Trying to load but no file {}'.format(fp))\n",
        "\n",
        "\n",
        "def load_klepto(filepath, print_msg):\n",
        "    rtn = klepto.archives.file_archive(filepath)\n",
        "    rtn.load()\n",
        "    if print_msg:\n",
        "        print('Loaded from {}'.format(filepath))\n",
        "    return rtn\n",
        "\n",
        "\n",
        "def save(obj, filepath, print_msg=True):\n",
        "    if type(obj) is not dict and type(obj) is not OrderedDict:\n",
        "        raise ValueError('Can only save a dict or OrderedDict'\n",
        "                         ' NOT {}'.format(type(obj)))\n",
        "    fp = proc_filepath(filepath, ext='.klepto')\n",
        "    create_dir_if_not_exists(dirname(filepath))\n",
        "    save_klepto(obj, fp, print_msg)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(dir):\n",
        "    if not exists(dir):\n",
        "        makedirs(dir)\n",
        "\n",
        "\n",
        "def save_klepto(dic, filepath, print_msg):\n",
        "    if print_msg:\n",
        "        print('Saving to {}'.format(filepath))\n",
        "    klepto.archives.file_archive(filepath, dict=dic).dump()\n",
        "\n",
        "\n",
        "def proc_filepath(filepath, ext='.klepto'):\n",
        "    if type(filepath) is not str:\n",
        "        raise RuntimeError('Did you pass a file path to this function?')\n",
        "    return append_ext_to_filepath(ext, filepath)\n",
        "\n",
        "\n",
        "def append_ext_to_filepath(ext, fp):\n",
        "    if not fp.endswith(ext):\n",
        "        fp += ext\n",
        "    return fp\n",
        "\n",
        "\n",
        "def parse_as_int_list(il):\n",
        "    rtn = []\n",
        "    for x in il.split('_'):\n",
        "        x = int(x)\n",
        "        rtn.append(x)\n",
        "    return rtn\n",
        "\n",
        "\n",
        "def get_user():\n",
        "    try:\n",
        "        home_user = expanduser(\"~\").split('/')[-1]\n",
        "    except:\n",
        "        home_user = 'user'\n",
        "    return home_user\n",
        "\n",
        "\n",
        "def get_host():\n",
        "    host = environ.get('HOSTNAME')\n",
        "    if host is not None:\n",
        "        return host\n",
        "    return gethostname()\n",
        "\n",
        "tstamp = None\n",
        "\n",
        "\n",
        "def get_ts():\n",
        "    global tstamp\n",
        "    if not tstamp:\n",
        "        tstamp = get_current_ts()\n",
        "    return tstamp\n",
        "\n",
        "\n",
        "def get_current_ts(zone='US/Pacific'):\n",
        "    return datetime.datetime.now(pytz.timezone(zone)).strftime(\n",
        "        '%Y-%m-%dT%H-%M-%S.%f')\n",
        "\n",
        "\n",
        "def sorted_nicely(l, reverse=False):\n",
        "    def tryint(s):\n",
        "        try:\n",
        "            return int(s)\n",
        "        except:\n",
        "            return s\n",
        "\n",
        "    def alphanum_key(s):\n",
        "        if type(s) is not str:\n",
        "            raise ValueError('{} must be a string in l: {}'.format(s, l))\n",
        "        return [tryint(c) for c in re.split('([0-9]+)', s)]\n",
        "\n",
        "    rtn = sorted(l, key=alphanum_key)\n",
        "    if reverse:\n",
        "        rtn = reversed(rtn)\n",
        "    return rtn"
      ],
      "metadata": {
        "id": "2YXqk-1BzlJk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpZbzmjV0V1z",
        "outputId": "94b19ab7-d628-4ae6-9133-0ff25008cada"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=3588737 sha256=84c136dc7ceeedb66d00467044eb487b1eaec9a15be426a6df5d5772c5d25eb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter, torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3 torch-scatter-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.py\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch_geometric.data import Data as PyGSingleGraphData\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "class TextDataset(object):\n",
        "    def __init__(self, name, sparse_graph, labels, vocab, word_id_map, docs_dict, loaded_dict, tvt='all',\n",
        "                 train_test_split=None):\n",
        "        if loaded_dict is not None:  # restore from content loaded from disk\n",
        "            self.__dict__ = loaded_dict\n",
        "            return\n",
        "        self.name = name\n",
        "        self.graph = sparse_graph\n",
        "        self.labels = labels\n",
        "        if 'twitter_asian_prejudice' in name:\n",
        "            if 'sentiment' not in name:\n",
        "                self.labels = ['discussion_of_eastasian_prejudice' if label =='counter_speech' else label for label in self.labels]\n",
        "            else:\n",
        "                if 'neutral' not in labels:\n",
        "                    sentiment_labels = []\n",
        "                    neutral_pos_labels = [\"none_of_the_above\", \"counter_speech\", \"discussion_of_eastasian_prejudice\"]\n",
        "                    for label in labels:\n",
        "                        if label in neutral_pos_labels:\n",
        "                            sentiment_labels.append(\"neutral\")\n",
        "                        else:\n",
        "                            sentiment_labels.append(\"negative\")\n",
        "                    self.labels = sentiment_labels\n",
        "        self.label_dict = {label: i for i, label in enumerate(list(set(self.labels)))}\n",
        "        self.label_inds = np.asarray([self.label_dict[label] for label in self.labels])\n",
        "        self.vocab = vocab\n",
        "        self.word_id_map = word_id_map\n",
        "        self.docs = docs_dict\n",
        "        self.node_ids = list(self.docs.keys())\n",
        "        self.tvt = tvt\n",
        "        self.train_test_split = train_test_split\n",
        "\n",
        "    def tvt_split(self, split_points, tvt_list, seed):\n",
        "        if self.train_test_split is None:\n",
        "            doc_id_chunks = self._chunk_doc_ids(split_points, seed)\n",
        "        else:\n",
        "            train_ids = []\n",
        "            test_ids = []\n",
        "            for k, v in self.train_test_split.items():\n",
        "                if v == 'test':\n",
        "                    test_ids.append(k)\n",
        "                elif v == 'train':\n",
        "                    train_ids.append(k)\n",
        "                else:\n",
        "                    raise ValueError\n",
        "            num_val = int(len(train_ids) * 0.1)\n",
        "            random.Random(seed).shuffle(train_ids)\n",
        "            val_ids = train_ids[:num_val]\n",
        "            train_ids = train_ids[num_val:]\n",
        "            doc_id_chunks = [train_ids, val_ids, test_ids]\n",
        "        sub_dataset = []\n",
        "        for i, chunk in enumerate(doc_id_chunks):\n",
        "            docs = {doc_id: self.docs[doc_id] for doc_id in chunk}\n",
        "            sub_dataset.append(TextDataset(self.name, self.graph, self.labels, self.vocab,\n",
        "                                           self.word_id_map, docs, None, tvt_list[i]))\n",
        "        return sub_dataset\n",
        "\n",
        "    def _chunk_doc_ids(self, split_points, seed):\n",
        "        ids = sorted(self.docs.keys())\n",
        "        id_chunks = self._chunk_list(ids, split_points, seed)\n",
        "        return id_chunks\n",
        "\n",
        "    def _chunk_list(self, li, split_points, seed):\n",
        "        rtn = []\n",
        "        random.Random(seed).shuffle(li)\n",
        "        left = 0\n",
        "        split_indices = [int(len(li) * sp) for sp in split_points]\n",
        "        for si in split_indices:\n",
        "            right = left + si\n",
        "            if type(right) is not int or right <= 0 or right >= len(li):\n",
        "                raise ValueError('Wrong split_points {}'.format(split_points))\n",
        "            take = li[left:right]\n",
        "            rtn.append(take)\n",
        "            left = right\n",
        "        # The last chunk is inferred.\n",
        "        rtn.append(li[left:])\n",
        "        return rtn\n",
        "\n",
        "    def init_node_feats(self, type, device):\n",
        "        if type == 'one_hot_init':\n",
        "            num_nodes = self.graph.shape[0]\n",
        "            identity = sp.identity(num_nodes)\n",
        "            ind0, ind1, values = sp.find(identity)\n",
        "            inds = np.stack((ind0, ind1), axis=0)\n",
        "            self.node_feats = torch.sparse_coo_tensor(inds, values, device=device,\n",
        "                                                      dtype=torch.float)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def get_pyg_graph(self, device):\n",
        "        if not hasattr(self, \"pyg_graph\"):\n",
        "            adj = self.graph\n",
        "            A = adj.tocoo()\n",
        "            row = torch.from_numpy(A.row).to(torch.long).to(device)\n",
        "            col = torch.from_numpy(A.col).to(torch.long).to(device)\n",
        "            edge_index = torch.stack([row, col], dim=0)\n",
        "            edge_weight = torch.from_numpy(A.data).to(torch.float).to(device)\n",
        "            if type(self.node_feats) is not torch.Tensor:\n",
        "                gx = torch.tensor(self.node_feats, dtype=torch.float32, device=device)\n",
        "            else:\n",
        "                gx = self.node_feats\n",
        "            self.pyg_graph = PyGSingleGraphData(x=gx, edge_index=edge_index, edge_attr=edge_weight, y=None)\n",
        "        return self.pyg_graph"
      ],
      "metadata": {
        "id": "lh8ejyTPzxkG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build_graph.py\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from math import log\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from os.path import join, exists\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def build_text_graph_dataset(dataset, window_size):\n",
        "    if \"small\" in dataset or \"presplit\" in dataset or 'sentiment' in dataset:\n",
        "        dataset_name = \"_\".join(dataset.split(\"_\")[:-1])\n",
        "    else:\n",
        "        dataset_name = dataset\n",
        "    clean_text_path = join(get_corpus_path(), dataset_name + '_sentences_clean.txt')\n",
        "    labels_path = join(get_corpus_path(), dataset_name + '_labels.txt')\n",
        "    labels = pd.read_csv(labels_path, header=None, sep='\\t')\n",
        "    doc_list = []\n",
        "    f = open(clean_text_path, 'rb')\n",
        "    for line in f.readlines():\n",
        "        doc_list.append(line.strip().decode())\n",
        "    f.close()\n",
        "    assert len(labels) == len(doc_list)\n",
        "    if 'presplit' not in dataset:\n",
        "        labels_list = labels.iloc[0:, 0].tolist()\n",
        "        split_dict = None\n",
        "    else:\n",
        "        labels_list = labels.iloc[0:, 2].tolist()\n",
        "        split = labels.iloc[0:, 1].tolist()\n",
        "        split_dict = {}\n",
        "        for i, v in enumerate(split):\n",
        "            split_dict[i] = v\n",
        "    if \"small\" in dataset:\n",
        "        doc_list = doc_list[:200]\n",
        "        labels_list = labels_list[:200]\n",
        "\n",
        "    word_freq = get_vocab(doc_list)\n",
        "    vocab = list(word_freq.keys())\n",
        "    if not exists(join(get_corpus_path(), dataset + '_vocab.txt')):\n",
        "        vocab_str = '\\n'.join(vocab)\n",
        "        f = open(join(get_corpus_path(), dataset + '_vocab.txt'), 'w')\n",
        "        f.write(vocab_str)\n",
        "        f.close()\n",
        "    words_in_docs, word_doc_freq = build_word_doc_edges(doc_list)\n",
        "    word_id_map = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    sparse_graph = build_edges(doc_list, word_id_map, vocab, word_doc_freq, window_size)\n",
        "    docs_dict = {i: doc for i, doc in enumerate(doc_list)}\n",
        "    return TextDataset(dataset, sparse_graph, labels_list, vocab, word_id_map, docs_dict, None,\n",
        "                       train_test_split=split_dict)\n",
        "\n",
        "\n",
        "def build_edges(doc_list, word_id_map, vocab, word_doc_freq, window_size=20):\n",
        "    # constructing all windows\n",
        "    windows = []\n",
        "    for doc_words in doc_list:\n",
        "        words = doc_words.split()\n",
        "        doc_length = len(words)\n",
        "        if doc_length <= window_size:\n",
        "            windows.append(words)\n",
        "        else:\n",
        "            for i in range(doc_length - window_size + 1):\n",
        "                window = words[i: i + window_size]\n",
        "                windows.append(window)\n",
        "    # constructing all single word frequency\n",
        "    word_window_freq = defaultdict(int)\n",
        "    for window in windows:\n",
        "        appeared = set()\n",
        "        for word in window:\n",
        "            if word not in appeared:\n",
        "                word_window_freq[word] += 1\n",
        "                appeared.add(word)\n",
        "    # constructing word pair count frequency\n",
        "    word_pair_count = defaultdict(int)\n",
        "    for window in tqdm(windows):\n",
        "        for i in range(1, len(window)):\n",
        "            for j in range(i):\n",
        "                word_i = window[i]\n",
        "                word_j = window[j]\n",
        "                word_i_id = word_id_map[word_i]\n",
        "                word_j_id = word_id_map[word_j]\n",
        "                if word_i_id == word_j_id:\n",
        "                    continue\n",
        "                word_pair_count[(word_i_id, word_j_id)] += 1\n",
        "                word_pair_count[(word_j_id, word_i_id)] += 1\n",
        "    row = []\n",
        "    col = []\n",
        "    weight = []\n",
        "\n",
        "    # pmi as weights\n",
        "    num_docs = len(doc_list)\n",
        "    num_window = len(windows)\n",
        "    for word_id_pair, count in tqdm(word_pair_count.items()):\n",
        "        i, j = word_id_pair[0], word_id_pair[1]\n",
        "        word_freq_i = word_window_freq[vocab[i]]\n",
        "        word_freq_j = word_window_freq[vocab[j]]\n",
        "        pmi = log((1.0 * count / num_window) /\n",
        "                  (1.0 * word_freq_i * word_freq_j / (num_window * num_window)))\n",
        "        if pmi <= 0:\n",
        "            continue\n",
        "        row.append(num_docs + i)\n",
        "        col.append(num_docs + j)\n",
        "        weight.append(pmi)\n",
        "\n",
        "    # frequency of document word pair\n",
        "    doc_word_freq = defaultdict(int)\n",
        "    for i, doc_words in enumerate(doc_list):\n",
        "        words = doc_words.split()\n",
        "        for word in words:\n",
        "            word_id = word_id_map[word]\n",
        "            doc_word_str = (i, word_id)\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "\n",
        "    for i, doc_words in enumerate(doc_list):\n",
        "        words = doc_words.split()\n",
        "        doc_word_set = set()\n",
        "        for word in words:\n",
        "            if word in doc_word_set:\n",
        "                continue\n",
        "            word_id = word_id_map[word]\n",
        "            freq = doc_word_freq[(i, word_id)]\n",
        "            row.append(i)\n",
        "            col.append(num_docs + word_id)\n",
        "            idf = log(1.0 * num_docs /\n",
        "                      word_doc_freq[vocab[word_id]])\n",
        "            weight.append(freq * idf)\n",
        "            doc_word_set.add(word)\n",
        "\n",
        "    number_nodes = num_docs + len(vocab)\n",
        "    adj_mat = sp.csr_matrix((weight, (row, col)), shape=(number_nodes, number_nodes))\n",
        "    adj = adj_mat + adj_mat.T.multiply(adj_mat.T > adj_mat) - adj_mat.multiply(adj_mat.T > adj_mat)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def get_vocab(text_list):\n",
        "    word_freq = defaultdict(int)\n",
        "    for doc_words in text_list:\n",
        "        words = doc_words.split()\n",
        "        for word in words:\n",
        "            word_freq[word] += 1\n",
        "    return word_freq\n",
        "\n",
        "\n",
        "def build_word_doc_edges(doc_list):\n",
        "    # build all docs that a word is contained in\n",
        "    words_in_docs = defaultdict(set)\n",
        "    for i, doc_words in enumerate(doc_list):\n",
        "        words = doc_words.split()\n",
        "        for word in words:\n",
        "            words_in_docs[word].add(i)\n",
        "\n",
        "    word_doc_freq = {}\n",
        "    for word, doc_list in words_in_docs.items():\n",
        "        word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "    return words_in_docs, word_doc_freq\n"
      ],
      "metadata": {
        "id": "-8TYLALS0kHO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prep_data.py\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from os.path import join, exists\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_data(dataset):\n",
        "    clean_text_path = join(get_data_path(), 'corpus', dataset + '_sentences_clean.txt')\n",
        "    if not exists(clean_text_path):\n",
        "        docs_list = []\n",
        "        old_name = dataset\n",
        "        if \"no_hashtag\" in dataset:\n",
        "            dataset = '_'.join(dataset.split('_')[:-2])\n",
        "        with open(join(get_data_path(), 'corpus', dataset + '_sentences.txt')) as f:\n",
        "            for line in f.readlines():\n",
        "                docs_list.append(line.strip())\n",
        "        dataset = old_name\n",
        "        word_counts = defaultdict(int)\n",
        "        for doc in docs_list:\n",
        "            temp = clean_doc(doc, dataset)\n",
        "            words = temp.split()\n",
        "            for word in words:\n",
        "                word_counts[word] += 1\n",
        "        clean_docs = clean_documents(docs_list, word_counts, dataset)\n",
        "        corpus_str = '\\n'.join(clean_docs)\n",
        "        f = open(clean_text_path, 'w')\n",
        "        f.write(corpus_str)\n",
        "        f.close()\n",
        "    f = open(clean_text_path, 'r')\n",
        "    lines = f.readlines()\n",
        "    min_len = 10000\n",
        "    aver_len = 0\n",
        "    max_len = 0\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        temp = line.split()\n",
        "        aver_len = aver_len + len(temp)\n",
        "        if len(temp) < min_len:\n",
        "            min_len = len(temp)\n",
        "        if len(temp) > max_len:\n",
        "            max_len = len(temp)\n",
        "    f.close()\n",
        "    aver_len = 1.0 * aver_len / len(lines)\n",
        "    print('min_len : ' + str(min_len))\n",
        "    print('max_len : ' + str(max_len))\n",
        "    print('average_len : ' + str(aver_len))\n",
        "\n",
        "\n",
        "def clean_documents(docs, word_counts, dataset):\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    print(stop_words)\n",
        "    ret = []\n",
        "    for doc in docs:\n",
        "        doc = clean_doc(doc, dataset)\n",
        "        words = doc.split()\n",
        "        words = [word for word in words if word not in stop_words and word_counts[word] >= 5]\n",
        "        doc = ' '.join(words).strip()\n",
        "        if doc != '':\n",
        "            ret.append(' '.join(words).strip())\n",
        "        else:\n",
        "            ret.append(' ')\n",
        "    return ret\n",
        "\n",
        "\n",
        "def clean_doc_ap(string):\n",
        "    string = re.sub(r\"http[s]?\\:\\/\\/.[a-zA-Z0-9\\.\\/\\_?=%&#\\-\\+!]+\", \" \", string)\n",
        "    string = re.sub(r\"[^A-Za-z0-9()_+,!?:\\'\\`]\", \" \", string)  # replace all non alpha numeric characters\n",
        "    string = re.sub(r\"(?<!HASHTAG)_\", \" \", string)\n",
        "    string = re.sub(r\"(?<!EASTASIA)\\+ | (?<!VIRUS)\\+\", \" \", string)\n",
        "    string = re.sub(r\"\\+\", \"_\", string)\n",
        "    string = re.sub(r\"HASHTAG_EASTASIA_VIRUS(?!(\\s))\", \"HASHTAG_EASTASIA_VIRUS \", string)\n",
        "    string = re.sub(r\"HASHTAG_EASTASIA(?!(\\s|_))\", \"HASHTAG_EASTASIA \", string)\n",
        "    string = re.sub(r\"HASHTAG_VIRUS(?!(\\s|_))\", \"HASHTAG_VIRUS \", string)\n",
        "    string = re.sub(r\"HASHTAG_VIRUS_OTHERCOUNTRY(?!(\\s))\", \"HASHTAG_VIRUS_OTHERCOUNTRY \", string)\n",
        "    string = re.sub(r\"HASHTAG(?!([\\s|_]))\", \"HASHTAG \", string)\n",
        "    if \"no_hashtag\" in dataset:\n",
        "        string = re.sub(r\"HASHTAG_EASTASIA_VIRUS\", \" \", string)\n",
        "        string = re.sub(r\"HASHTAG_EASTASIA\", \" \", string)\n",
        "        string = re.sub(r\"HASHTAG_VIRUS\", \" \", string)\n",
        "        string = re.sub(r\"HASHTAG_VIRUS_OTHERCOUNTRY\", \" \", string)\n",
        "        string = re.sub(r\"HASHTAG\", \" \", string)\n",
        "    return string\n",
        "\n",
        "\n",
        "def clean_doc(string, dataset):\n",
        "    if 'twitter_asian_prejudice' in dataset:\n",
        "        string = clean_doc_ap(string)\n",
        "    else:\n",
        "        pass\n",
        "    string = re.sub(r\"^\\\"\", \"\", string)\n",
        "    string = re.sub(r\"\\\"$\", \"\", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\"\\.\", \" \", string)\n",
        "    string = re.sub(r\",\", \" \", string)\n",
        "    string = re.sub(r\"!\", \" \", string)\n",
        "    string = re.sub(r\"\\(\", \" \", string)\n",
        "    string = re.sub(r\"\\)\", \" \", string)\n",
        "    string = re.sub(r\"\\?\", \" \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()"
      ],
      "metadata": {
        "id": "XL-kBqQw0xV-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import Parameter\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "\n",
        "\n",
        "class TextGNN(nn.Module):\n",
        "    def __init__(self, pred_type, node_embd_type, num_layers, layer_dim_list, act, bn, num_labels, class_weights, dropout):\n",
        "        super(TextGNN, self).__init__()\n",
        "        self.node_embd_type = node_embd_type\n",
        "        self.layer_dim_list = layer_dim_list\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        if pred_type == 'softmax':\n",
        "            assert layer_dim_list[-1] == num_labels\n",
        "        elif pred_type == 'mlp':\n",
        "            dims = self._calc_mlp_dims(layer_dim_list[-1], num_labels)\n",
        "            self.mlp = MLP(layer_dim_list[-1], num_labels, num_hidden_lyr=len(dims), hidden_channels=dims, bn=False)\n",
        "        self.pred_type = pred_type\n",
        "        assert len(layer_dim_list) == (num_layers + 1)\n",
        "        self.act = act\n",
        "        self.bn = bn\n",
        "        self.layers = self._create_node_embd_layers()\n",
        "        self.loss = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    def forward(self, pyg_graph, dataset):\n",
        "        acts = [pyg_graph.x]\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            ins = acts[-1]\n",
        "            outs = layer(ins, pyg_graph)\n",
        "            acts.append(outs)\n",
        "\n",
        "        return self._loss(acts[-1], dataset)\n",
        "\n",
        "    def _loss(self, ins, dataset):\n",
        "        pred_inds = dataset.node_ids\n",
        "        if self.pred_type == 'softmax':\n",
        "            y_preds = ins[pred_inds]\n",
        "        elif self.pred_type == 'mlp':\n",
        "            y_preds = self.mlp(ins[pred_inds])\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        y_true = torch.tensor(dataset.label_inds[pred_inds], dtype=torch.long, device=device)\n",
        "        loss = self.loss(y_preds, y_true)\n",
        "        return loss, y_preds.cpu().detach().numpy()\n",
        "\n",
        "    def _create_node_embd_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        for i in range(self.num_layers):\n",
        "            act = self.act if i < self.num_layers - 1 else 'identity'\n",
        "            layers.append(NodeEmbedding(\n",
        "                type=self.node_embd_type,\n",
        "                in_dim=self.layer_dim_list[i],\n",
        "                out_dim=self.layer_dim_list[i + 1],\n",
        "                act=act,\n",
        "                bn=self.bn,\n",
        "                dropout=self.dropout if i != 0 else False\n",
        "            ))\n",
        "        return layers\n",
        "\n",
        "    def _calc_mlp_dims(self, mlp_dim, output_dim=1):\n",
        "        dim = mlp_dim\n",
        "        dims = []\n",
        "        while dim > output_dim:\n",
        "            dim = dim // 2\n",
        "            dims.append(dim)\n",
        "        dims = dims[:-1]\n",
        "        return dims\n",
        "\n",
        "\n",
        "class NodeEmbedding(nn.Module):\n",
        "    def __init__(self, type, in_dim, out_dim, act, bn, dropout):\n",
        "        super(NodeEmbedding, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.type = type\n",
        "        if type == 'gcn':\n",
        "            self.conv = GCNConv(in_dim, out_dim)\n",
        "            self.act = create_act(act, out_dim)\n",
        "        elif type == 'gat':\n",
        "            self.conv = GATConv(in_dim, out_dim)\n",
        "            self.act = create_act(act, out_dim)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'Unknown node embedding layer type {}'.format(type))\n",
        "        self.bn = bn\n",
        "        if self.bn:\n",
        "            self.bn = torch.nn.BatchNorm1d(out_dim)\n",
        "        self.dropout = dropout\n",
        "        if dropout:\n",
        "            self.dropout = torch.nn.Dropout()\n",
        "\n",
        "    def forward(self, ins, pyg_graph):\n",
        "        if self.dropout:\n",
        "            ins = self.dropout(ins)\n",
        "        if self.type == 'gcn':\n",
        "            if use_edge_weights:\n",
        "                x = self.conv(ins, pyg_graph.edge_index, edge_weight=pyg_graph.edge_attr)\n",
        "            else:\n",
        "                x = self.conv(ins, pyg_graph.edge_index)\n",
        "        else:\n",
        "            x = self.conv(ins, pyg_graph.edge_index)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    '''mlp can specify number of hidden layers and hidden layer channels'''\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, activation_type='relu', num_hidden_lyr=2,\n",
        "                 hidden_channels=None, bn=False):\n",
        "        super().__init__()\n",
        "        self.out_dim = output_dim\n",
        "        if not hidden_channels:\n",
        "            hidden_channels = [input_dim for _ in range(num_hidden_lyr)]\n",
        "        elif len(hidden_channels) != num_hidden_lyr:\n",
        "            raise ValueError(\n",
        "                \"number of hidden layers should be the same as the lengh of hidden_channels\")\n",
        "        self.layer_channels = [input_dim] + hidden_channels + [output_dim]\n",
        "        self.activation = create_act(activation_type)\n",
        "        self.layers = nn.ModuleList(list(\n",
        "            map(self.weight_init, [nn.Linear(self.layer_channels[i], self.layer_channels[i + 1])\n",
        "                                   for i in range(len(self.layer_channels) - 1)])))\n",
        "        self.bn = bn\n",
        "        if self.bn:\n",
        "            self.bn = torch.nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def weight_init(self, m):\n",
        "        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        return m\n",
        "\n",
        "    def forward(self, x):\n",
        "        layer_inputs = [x]\n",
        "        for layer in self.layers:\n",
        "            input = layer_inputs[-1]\n",
        "            if layer == self.layers[-1]:\n",
        "                layer_inputs.append(layer(input))\n",
        "            else:\n",
        "                layer_inputs.append(self.activation(layer(input)))\n",
        "        # model.store_layer_output(self, layer_inputs[-1])\n",
        "        if self.bn:\n",
        "            layer_inputs[-1] = self.bn(layer_inputs[-1])\n",
        "        return layer_inputs[-1]\n",
        "\n",
        "\n",
        "def create_act(act, num_parameters=None):\n",
        "    if act == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif act == 'prelu':\n",
        "        return nn.PReLU(num_parameters)\n",
        "    elif act == 'sigmoid':\n",
        "        return nn.Sigmoid()\n",
        "    elif act == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    elif act == 'identity':\n",
        "        class Identity(nn.Module):\n",
        "            def forward(self, x):\n",
        "                return x\n",
        "\n",
        "        return Identity()\n",
        "    else:\n",
        "        raise ValueError('Unknown activation function {}'.format(act))\n",
        "\n",
        "\n",
        "class GCNConv(MessagePassing):\n",
        "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\n",
        "    Classfication with Graph Convolutional Networks\"\n",
        "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
        "\n",
        "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
        "    adjacency matrix with inserted self-loops and\n",
        "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
        "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
        "            (default: :obj:`False`)\n",
        "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
        "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
        "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}`.\n",
        "            (default: :obj:`False`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 improved=False,\n",
        "                 cached=False,\n",
        "                 bias=True):\n",
        "        super(GCNConv, self).__init__('add')\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.improved = improved\n",
        "        self.cached = cached\n",
        "        self.cached_result = None\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight)\n",
        "        zeros(self.bias)\n",
        "        self.cached_result = None\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1), ),\n",
        "                                     dtype=dtype,\n",
        "                                     device=edge_index.device)\n",
        "        edge_weight = edge_weight.view(-1)\n",
        "        assert edge_weight.size(0) == edge_index.size(1)\n",
        "\n",
        "        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n",
        "        #edge_index = add_self_loops(edge_index, num_nodes)\n",
        "        # Calling add_self_loops with optional edge_attr:\n",
        "        edge_attr = torch.ones([edge_index.shape[1]], dtype=torch.float32)  # Assuming one attribute per edge\n",
        "\n",
        "        edge_index, edge_attr = add_self_loops(edge_index, edge_attr=edge_attr if edge_attr is not None else None)\n",
        "\n",
        "        loop_weight = torch.full((num_nodes, ),\n",
        "                                 1 if not improved else 2,\n",
        "                                 dtype=edge_weight.dtype,\n",
        "                                 device=edge_weight.device)\n",
        "        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        \"\"\"\"\"\"\n",
        "        if x.is_sparse:\n",
        "            x = torch.sparse.mm(x, self.weight)\n",
        "        else:\n",
        "            x = torch.matmul(x, self.weight)\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\n",
        "                                            self.improved, x.dtype)\n",
        "            self.cached_result = edge_index, norm\n",
        "\n",
        "        edge_index, norm = self.cached_result\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        if self.bias is not None:\n",
        "            aggr_out = aggr_out + self.bias\n",
        "        return aggr_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)\n",
        "\n",
        "\n",
        "class GATConv(MessagePassing):\n",
        "    r\"\"\"The graph attentional operator from the `\"Graph Attention Networks\"\n",
        "    <https://arxiv.org/abs/1710.10903>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{j} +\n",
        "        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\n",
        "\n",
        "    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\n",
        "\n",
        "    .. math::\n",
        "        \\alpha_{i,j} =\n",
        "        \\frac{\n",
        "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
        "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n",
        "        \\right)\\right)}\n",
        "        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n",
        "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
        "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n",
        "        \\right)\\right)}.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        heads (int, optional): Number of multi-head-attentions. (default:\n",
        "            :obj:`1`)\n",
        "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
        "        attentions are averaged instead of concatenated. (default: :obj:`True`)\n",
        "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
        "            slope. (default: :obj:`0.2`)\n",
        "        dropout (float, optional): Dropout probability of the normalized\n",
        "            attention coefficients which exposes each node to a stochastically\n",
        "            sampled neighborhood during training. (default: :obj:`0`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 heads=1,\n",
        "                 concat=True,\n",
        "                 negative_slope=0.2,\n",
        "                 dropout=0,\n",
        "                 bias=True):\n",
        "        super(GATConv, self).__init__('add')\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.concat = concat\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.weight = Parameter(\n",
        "            torch.Tensor(in_channels, heads * out_channels))\n",
        "        self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels))\n",
        "\n",
        "        if bias and concat:\n",
        "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
        "        elif bias and not concat:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight)\n",
        "        glorot(self.att)\n",
        "        zeros(self.bias)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\"\"\"\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        if x.is_sparse:\n",
        "            x = torch.sparse.mm(x, self.weight).view(-1, self.heads, self.out_channels)\n",
        "        else:\n",
        "            x = torch.matmul(x, self.weight).view(-1, self.heads, self.out_channels)\n",
        "        return self.propagate(edge_index, x=x, num_nodes=x.size(0))\n",
        "\n",
        "    def message(self, x_i, x_j, edge_index, num_nodes):\n",
        "        # Compute attention coefficients.\n",
        "        alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n",
        "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
        "        alpha = softmax(alpha, edge_index[0], num_nodes)\n",
        "\n",
        "        # Sample attention coefficients stochastically.\n",
        "        if self.training and self.dropout > 0:\n",
        "            alpha = F.dropout(alpha, p=self.dropout, training=True)\n",
        "\n",
        "        return x_j * alpha.view(-1, self.heads, 1)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        if self.concat is True:\n",
        "            aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n",
        "        else:\n",
        "            aggr_out = aggr_out.mean(dim=1)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            aggr_out = aggr_out + self.bias\n",
        "        return aggr_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
        "                                             self.in_channels,\n",
        "                                             self.out_channels, self.heads)"
      ],
      "metadata": {
        "id": "jlvPXsBd8LUy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def create_model(dataset):\n",
        "    name = model  # Directly use the global variable 'model'\n",
        "    layer_info = model_params  # Directly use the global dictionary 'model_params'\n",
        "    if name in model_ctors:\n",
        "        return model_ctors[name](layer_info, dataset)\n",
        "    else:\n",
        "        raise ValueError(\"Model not implemented {}\".format(name))\n",
        "\n",
        "\n",
        "\n",
        "def create_text_gnn(layer_info, dataset):\n",
        "\n",
        "    lyr_dims = layer_info[\"layer_dims\"]\n",
        "    lyr_dims = [dataset.node_feats.shape[1]] + lyr_dims\n",
        "    weights = None\n",
        "    if layer_info[\"class_weights\"] == True:\n",
        "        counts = Counter(dataset.label_inds[dataset.node_ids])\n",
        "        weights = len(counts) * [0]\n",
        "        min_weight = min(counts.values())\n",
        "        for k, v in counts.items():\n",
        "            weights[k] = min_weight / float(v)\n",
        "        weights = torch.tensor(weights, device=device)\n",
        "\n",
        "    return TextGNN(\n",
        "        pred_type=layer_info[\"pred_type\"],\n",
        "        node_embd_type=layer_info[\"node_embd\"],\n",
        "        num_layers=int(layer_info[\"num_layers\"]),\n",
        "        layer_dim_list=lyr_dims,\n",
        "        act=layer_info[\"act\"],\n",
        "        bn=False,\n",
        "        num_labels=len(dataset.label_dict),\n",
        "        class_weights=weights,\n",
        "        dropout=layer_info[\"dropout\"]\n",
        "    )\n",
        "\n",
        "\n",
        "model_ctors = {\n",
        "    'TextGNN': create_text_gnn,\n",
        "}"
      ],
      "metadata": {
        "id": "3ymWi6av5m_B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def eval(preds, dataset, test=False):\n",
        "    y_true = dataset.label_inds[dataset.node_ids]\n",
        "    y_pred_label = np.asarray([np.argmax(pred) for pred in preds])\n",
        "    accuracy = metrics.accuracy_score(y_true, y_pred_label)\n",
        "    f1_weighted = metrics.f1_score(y_true, y_pred_label, average='weighted')\n",
        "    f1_macro = metrics.f1_score(y_true, y_pred_label, average='macro')\n",
        "    f1_micro = metrics.f1_score(y_true, y_pred_label, average='micro')\n",
        "    precision_weighted = metrics.precision_score(y_true, y_pred_label, average='weighted')\n",
        "    precision_macro = metrics.precision_score(y_true, y_pred_label, average='macro')\n",
        "    precision_micro = metrics.precision_score(y_true, y_pred_label, average='micro')\n",
        "    recall_weighted = metrics.recall_score(y_true, y_pred_label, average='weighted')\n",
        "    recall_macro = metrics.recall_score(y_true, y_pred_label, average='macro')\n",
        "    recall_micro = metrics.recall_score(y_true, y_pred_label, average='micro')\n",
        "    results = {\"accuracy\": accuracy,\n",
        "               \"f1_weighted\": f1_weighted,\n",
        "               \"f1_macro\": f1_macro,\n",
        "               \"f1_micro\": f1_micro,\n",
        "               \"precision_weighted\": precision_weighted,\n",
        "               \"precision_macro\": precision_macro,\n",
        "               \"precision_micro\": precision_micro,\n",
        "               \"recall_weighted\": recall_weighted,\n",
        "               \"recall_macro\": recall_macro,\n",
        "               \"recall_micro\": recall_micro\n",
        "               }\n",
        "    if test:\n",
        "        one_hot_true = np.zeros((y_true.size, len(dataset.label_dict)))\n",
        "        one_hot_true[np.arange(y_true.size), y_true] = 1\n",
        "        results[\"y_true\"] = one_hot_true\n",
        "        one_hot_pred = np.zeros((y_true.size, len(dataset.label_dict)))\n",
        "        one_hot_pred[np.arange(y_pred_label.size),y_pred_label] = 1\n",
        "        results[\"y_pred\"] = one_hot_pred\n",
        "    return results\n",
        "\n",
        "\n",
        "class MovingAverage(object):\n",
        "    def __init__(self, window, want_increase=True):\n",
        "        self.moving_avg = [float('-inf')] if want_increase else [float('inf')]\n",
        "        self.want_increase = want_increase\n",
        "        self.results = []\n",
        "        self.window = window\n",
        "\n",
        "    def add_to_moving_avg(self, x):\n",
        "        self.results.append(x)\n",
        "        if len(self.results) >= self.window:\n",
        "            next_val = sum(self.results[-self.window:]) / self.window\n",
        "            self.moving_avg.append(next_val)\n",
        "\n",
        "    def best_result(self, x):\n",
        "        if self.want_increase:\n",
        "            return (x - 1e-7) > max(self.results)\n",
        "        else:\n",
        "            return (x + 1e-7) < min(self.results)\n",
        "\n",
        "    def stop(self):\n",
        "        if len(self.moving_avg) < 2:\n",
        "            return False\n",
        "        if self.want_increase:\n",
        "            return (self.moving_avg[-1] + 1e-7) < self.moving_avg[-2]\n",
        "        else:\n",
        "            return (self.moving_avg[-2] + 1e-7) < self.moving_avg[-1]"
      ],
      "metadata": {
        "id": "prsKtlIV_HEc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from os.path import join, getctime\n",
        "import torch\n",
        "\n",
        "class Saver(object):\n",
        "    def __init__(self):\n",
        "        model_str = self.get_model_str()\n",
        "        self.logdir = join(\n",
        "            '/content/drive/My Drive/CPSC_577_FP/logs',  # 使用 Colab 的默认工作目录下的 logs 目录\n",
        "            '{}_{}'.format(model_str, get_ts()))\n",
        "        create_dir_if_not_exists(self.logdir)\n",
        "        self.model_info_f = self._open('model_info.txt')\n",
        "        print('Logging to {}'.format(self.logdir))\n",
        "\n",
        "    def save_trained_model(self, trained_model, epoch=None):\n",
        "        epoch = \"_epoch_{}\".format(epoch) if epoch is not None else \"\"\n",
        "        p = join(self.logdir, 'trained_model{}.pt'.format(epoch))\n",
        "        torch.save(trained_model.state_dict(), p)\n",
        "        print('Trained model saved to {}'.format(p))\n",
        "\n",
        "    def load_trained_model(self, train_data):\n",
        "        p = join(self.logdir, 'trained_model*')\n",
        "        files = glob.glob(p)\n",
        "        best_trained_model_path = max(files, key=getctime)\n",
        "        trained_model = create_model(train_data)\n",
        "        trained_model.load_state_dict(\n",
        "            torch.load(best_trained_model_path, map_location=device))\n",
        "        trained_model.to(device)\n",
        "        return trained_model\n",
        "\n",
        "    def get_model_str(self):\n",
        "        li = []\n",
        "        key_flags = [model, dataset, \"_\".join([str(i) for i in tvt_ratio])]\n",
        "        for f in key_flags:\n",
        "            li.append(str(f))\n",
        "        return '_'.join(li)\n",
        "\n",
        "    def _open(self, f):\n",
        "        return open(join(self.logdir, f), 'w')\n"
      ],
      "metadata": {
        "id": "EuKe9AUyAEI4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    global dataset\n",
        "    dir = join(get_save_path(), 'split')\n",
        "    dataset_name = dataset\n",
        "    train_ratio = int(tvt_ratio[0] * 100)\n",
        "    val_ratio = int(tvt_ratio[1] * 100)\n",
        "    test_ratio = 100 - train_ratio - val_ratio\n",
        "    if 'presplit' not in dataset_name:\n",
        "        save_fn = '{}_train_{}_val_{}_test_{}_seed_{}_window_size_{}'.format(dataset_name, train_ratio,\n",
        "                                                              val_ratio, test_ratio,\n",
        "                                                              random_seed, word_window_size)\n",
        "    else:\n",
        "        save_fn = '{}_train_val_test_{}_window_size_{}'.format(dataset_name, random_seed, word_window_size)\n",
        "    path = join(dir, save_fn)\n",
        "    rtn = load(path)\n",
        "    if rtn:\n",
        "        train_data, val_data, test_data = rtn['train_data'], rtn['val_data'], rtn['test_data']\n",
        "    else:\n",
        "        train_data, val_data, test_data = _load_tvt_data_helper()\n",
        "        save({'train_data': train_data, 'val_data': val_data, 'test_data': test_data}, path)\n",
        "    dataset = dataset\n",
        "    if \"small\" in dataset or \"presplit\" in dataset or 'sentiment' in dataset:\n",
        "        dataset_name = \"_\".join(dataset.split(\"_\")[:-1])\n",
        "    else:\n",
        "        dataset_name = dataset\n",
        "\n",
        "    orig_text_path = join(get_corpus_path(), dataset_name + \"_sentences.txt\")\n",
        "    raw_doc_list = []\n",
        "    f = open(orig_text_path, 'rb')\n",
        "    for line in f.readlines():\n",
        "        raw_doc_list.append(line.strip().decode())\n",
        "    f.close()\n",
        "\n",
        "    return train_data, val_data, test_data, raw_doc_list\n",
        "\n",
        "\n",
        "def _load_tvt_data_helper():\n",
        "    global dataset\n",
        "    dir = join(get_save_path(), 'all')\n",
        "    path = join(dir, dataset + '_all_window_' + str(word_window_size))\n",
        "    rtn = load(path)\n",
        "    if rtn:\n",
        "        dataset = TextDataset(None, None, None, None, None, None, rtn)\n",
        "    else:\n",
        "        dataset = build_text_graph_dataset(dataset, word_window_size)\n",
        "        gc.collect()\n",
        "        save(dataset.__dict__, path)\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = dataset.tvt_split(tvt_ratio[:2], tvt_list, random_seed)\n",
        "    return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "31ixQVgSCIeK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import time\n",
        "\n",
        "\n",
        "def train(train_data, val_data, saver, logs = True):\n",
        "    train_data.init_node_feats(init_type, device)\n",
        "    val_data.init_node_feats(init_type, device)\n",
        "    model = create_model(train_data)\n",
        "    model = model.to(device)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    if logs:\n",
        "      print(\"Number params: \", pytorch_total_params)\n",
        "    moving_avg = MovingAverage(validation_window_size, validation_metric != 'loss')\n",
        "    pyg_graph = train_data.get_pyg_graph(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        loss, preds_train = model(pyg_graph, train_data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.item()\n",
        "        with torch.no_grad():\n",
        "            val_loss, preds_val = model(pyg_graph, val_data)\n",
        "            val_loss = val_loss.item()\n",
        "            eval_res_val = eval(preds_val, val_data)\n",
        "            if logs:\n",
        "              print(\"Epoch: {:04d}, Train Loss: {:.5f}, Time: {:.5f}\".format(epoch, loss, time.time() - t))\n",
        "              print(\"Val Loss: {:.5f}\".format(val_loss))\n",
        "              print(\"Val Results: ...\")\n",
        "              pprint(eval_res_val)\n",
        "            eval_res_val[\"loss\"] = val_loss\n",
        "\n",
        "            if len(moving_avg.results) == 0 or moving_avg.best_result(eval_res_val[validation_metric]):\n",
        "                saver.save_trained_model(model, epoch + 1)\n",
        "            moving_avg.add_to_moving_avg(eval_res_val[validation_metric])\n",
        "            if moving_avg.stop():\n",
        "                break\n",
        "    best_model = saver.load_trained_model(train_data)\n",
        "    return best_model, model"
      ],
      "metadata": {
        "id": "omxsss62_tpo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "\"\"\"\n",
        "Most Relevant\n",
        "\"\"\"\n",
        "\n",
        "debug = False\n",
        "gpu = -1\n",
        "use_comet_ml = False\n",
        "\n",
        "\"\"\"\n",
        "dataset:\n",
        " sentiment suffix for twitter means the negative classes of the original dataset are combined and the other classes are combined for sentiment analysis\n",
        " presplit suffix means training and test are predetermined in [dataset]_labels.txt\n",
        " small suffix means a very small dataset used for debugging\n",
        "\"\"\"\n",
        "random_seed = 123\n",
        "dataset = 'r8_presplit'\n",
        "# dataset = 'ag_presplit'\n",
        "\n",
        "if 'ag' in dataset:\n",
        "    num_labels = 4\n",
        "elif 'r8' in dataset:\n",
        "    num_labels = 8\n",
        "\n",
        "\"\"\"\n",
        "Model. Pt1\n",
        "\"\"\"\n",
        "\n",
        "model = \"TextGNN\"\n",
        "\n",
        "model_params = {}\n",
        "use_edge_weights = False\n",
        "init_type = 'one_hot_init'\n",
        "if model == 'TextGNN':\n",
        "    pred_type = 'softmax'\n",
        "    node_embd_type = 'gcn'\n",
        "    layer_dim_list = [200, num_labels]\n",
        "    num_layers = len(layer_dim_list)\n",
        "    class_weights = True\n",
        "    dropout = True\n",
        "    s = 'TextGCN:pred_type={},node_embd_type={},num_layers={},layer_dim_list={},act={},' \\\n",
        "        'dropout={},class_weights={}'.format(\n",
        "        pred_type, node_embd_type, num_layers, \"_\".join([str(i) for i in layer_dim_list]), 'relu', dropout, class_weights\n",
        "    )\n",
        "    model_params = {\n",
        "        'pred_type': pred_type,\n",
        "        'node_embd':  node_embd_type,\n",
        "        'num_layers': num_layers,\n",
        "        'layer_dims': layer_dim_list,\n",
        "        'act': 'relu',\n",
        "        'class_weights': class_weights,\n",
        "        'dropout': dropout\n",
        "    }\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "print(\"{}: {}\\n\".format(model, model_params))\n",
        "\n",
        "\"\"\"\n",
        "Sampling\n",
        "\"\"\"\n",
        "word_window_size = 10\n",
        "validation_window_size = 10\n",
        "\n",
        "\"\"\"\n",
        "Validation\n",
        "\"\"\"\n",
        "validation_metric = \"accuracy\"  # Alternatively, \"f1_weighted\" or \"loss\"\n",
        "\n",
        "use_best_val_model_for_inference = True\n",
        "\n",
        "\"\"\"\n",
        "Evaluation.\n",
        "\"\"\"\n",
        "tvt_ratio = [0.8, 0.1, 0.1]\n",
        "tvt_list = [\"train\", \"test\", \"val\"]\n",
        "\n",
        "\"\"\"\n",
        "Optimization.\n",
        "\"\"\"\n",
        "\n",
        "lr = 1e-2\n",
        "\n",
        "device = 'cuda'#.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu'\n",
        "\n",
        "num_epochs = 2 if debug else 400\n",
        "\n",
        "\"\"\"\n",
        "Other info.\n",
        "\"\"\"\n",
        "# Assuming get_user() and get_host() are function calls that need to be defined or imported\n",
        "user = get_user()\n",
        "hostname = get_host()"
      ],
      "metadata": {
        "id": "ASbR0ZDG06yE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec7008c-d601-45fa-8f32-4fde9f3426b1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextGNN: {'pred_type': 'softmax', 'node_embd': 'gcn', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "dataset = 'r8_presplit'\n",
        "num_experiments = 5\n",
        "random_seeds = [33, 15, 86, 109, 78]\n",
        "model = \"TextGNN\"\n",
        "all_experiment_results = []\n",
        "warnings.filterwarnings('ignore')\n",
        "for exp in range(num_experiments):\n",
        "    random_seed = random_seeds[exp]\n",
        "    saver = Saver()\n",
        "    train_data, val_data, test_data, raw_doc_list = load_data()\n",
        "\n",
        "    saved_model, model = train(train_data, val_data, saver, False)\n",
        "    with torch.no_grad():\n",
        "        test_loss_model, preds_model = model(train_data.get_pyg_graph(device=device), test_data)\n",
        "    eval_res = eval(preds_model, test_data, True)\n",
        "    y_true = eval_res.pop('y_true')\n",
        "    y_pred = eval_res.pop('y_pred')\n",
        "    print(\"Test...experiment \", exp+1)\n",
        "    pprint(eval_res)\n",
        "    all_experiment_results.append(eval_res)\n",
        "    model = \"TextGNN\"\n",
        "\n",
        "# Calculate mean and standard deviation across experiments\n",
        "final_metrics = {key: [] for key in all_experiment_results[0]}\n",
        "for results in all_experiment_results:\n",
        "    for key in results:\n",
        "        final_metrics[key].append(results[key])\n",
        "\n",
        "for metric in final_metrics:\n",
        "    values = np.array(final_metrics[metric])\n",
        "    mean = values.mean()\n",
        "    std = values.std()\n",
        "    print(f'{metric}: Mean={mean:.6f}, Std={std:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlxwWyMgCTfI",
        "outputId": "06e39e2d-6a53-49cc-851a-2918b6ea2d19"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/r8_presplit_train_val_test_33_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_5.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_6.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_7.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_8.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_9.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_10.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_11.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_12.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_13.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_14.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_15.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_16.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_17.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_18.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_20.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_23.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_24.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_26.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_28.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_31.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_35.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_38.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_39.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_40.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_41.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_45.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_50.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_53.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_54.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_57.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_59.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_60.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_64.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_69.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_72.pt\n",
            "Test...experiment  1\n",
            "{'accuracy': 0.9547738693467337,\n",
            " 'f1_macro': 0.8823359183371983,\n",
            " 'f1_micro': 0.9547738693467337,\n",
            " 'f1_weighted': 0.9550895612896675,\n",
            " 'precision_macro': 0.8652641489651627,\n",
            " 'precision_micro': 0.9547738693467337,\n",
            " 'precision_weighted': 0.9572306244662672,\n",
            " 'recall_macro': 0.910487086754322,\n",
            " 'recall_micro': 0.9547738693467337,\n",
            " 'recall_weighted': 0.9547738693467337}\n",
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/r8_presplit_train_val_test_15_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_3.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_4.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_5.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_6.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_7.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_8.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_9.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_10.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_11.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_12.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_13.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_14.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_15.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_17.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_18.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_20.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_22.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_24.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_27.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_29.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_30.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_31.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_36.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_37.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_38.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_40.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_44.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_47.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_48.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_49.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_55.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_60.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_67.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_68.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_72.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_77.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_81.pt\n",
            "Test...experiment  2\n",
            "{'accuracy': 0.9602558245774326,\n",
            " 'f1_macro': 0.888571501469031,\n",
            " 'f1_micro': 0.9602558245774326,\n",
            " 'f1_weighted': 0.9605935246570255,\n",
            " 'precision_macro': 0.8680278382096172,\n",
            " 'precision_micro': 0.9602558245774326,\n",
            " 'precision_weighted': 0.9620587517897314,\n",
            " 'recall_macro': 0.9198445818918943,\n",
            " 'recall_micro': 0.9602558245774326,\n",
            " 'recall_weighted': 0.9602558245774326}\n",
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/r8_presplit_train_val_test_86_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_3.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_4.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_5.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_6.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_7.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_8.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_9.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_10.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_11.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_12.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_13.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_14.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_15.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_16.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_19.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_20.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_22.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_23.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_28.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_30.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_31.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_33.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_36.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_38.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_39.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_41.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_44.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_51.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_59.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_63.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_67.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_74.pt\n",
            "Test...experiment  3\n",
            "{'accuracy': 0.9561443581544085,\n",
            " 'f1_macro': 0.8697749910518104,\n",
            " 'f1_micro': 0.9561443581544085,\n",
            " 'f1_weighted': 0.9565543430311001,\n",
            " 'precision_macro': 0.8543908975317656,\n",
            " 'precision_micro': 0.9561443581544085,\n",
            " 'precision_weighted': 0.9587100999045272,\n",
            " 'recall_macro': 0.9014726828392119,\n",
            " 'recall_micro': 0.9561443581544085,\n",
            " 'recall_weighted': 0.9561443581544085}\n",
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/r8_presplit_train_val_test_109_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_3.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_4.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_5.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_6.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_7.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_8.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_10.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_11.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_12.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_13.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_14.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_15.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_16.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_17.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_18.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_19.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_20.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_21.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_23.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_24.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_26.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_28.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_35.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_37.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_40.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_48.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_51.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_52.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_61.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_62.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_63.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_73.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_74.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_75.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_77.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_84.pt\n",
            "Test...experiment  4\n",
            "{'accuracy': 0.9579716765646414,\n",
            " 'f1_macro': 0.8653727086124509,\n",
            " 'f1_micro': 0.9579716765646414,\n",
            " 'f1_weighted': 0.9589653473983044,\n",
            " 'precision_macro': 0.8383978196548483,\n",
            " 'precision_micro': 0.9579716765646414,\n",
            " 'precision_weighted': 0.9607401174172141,\n",
            " 'recall_macro': 0.9052735744691215,\n",
            " 'recall_micro': 0.9579716765646414,\n",
            " 'recall_weighted': 0.9579716765646414}\n",
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/r8_presplit_train_val_test_78_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_3.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_4.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_5.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_6.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_7.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_8.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_9.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_10.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_11.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_12.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_13.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_14.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_15.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_16.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_17.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_19.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_20.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_22.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_25.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_28.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_30.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_31.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_33.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_34.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_37.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_38.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_40.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_41.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_45.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_46.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_51.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_53.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_57.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_58.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_62.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_66.pt\n",
            "Test...experiment  5\n",
            "{'accuracy': 0.9534033805390589,\n",
            " 'f1_macro': 0.8762700943856454,\n",
            " 'f1_micro': 0.9534033805390589,\n",
            " 'f1_weighted': 0.9542165718635244,\n",
            " 'precision_macro': 0.8513169303865393,\n",
            " 'precision_micro': 0.9534033805390589,\n",
            " 'precision_weighted': 0.9557631165798918,\n",
            " 'recall_macro': 0.908243932869997,\n",
            " 'recall_micro': 0.9534033805390589,\n",
            " 'recall_weighted': 0.9534033805390589}\n",
            "accuracy: Mean=0.956510, Std=0.002407\n",
            "f1_weighted: Mean=0.957084, Std=0.002381\n",
            "f1_macro: Mean=0.876465, Std=0.008353\n",
            "f1_micro: Mean=0.956510, Std=0.002407\n",
            "precision_weighted: Mean=0.958901, Std=0.002282\n",
            "precision_macro: Mean=0.855480, Std=0.010616\n",
            "precision_micro: Mean=0.956510, Std=0.002407\n",
            "recall_weighted: Mean=0.956510, Std=0.002407\n",
            "recall_macro: Mean=0.909064, Std=0.006179\n",
            "recall_micro: Mean=0.956510, Std=0.002407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'r8_presplit'\n",
        "saver = Saver()\n",
        "train_data, val_data, test_data, raw_doc_list = load_data()\n",
        "\n",
        "print(train_data.graph.shape)\n",
        "\n",
        "\n",
        "saved_model, model = train(train_data, val_data, saver)\n",
        "with torch.no_grad():\n",
        "    test_loss_model, preds_model = model(train_data.get_pyg_graph(device=device), test_data)\n",
        "eval_res = eval(preds_model, test_data, True)\n",
        "y_true = eval_res.pop('y_true')\n",
        "y_pred = eval_res.pop('y_pred')\n",
        "print(\"Test...\")\n",
        "pprint(eval_res)"
      ],
      "metadata": {
        "id": "gMb_bP2XBnRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3794b126-7e13-4eba-dfff-5ef9392bd9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/r8_presplit_train_val_test_123_window_size_10.klepto\n",
            "(15362, 15362)\n",
            "Number params:  3074208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0000, Train Loss: 2.07944, Time: 10.06621\n",
            "Val Loss: 2.05783\n",
            "Val Results: ...\n",
            "{'accuracy': 0.33211678832116787,\n",
            " 'f1_macro': 0.10374111182934712,\n",
            " 'f1_micro': 0.33211678832116787,\n",
            " 'f1_weighted': 0.3736514987803094,\n",
            " 'precision_macro': 0.13358778625954199,\n",
            " 'precision_micro': 0.33211678832116787,\n",
            " 'precision_weighted': 0.527107594584053,\n",
            " 'recall_macro': 0.19250871080139373,\n",
            " 'recall_micro': 0.33211678832116787,\n",
            " 'recall_weighted': 0.33211678832116787}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001, Train Loss: 2.06107, Time: 10.73224\n",
            "Val Loss: 2.03175\n",
            "Val Results: ...\n",
            "{'accuracy': 0.3905109489051095,\n",
            " 'f1_macro': 0.11602559919961722,\n",
            " 'f1_micro': 0.3905109489051095,\n",
            " 'f1_weighted': 0.42008979699272236,\n",
            " 'precision_macro': 0.13434903047091412,\n",
            " 'precision_micro': 0.3905109489051095,\n",
            " 'precision_weighted': 0.5274076470469297,\n",
            " 'recall_macro': 0.20644599303135888,\n",
            " 'recall_micro': 0.3905109489051095,\n",
            " 'recall_weighted': 0.3905109489051095}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0002, Train Loss: 2.03371, Time: 11.29053\n",
            "Val Loss: 1.99904\n",
            "Val Results: ...\n",
            "{'accuracy': 0.4124087591240876,\n",
            " 'f1_macro': 0.12270233798272981,\n",
            " 'f1_micro': 0.4124087591240876,\n",
            " 'f1_weighted': 0.44062406348140515,\n",
            " 'precision_macro': 0.2596704871060172,\n",
            " 'precision_micro': 0.4124087591240876,\n",
            " 'precision_weighted': 0.8286292430928827,\n",
            " 'recall_macro': 0.2123165452433745,\n",
            " 'recall_micro': 0.4124087591240876,\n",
            " 'recall_weighted': 0.4124087591240876}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_3.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0003, Train Loss: 1.99790, Time: 13.02129\n",
            "Val Loss: 1.95721\n",
            "Val Results: ...\n",
            "{'accuracy': 0.45985401459854014,\n",
            " 'f1_macro': 0.14840308297506302,\n",
            " 'f1_micro': 0.45985401459854014,\n",
            " 'f1_weighted': 0.5048530660293316,\n",
            " 'precision_macro': 0.24268973214285716,\n",
            " 'precision_micro': 0.45985401459854014,\n",
            " 'precision_weighted': 0.7859611248696559,\n",
            " 'recall_macro': 0.22879315806145073,\n",
            " 'recall_micro': 0.45985401459854014,\n",
            " 'recall_weighted': 0.45985401459854014}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_4.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0004, Train Loss: 1.95571, Time: 10.99047\n",
            "Val Loss: 1.90734\n",
            "Val Results: ...\n",
            "{'accuracy': 0.5547445255474452,\n",
            " 'f1_macro': 0.21234948002189385,\n",
            " 'f1_micro': 0.5547445255474452,\n",
            " 'f1_weighted': 0.6166898389526128,\n",
            " 'precision_macro': 0.37290732648544356,\n",
            " 'precision_micro': 0.5547445255474452,\n",
            " 'precision_weighted': 0.8176466616433526,\n",
            " 'recall_macro': 0.27382454943430556,\n",
            " 'recall_micro': 0.5547445255474452,\n",
            " 'recall_weighted': 0.5547445255474452}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0005, Train Loss: 1.90780, Time: 10.68517\n",
            "Val Loss: 1.85326\n",
            "Val Results: ...\n",
            "{'accuracy': 0.7062043795620438,\n",
            " 'f1_macro': 0.4321898189100202,\n",
            " 'f1_micro': 0.7062043795620438,\n",
            " 'f1_weighted': 0.7488764432011759,\n",
            " 'precision_macro': 0.5342315171673442,\n",
            " 'precision_micro': 0.7062043795620438,\n",
            " 'precision_weighted': 0.8498713928037006,\n",
            " 'recall_macro': 0.463704860014231,\n",
            " 'recall_micro': 0.7062043795620438,\n",
            " 'recall_weighted': 0.7062043795620438}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_6.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0006, Train Loss: 1.85274, Time: 10.09942\n",
            "Val Loss: 1.79667\n",
            "Val Results: ...\n",
            "{'accuracy': 0.7609489051094891,\n",
            " 'f1_macro': 0.4728033268101761,\n",
            " 'f1_micro': 0.7609489051094891,\n",
            " 'f1_weighted': 0.771328224320425,\n",
            " 'precision_macro': 0.46667484016274335,\n",
            " 'precision_micro': 0.7609489051094891,\n",
            " 'precision_weighted': 0.816387123820312,\n",
            " 'recall_macro': 0.5315646425081598,\n",
            " 'recall_micro': 0.7609489051094891,\n",
            " 'recall_weighted': 0.7609489051094891}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_7.pt\n",
            "Epoch: 0007, Train Loss: 1.79316, Time: 10.56578\n",
            "Val Loss: 1.73503\n",
            "Val Results: ...\n",
            "{'accuracy': 0.7937956204379562,\n",
            " 'f1_macro': 0.6010840845318606,\n",
            " 'f1_micro': 0.7937956204379562,\n",
            " 'f1_weighted': 0.8005014123845832,\n",
            " 'precision_macro': 0.738354771847235,\n",
            " 'precision_micro': 0.7937956204379562,\n",
            " 'precision_weighted': 0.8584310521988268,\n",
            " 'recall_macro': 0.63057066519037,\n",
            " 'recall_micro': 0.7937956204379562,\n",
            " 'recall_weighted': 0.7937956204379562}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_8.pt\n",
            "Epoch: 0008, Train Loss: 1.72747, Time: 10.84336\n",
            "Val Loss: 1.66848\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8284671532846716,\n",
            " 'f1_macro': 0.7077232268969844,\n",
            " 'f1_micro': 0.8284671532846716,\n",
            " 'f1_weighted': 0.8344972955812312,\n",
            " 'precision_macro': 0.7148715448823206,\n",
            " 'precision_micro': 0.8284671532846716,\n",
            " 'precision_weighted': 0.8656323911601724,\n",
            " 'recall_macro': 0.7327968322031223,\n",
            " 'recall_micro': 0.8284671532846716,\n",
            " 'recall_weighted': 0.8284671532846716}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_9.pt\n",
            "Epoch: 0009, Train Loss: 1.66008, Time: 12.36154\n",
            "Val Loss: 1.60287\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8375912408759124,\n",
            " 'f1_macro': 0.7324156985297094,\n",
            " 'f1_micro': 0.8375912408759124,\n",
            " 'f1_weighted': 0.8432078273083461,\n",
            " 'precision_macro': 0.7208710142154334,\n",
            " 'precision_micro': 0.8375912408759124,\n",
            " 'precision_weighted': 0.8689082200298138,\n",
            " 'recall_macro': 0.764366013924743,\n",
            " 'recall_micro': 0.8375912408759124,\n",
            " 'recall_weighted': 0.8375912408759124}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_10.pt\n",
            "Epoch: 0010, Train Loss: 1.58491, Time: 10.73372\n",
            "Val Loss: 1.53795\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8412408759124088,\n",
            " 'f1_macro': 0.7507279701397904,\n",
            " 'f1_micro': 0.8412408759124088,\n",
            " 'f1_weighted': 0.847660086414239,\n",
            " 'precision_macro': 0.7632359538609539,\n",
            " 'precision_micro': 0.8412408759124088,\n",
            " 'precision_weighted': 0.878640388403162,\n",
            " 'recall_macro': 0.7767757046239074,\n",
            " 'recall_micro': 0.8412408759124088,\n",
            " 'recall_weighted': 0.8412408759124088}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_11.pt\n",
            "Epoch: 0011, Train Loss: 1.51978, Time: 10.72324\n",
            "Val Loss: 1.47442\n",
            "Val Results: ...\n",
            "{'accuracy': 0.833941605839416,\n",
            " 'f1_macro': 0.7326426003710971,\n",
            " 'f1_micro': 0.8339416058394159,\n",
            " 'f1_weighted': 0.8413994767831569,\n",
            " 'precision_macro': 0.7303668842968185,\n",
            " 'precision_micro': 0.833941605839416,\n",
            " 'precision_weighted': 0.870298951401167,\n",
            " 'recall_macro': 0.7686722340204368,\n",
            " 'recall_micro': 0.833941605839416,\n",
            " 'recall_weighted': 0.833941605839416}\n",
            "Epoch: 0012, Train Loss: 1.45155, Time: 10.38900\n",
            "Val Loss: 1.40976\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8357664233576643,\n",
            " 'f1_macro': 0.7200412235199916,\n",
            " 'f1_micro': 0.8357664233576643,\n",
            " 'f1_weighted': 0.8428680833094956,\n",
            " 'precision_macro': 0.7161764873993292,\n",
            " 'precision_micro': 0.8357664233576643,\n",
            " 'precision_weighted': 0.8693741619900214,\n",
            " 'recall_macro': 0.7466375420804175,\n",
            " 'recall_micro': 0.8357664233576643,\n",
            " 'recall_weighted': 0.8357664233576643}\n",
            "Epoch: 0013, Train Loss: 1.38148, Time: 10.11910\n",
            "Val Loss: 1.34622\n",
            "Val Results: ...\n",
            "{'accuracy': 0.833941605839416,\n",
            " 'f1_macro': 0.7257140098065492,\n",
            " 'f1_micro': 0.8339416058394159,\n",
            " 'f1_weighted': 0.8407026614665712,\n",
            " 'precision_macro': 0.726351078113642,\n",
            " 'precision_micro': 0.833941605839416,\n",
            " 'precision_weighted': 0.8706360436189088,\n",
            " 'recall_macro': 0.7584558558804386,\n",
            " 'recall_micro': 0.833941605839416,\n",
            " 'recall_weighted': 0.833941605839416}\n",
            "Epoch: 0014, Train Loss: 1.31750, Time: 10.62714\n",
            "Val Loss: 1.27441\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8302919708029197,\n",
            " 'f1_macro': 0.7115022312991599,\n",
            " 'f1_micro': 0.8302919708029197,\n",
            " 'f1_weighted': 0.8363957253351164,\n",
            " 'precision_macro': 0.7114618596253668,\n",
            " 'precision_micro': 0.8302919708029197,\n",
            " 'precision_weighted': 0.8684847713603724,\n",
            " 'recall_macro': 0.7509277875051302,\n",
            " 'recall_micro': 0.8302919708029197,\n",
            " 'recall_weighted': 0.8302919708029197}\n",
            "Epoch: 0015, Train Loss: 1.24498, Time: 11.77845\n",
            "Val Loss: 1.22675\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8394160583941606,\n",
            " 'f1_macro': 0.743084232769885,\n",
            " 'f1_micro': 0.8394160583941606,\n",
            " 'f1_weighted': 0.8452728430619675,\n",
            " 'precision_macro': 0.7284749008581962,\n",
            " 'precision_micro': 0.8394160583941606,\n",
            " 'precision_weighted': 0.8737528265251363,\n",
            " 'recall_macro': 0.7872338303901205,\n",
            " 'recall_micro': 0.8394160583941606,\n",
            " 'recall_weighted': 0.8394160583941606}\n",
            "Epoch: 0016, Train Loss: 1.18265, Time: 10.70894\n",
            "Val Loss: 1.16539\n",
            "Val Results: ...\n",
            "{'accuracy': 0.843065693430657,\n",
            " 'f1_macro': 0.7574912526454429,\n",
            " 'f1_micro': 0.843065693430657,\n",
            " 'f1_weighted': 0.8485690698874541,\n",
            " 'precision_macro': 0.7308340864715133,\n",
            " 'precision_micro': 0.843065693430657,\n",
            " 'precision_weighted': 0.8761930936437373,\n",
            " 'recall_macro': 0.8350782389293301,\n",
            " 'recall_micro': 0.843065693430657,\n",
            " 'recall_weighted': 0.843065693430657}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_17.pt\n",
            "Epoch: 0017, Train Loss: 1.10528, Time: 10.47527\n",
            "Val Loss: 1.10951\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8448905109489051,\n",
            " 'f1_macro': 0.7292834270563662,\n",
            " 'f1_micro': 0.8448905109489051,\n",
            " 'f1_weighted': 0.8512028006410106,\n",
            " 'precision_macro': 0.6998307411291672,\n",
            " 'precision_micro': 0.8448905109489051,\n",
            " 'precision_weighted': 0.8745949084969693,\n",
            " 'recall_macro': 0.7926040497205452,\n",
            " 'recall_micro': 0.8448905109489051,\n",
            " 'recall_weighted': 0.8448905109489051}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_18.pt\n",
            "Epoch: 0018, Train Loss: 1.04910, Time: 10.18195\n",
            "Val Loss: 1.04599\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8521897810218978,\n",
            " 'f1_macro': 0.7375806845497879,\n",
            " 'f1_micro': 0.8521897810218978,\n",
            " 'f1_weighted': 0.8587753313261137,\n",
            " 'precision_macro': 0.7070059026052091,\n",
            " 'precision_micro': 0.8521897810218978,\n",
            " 'precision_weighted': 0.8812530519433773,\n",
            " 'recall_macro': 0.8061629050476932,\n",
            " 'recall_micro': 0.8521897810218978,\n",
            " 'recall_weighted': 0.8521897810218978}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_19.pt\n",
            "Epoch: 0019, Train Loss: 0.98330, Time: 10.56332\n",
            "Val Loss: 0.97765\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8667883211678832,\n",
            " 'f1_macro': 0.762266359739903,\n",
            " 'f1_micro': 0.8667883211678832,\n",
            " 'f1_weighted': 0.8719504354391602,\n",
            " 'precision_macro': 0.7209895915702194,\n",
            " 'precision_micro': 0.8667883211678832,\n",
            " 'precision_weighted': 0.8914283165309855,\n",
            " 'recall_macro': 0.8402596396258142,\n",
            " 'recall_micro': 0.8667883211678832,\n",
            " 'recall_weighted': 0.8667883211678832}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_20.pt\n",
            "Epoch: 0020, Train Loss: 0.92380, Time: 10.80597\n",
            "Val Loss: 0.94151\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8613138686131386,\n",
            " 'f1_macro': 0.7393445580742457,\n",
            " 'f1_micro': 0.8613138686131386,\n",
            " 'f1_weighted': 0.8665106720519772,\n",
            " 'precision_macro': 0.7046501064299964,\n",
            " 'precision_micro': 0.8613138686131386,\n",
            " 'precision_weighted': 0.8850005951982896,\n",
            " 'recall_macro': 0.8021102753211226,\n",
            " 'recall_micro': 0.8613138686131386,\n",
            " 'recall_weighted': 0.8613138686131386}\n",
            "Epoch: 0021, Train Loss: 0.85934, Time: 10.73434\n",
            "Val Loss: 0.86995\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8759124087591241,\n",
            " 'f1_macro': 0.7836962877606127,\n",
            " 'f1_micro': 0.8759124087591241,\n",
            " 'f1_weighted': 0.8808786955645272,\n",
            " 'precision_macro': 0.7510371548918426,\n",
            " 'precision_micro': 0.8759124087591241,\n",
            " 'precision_weighted': 0.8988473314623846,\n",
            " 'recall_macro': 0.8605560268429331,\n",
            " 'recall_micro': 0.8759124087591241,\n",
            " 'recall_weighted': 0.8759124087591241}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_22.pt\n",
            "Epoch: 0022, Train Loss: 0.81664, Time: 10.60665\n",
            "Val Loss: 0.84488\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8686131386861314,\n",
            " 'f1_macro': 0.7497693634447365,\n",
            " 'f1_micro': 0.8686131386861314,\n",
            " 'f1_weighted': 0.8739002171824375,\n",
            " 'precision_macro': 0.717769383959143,\n",
            " 'precision_micro': 0.8686131386861314,\n",
            " 'precision_weighted': 0.8895264883799313,\n",
            " 'recall_macro': 0.8078114122481901,\n",
            " 'recall_micro': 0.8686131386861314,\n",
            " 'recall_weighted': 0.8686131386861314}\n",
            "Epoch: 0023, Train Loss: 0.75992, Time: 10.00292\n",
            "Val Loss: 0.79058\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8740875912408759,\n",
            " 'f1_macro': 0.7501980380387496,\n",
            " 'f1_micro': 0.8740875912408759,\n",
            " 'f1_weighted': 0.881058527450276,\n",
            " 'precision_macro': 0.7195645659461449,\n",
            " 'precision_micro': 0.8740875912408759,\n",
            " 'precision_weighted': 0.8997502155204805,\n",
            " 'recall_macro': 0.826669176340229,\n",
            " 'recall_micro': 0.8740875912408759,\n",
            " 'recall_weighted': 0.8740875912408759}\n",
            "Epoch: 0024, Train Loss: 0.72217, Time: 10.46315\n",
            "Val Loss: 0.74894\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8740875912408759,\n",
            " 'f1_macro': 0.7599370246468806,\n",
            " 'f1_micro': 0.8740875912408759,\n",
            " 'f1_weighted': 0.8789034829269566,\n",
            " 'precision_macro': 0.7197612230760159,\n",
            " 'precision_micro': 0.8740875912408759,\n",
            " 'precision_weighted': 0.8941539958745903,\n",
            " 'recall_macro': 0.8302285662442914,\n",
            " 'recall_micro': 0.8740875912408759,\n",
            " 'recall_weighted': 0.8740875912408759}\n",
            "Epoch: 0025, Train Loss: 0.65388, Time: 10.64431\n",
            "Val Loss: 0.72222\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8905109489051095,\n",
            " 'f1_macro': 0.7980782626239746,\n",
            " 'f1_micro': 0.8905109489051094,\n",
            " 'f1_weighted': 0.8941284247095914,\n",
            " 'precision_macro': 0.7644845975280757,\n",
            " 'precision_micro': 0.8905109489051095,\n",
            " 'precision_weighted': 0.9072592543744559,\n",
            " 'recall_macro': 0.8697575863493706,\n",
            " 'recall_micro': 0.8905109489051095,\n",
            " 'recall_weighted': 0.8905109489051095}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_26.pt\n",
            "Epoch: 0026, Train Loss: 0.61278, Time: 10.69493\n",
            "Val Loss: 0.69759\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8905109489051095,\n",
            " 'f1_macro': 0.751034283288246,\n",
            " 'f1_micro': 0.8905109489051094,\n",
            " 'f1_weighted': 0.8959863554662552,\n",
            " 'precision_macro': 0.7281619277823724,\n",
            " 'precision_micro': 0.8905109489051095,\n",
            " 'precision_weighted': 0.9096892594430288,\n",
            " 'recall_macro': 0.8004621098010058,\n",
            " 'recall_micro': 0.8905109489051095,\n",
            " 'recall_weighted': 0.8905109489051095}\n",
            "Epoch: 0027, Train Loss: 0.57682, Time: 11.68026\n",
            "Val Loss: 0.63276\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8978102189781022,\n",
            " 'f1_macro': 0.7992690117408728,\n",
            " 'f1_micro': 0.8978102189781021,\n",
            " 'f1_weighted': 0.9012606794526065,\n",
            " 'precision_macro': 0.7635961148025101,\n",
            " 'precision_micro': 0.8978102189781022,\n",
            " 'precision_weighted': 0.9122283285237368,\n",
            " 'recall_macro': 0.8603161324839759,\n",
            " 'recall_micro': 0.8978102189781022,\n",
            " 'recall_weighted': 0.8978102189781022}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_28.pt\n",
            "Epoch: 0028, Train Loss: 0.53922, Time: 10.76766\n",
            "Val Loss: 0.62463\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8886861313868614,\n",
            " 'f1_macro': 0.7741681507236269,\n",
            " 'f1_micro': 0.8886861313868614,\n",
            " 'f1_weighted': 0.892602533499767,\n",
            " 'precision_macro': 0.7368125180773071,\n",
            " 'precision_micro': 0.8886861313868614,\n",
            " 'precision_weighted': 0.9045305459767795,\n",
            " 'recall_macro': 0.8377154311124144,\n",
            " 'recall_micro': 0.8886861313868614,\n",
            " 'recall_weighted': 0.8886861313868614}\n",
            "Epoch: 0029, Train Loss: 0.50465, Time: 10.22617\n",
            "Val Loss: 0.59766\n",
            "Val Results: ...\n",
            "{'accuracy': 0.8941605839416058,\n",
            " 'f1_macro': 0.7729071842205102,\n",
            " 'f1_micro': 0.8941605839416058,\n",
            " 'f1_weighted': 0.9016985629123309,\n",
            " 'precision_macro': 0.7520006526041009,\n",
            " 'precision_micro': 0.8941605839416058,\n",
            " 'precision_weighted': 0.9180671094294302,\n",
            " 'recall_macro': 0.8458266336074424,\n",
            " 'recall_micro': 0.8941605839416058,\n",
            " 'recall_weighted': 0.8941605839416058}\n",
            "Epoch: 0030, Train Loss: 0.46103, Time: 10.57239\n",
            "Val Loss: 0.56376\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9032846715328468,\n",
            " 'f1_macro': 0.7934441012194846,\n",
            " 'f1_micro': 0.9032846715328468,\n",
            " 'f1_weighted': 0.9076892235778368,\n",
            " 'precision_macro': 0.7621513902763902,\n",
            " 'precision_micro': 0.9032846715328468,\n",
            " 'precision_weighted': 0.919298174453284,\n",
            " 'recall_macro': 0.860509051579334,\n",
            " 'recall_micro': 0.9032846715328468,\n",
            " 'recall_weighted': 0.9032846715328468}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_31.pt\n",
            "Epoch: 0031, Train Loss: 0.43966, Time: 10.91301\n",
            "Val Loss: 0.53809\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9087591240875912,\n",
            " 'f1_macro': 0.7975232647786996,\n",
            " 'f1_micro': 0.9087591240875912,\n",
            " 'f1_weighted': 0.9123072110635583,\n",
            " 'precision_macro': 0.7629661958765134,\n",
            " 'precision_micro': 0.9087591240875912,\n",
            " 'precision_weighted': 0.9205896122364222,\n",
            " 'recall_macro': 0.859178821285689,\n",
            " 'recall_micro': 0.9087591240875912,\n",
            " 'recall_weighted': 0.9087591240875912}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_32.pt\n",
            "Epoch: 0032, Train Loss: 0.42177, Time: 10.84053\n",
            "Val Loss: 0.50030\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9124087591240876,\n",
            " 'f1_macro': 0.8051099399703396,\n",
            " 'f1_micro': 0.9124087591240876,\n",
            " 'f1_weighted': 0.915794440486366,\n",
            " 'precision_macro': 0.7733169281698693,\n",
            " 'precision_micro': 0.9124087591240876,\n",
            " 'precision_weighted': 0.9249215540434947,\n",
            " 'recall_macro': 0.862163799270667,\n",
            " 'recall_micro': 0.9124087591240876,\n",
            " 'recall_weighted': 0.9124087591240876}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_33.pt\n",
            "Epoch: 0033, Train Loss: 0.38775, Time: 10.73784\n",
            "Val Loss: 0.49789\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9142335766423357,\n",
            " 'f1_macro': 0.813098154146283,\n",
            " 'f1_micro': 0.9142335766423357,\n",
            " 'f1_weighted': 0.9171831351027157,\n",
            " 'precision_macro': 0.7897537404116352,\n",
            " 'precision_micro': 0.9142335766423357,\n",
            " 'precision_weighted': 0.9248694064291453,\n",
            " 'recall_macro': 0.8579790814306218,\n",
            " 'recall_micro': 0.9142335766423357,\n",
            " 'recall_weighted': 0.9142335766423357}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_34.pt\n",
            "Epoch: 0034, Train Loss: 0.37303, Time: 10.38144\n",
            "Val Loss: 0.47846\n",
            "Val Results: ...\n",
            "{'accuracy': 0.906934306569343,\n",
            " 'f1_macro': 0.7903071084564219,\n",
            " 'f1_micro': 0.906934306569343,\n",
            " 'f1_weighted': 0.9117184100324374,\n",
            " 'precision_macro': 0.7705672696428034,\n",
            " 'precision_micro': 0.906934306569343,\n",
            " 'precision_weighted': 0.9229740795574531,\n",
            " 'recall_macro': 0.8599814771830177,\n",
            " 'recall_micro': 0.906934306569343,\n",
            " 'recall_weighted': 0.906934306569343}\n",
            "Epoch: 0035, Train Loss: 0.35661, Time: 10.38726\n",
            "Val Loss: 0.46662\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9142335766423357,\n",
            " 'f1_macro': 0.8150266138889619,\n",
            " 'f1_micro': 0.9142335766423357,\n",
            " 'f1_weighted': 0.9180316241180406,\n",
            " 'precision_macro': 0.7902282590681471,\n",
            " 'precision_micro': 0.9142335766423357,\n",
            " 'precision_weighted': 0.9295541043744939,\n",
            " 'recall_macro': 0.8886289612541216,\n",
            " 'recall_micro': 0.9142335766423357,\n",
            " 'recall_weighted': 0.9142335766423357}\n",
            "Epoch: 0036, Train Loss: 0.33287, Time: 10.64420\n",
            "Val Loss: 0.44541\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9178832116788321,\n",
            " 'f1_macro': 0.8118944082388608,\n",
            " 'f1_micro': 0.9178832116788321,\n",
            " 'f1_weighted': 0.9202740564851339,\n",
            " 'precision_macro': 0.7806213187902151,\n",
            " 'precision_micro': 0.9178832116788321,\n",
            " 'precision_weighted': 0.9268629625665242,\n",
            " 'recall_macro': 0.8639933721368254,\n",
            " 'recall_micro': 0.9178832116788321,\n",
            " 'recall_weighted': 0.9178832116788321}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_37.pt\n",
            "Epoch: 0037, Train Loss: 0.31889, Time: 10.84470\n",
            "Val Loss: 0.42309\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9178832116788321,\n",
            " 'f1_macro': 0.8212149223381041,\n",
            " 'f1_micro': 0.9178832116788321,\n",
            " 'f1_weighted': 0.9217675141970186,\n",
            " 'precision_macro': 0.7899030013145508,\n",
            " 'precision_micro': 0.9178832116788321,\n",
            " 'precision_weighted': 0.9310800761878534,\n",
            " 'recall_macro': 0.8745961952396484,\n",
            " 'recall_micro': 0.9178832116788321,\n",
            " 'recall_weighted': 0.9178832116788321}\n",
            "Epoch: 0038, Train Loss: 0.29991, Time: 10.59067\n",
            "Val Loss: 0.41316\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9105839416058394,\n",
            " 'f1_macro': 0.7938251228913867,\n",
            " 'f1_micro': 0.9105839416058394,\n",
            " 'f1_weighted': 0.9143209642961231,\n",
            " 'precision_macro': 0.7641600857864698,\n",
            " 'precision_micro': 0.9105839416058394,\n",
            " 'precision_weighted': 0.9231415167152031,\n",
            " 'recall_macro': 0.8503811938905006,\n",
            " 'recall_micro': 0.9105839416058394,\n",
            " 'recall_weighted': 0.9105839416058394}\n",
            "Epoch: 0039, Train Loss: 0.29440, Time: 10.40810\n",
            "Val Loss: 0.37927\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9215328467153284,\n",
            " 'f1_macro': 0.8196570841538393,\n",
            " 'f1_micro': 0.9215328467153284,\n",
            " 'f1_weighted': 0.9245724255844946,\n",
            " 'precision_macro': 0.7883151756117661,\n",
            " 'precision_micro': 0.9215328467153284,\n",
            " 'precision_weighted': 0.9313352152174313,\n",
            " 'recall_macro': 0.8754099906298237,\n",
            " 'recall_micro': 0.9215328467153284,\n",
            " 'recall_weighted': 0.9215328467153284}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_40.pt\n",
            "Epoch: 0040, Train Loss: 0.26793, Time: 10.95098\n",
            "Val Loss: 0.40426\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9178832116788321,\n",
            " 'f1_macro': 0.8097226909970698,\n",
            " 'f1_micro': 0.9178832116788321,\n",
            " 'f1_weighted': 0.9208697969148942,\n",
            " 'precision_macro': 0.7990605735475138,\n",
            " 'precision_micro': 0.9178832116788321,\n",
            " 'precision_weighted': 0.9297603966345115,\n",
            " 'recall_macro': 0.849314191668171,\n",
            " 'recall_micro': 0.9178832116788321,\n",
            " 'recall_weighted': 0.9178832116788321}\n",
            "Epoch: 0041, Train Loss: 0.26375, Time: 10.31944\n",
            "Val Loss: 0.39883\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9124087591240876,\n",
            " 'f1_macro': 0.8120556748511207,\n",
            " 'f1_micro': 0.9124087591240876,\n",
            " 'f1_weighted': 0.9152717204457362,\n",
            " 'precision_macro': 0.7890183635033495,\n",
            " 'precision_micro': 0.9124087591240876,\n",
            " 'precision_weighted': 0.9230278888950084,\n",
            " 'recall_macro': 0.8575435413609354,\n",
            " 'recall_micro': 0.9124087591240876,\n",
            " 'recall_weighted': 0.9124087591240876}\n",
            "Epoch: 0042, Train Loss: 0.24551, Time: 10.55458\n",
            "Val Loss: 0.39349\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9124087591240876,\n",
            " 'f1_macro': 0.8342868284973058,\n",
            " 'f1_micro': 0.9124087591240876,\n",
            " 'f1_weighted': 0.9150925184734797,\n",
            " 'precision_macro': 0.8115505262095918,\n",
            " 'precision_micro': 0.9124087591240876,\n",
            " 'precision_weighted': 0.9236377606610862,\n",
            " 'recall_macro': 0.8865128162143566,\n",
            " 'recall_micro': 0.9124087591240876,\n",
            " 'recall_weighted': 0.9124087591240876}\n",
            "Epoch: 0043, Train Loss: 0.23816, Time: 10.50260\n",
            "Val Loss: 0.40038\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9251824817518248,\n",
            " 'f1_macro': 0.8379418102013021,\n",
            " 'f1_micro': 0.9251824817518248,\n",
            " 'f1_weighted': 0.9272495430347094,\n",
            " 'precision_macro': 0.8205849510530366,\n",
            " 'precision_micro': 0.9251824817518248,\n",
            " 'precision_weighted': 0.9329816840088582,\n",
            " 'recall_macro': 0.8720869812092533,\n",
            " 'recall_micro': 0.9251824817518248,\n",
            " 'recall_weighted': 0.9251824817518248}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_44.pt\n",
            "Epoch: 0044, Train Loss: 0.22589, Time: 10.46385\n",
            "Val Loss: 0.34297\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9288321167883211,\n",
            " 'f1_macro': 0.8568896912050437,\n",
            " 'f1_micro': 0.9288321167883211,\n",
            " 'f1_weighted': 0.9306282008762737,\n",
            " 'precision_macro': 0.8208676652612579,\n",
            " 'precision_micro': 0.9288321167883211,\n",
            " 'precision_weighted': 0.9354912822890824,\n",
            " 'recall_macro': 0.9167000207276201,\n",
            " 'recall_micro': 0.9288321167883211,\n",
            " 'recall_weighted': 0.9288321167883211}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_45.pt\n",
            "Epoch: 0045, Train Loss: 0.21351, Time: 10.16376\n",
            "Val Loss: 0.35463\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9288321167883211,\n",
            " 'f1_macro': 0.8347611075675048,\n",
            " 'f1_micro': 0.9288321167883211,\n",
            " 'f1_weighted': 0.9309861091160031,\n",
            " 'precision_macro': 0.8089803660557113,\n",
            " 'precision_micro': 0.9288321167883211,\n",
            " 'precision_weighted': 0.935812557011198,\n",
            " 'recall_macro': 0.8756729527188449,\n",
            " 'recall_micro': 0.9288321167883211,\n",
            " 'recall_weighted': 0.9288321167883211}\n",
            "Epoch: 0046, Train Loss: 0.21184, Time: 10.23398\n",
            "Val Loss: 0.36990\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9233576642335767,\n",
            " 'f1_macro': 0.8237113274264978,\n",
            " 'f1_micro': 0.9233576642335767,\n",
            " 'f1_weighted': 0.9252218469103398,\n",
            " 'precision_macro': 0.7926113480963229,\n",
            " 'precision_micro': 0.9233576642335767,\n",
            " 'precision_weighted': 0.928839368161933,\n",
            " 'recall_macro': 0.8644175278080927,\n",
            " 'recall_micro': 0.9233576642335767,\n",
            " 'recall_weighted': 0.9233576642335767}\n",
            "Epoch: 0047, Train Loss: 0.19860, Time: 10.62104\n",
            "Val Loss: 0.33939\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9288321167883211,\n",
            " 'f1_macro': 0.8478642811409873,\n",
            " 'f1_micro': 0.9288321167883211,\n",
            " 'f1_weighted': 0.9306205929883778,\n",
            " 'precision_macro': 0.8257170580560914,\n",
            " 'precision_micro': 0.9288321167883211,\n",
            " 'precision_weighted': 0.934901586198681,\n",
            " 'recall_macro': 0.8850499441722163,\n",
            " 'recall_micro': 0.9288321167883211,\n",
            " 'recall_weighted': 0.9288321167883211}\n",
            "Epoch: 0048, Train Loss: 0.19714, Time: 10.78512\n",
            "Val Loss: 0.33068\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9215328467153284,\n",
            " 'f1_macro': 0.8388788922489244,\n",
            " 'f1_micro': 0.9215328467153284,\n",
            " 'f1_weighted': 0.923362385283285,\n",
            " 'precision_macro': 0.8233661517940112,\n",
            " 'precision_micro': 0.9215328467153284,\n",
            " 'precision_weighted': 0.9292022350157508,\n",
            " 'recall_macro': 0.8669083071281403,\n",
            " 'recall_micro': 0.9215328467153284,\n",
            " 'recall_weighted': 0.9215328467153284}\n",
            "Epoch: 0049, Train Loss: 0.18577, Time: 10.69140\n",
            "Val Loss: 0.33264\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9251824817518248,\n",
            " 'f1_macro': 0.8635416125466238,\n",
            " 'f1_micro': 0.9251824817518248,\n",
            " 'f1_weighted': 0.9266569567989166,\n",
            " 'precision_macro': 0.838650705446371,\n",
            " 'precision_micro': 0.9251824817518248,\n",
            " 'precision_weighted': 0.9321472454048554,\n",
            " 'recall_macro': 0.9025854969211837,\n",
            " 'recall_micro': 0.9251824817518248,\n",
            " 'recall_weighted': 0.9251824817518248}\n",
            "Epoch: 0050, Train Loss: 0.17514, Time: 10.76949\n",
            "Val Loss: 0.34798\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9251824817518248,\n",
            " 'f1_macro': 0.8495720672835695,\n",
            " 'f1_micro': 0.9251824817518248,\n",
            " 'f1_weighted': 0.9262188273864148,\n",
            " 'precision_macro': 0.8291763235358016,\n",
            " 'precision_micro': 0.9251824817518248,\n",
            " 'precision_weighted': 0.9299754910273322,\n",
            " 'recall_macro': 0.876603106457086,\n",
            " 'recall_micro': 0.9251824817518248,\n",
            " 'recall_weighted': 0.9251824817518248}\n",
            "Epoch: 0051, Train Loss: 0.17203, Time: 9.89249\n",
            "Val Loss: 0.33359\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9324817518248175,\n",
            " 'f1_macro': 0.8387093163644037,\n",
            " 'f1_micro': 0.9324817518248175,\n",
            " 'f1_weighted': 0.9336926974723672,\n",
            " 'precision_macro': 0.8312359891385972,\n",
            " 'precision_micro': 0.9324817518248175,\n",
            " 'precision_weighted': 0.938044543869587,\n",
            " 'recall_macro': 0.8537578566301288,\n",
            " 'recall_micro': 0.9324817518248175,\n",
            " 'recall_weighted': 0.9324817518248175}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_52.pt\n",
            "Epoch: 0052, Train Loss: 0.16425, Time: 11.45120\n",
            "Val Loss: 0.29431\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9361313868613139,\n",
            " 'f1_macro': 0.869356557268899,\n",
            " 'f1_micro': 0.9361313868613139,\n",
            " 'f1_weighted': 0.9374774595798209,\n",
            " 'precision_macro': 0.8453752180881998,\n",
            " 'precision_micro': 0.9361313868613139,\n",
            " 'precision_weighted': 0.9411855500254027,\n",
            " 'recall_macro': 0.9164928632675744,\n",
            " 'recall_micro': 0.9361313868613139,\n",
            " 'recall_weighted': 0.9361313868613139}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_53.pt\n",
            "Epoch: 0053, Train Loss: 0.16089, Time: 10.84258\n",
            "Val Loss: 0.32494\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9379562043795621,\n",
            " 'f1_macro': 0.8596247078001489,\n",
            " 'f1_micro': 0.9379562043795621,\n",
            " 'f1_weighted': 0.9394539227148363,\n",
            " 'precision_macro': 0.8504383698659689,\n",
            " 'precision_micro': 0.9379562043795621,\n",
            " 'precision_weighted': 0.9434707144370122,\n",
            " 'recall_macro': 0.8810407758337797,\n",
            " 'recall_micro': 0.9379562043795621,\n",
            " 'recall_weighted': 0.9379562043795621}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_54.pt\n",
            "Epoch: 0054, Train Loss: 0.15330, Time: 10.75632\n",
            "Val Loss: 0.27732\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9434306569343066,\n",
            " 'f1_macro': 0.8875573038117617,\n",
            " 'f1_micro': 0.9434306569343066,\n",
            " 'f1_weighted': 0.9443459066935413,\n",
            " 'precision_macro': 0.8688498264090732,\n",
            " 'precision_micro': 0.9434306569343066,\n",
            " 'precision_weighted': 0.947134031435504,\n",
            " 'recall_macro': 0.9196707603479105,\n",
            " 'recall_micro': 0.9434306569343066,\n",
            " 'recall_weighted': 0.9434306569343066}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_55.pt\n",
            "Epoch: 0055, Train Loss: 0.15063, Time: 10.78263\n",
            "Val Loss: 0.26094\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9434306569343066,\n",
            " 'f1_macro': 0.8921401402536105,\n",
            " 'f1_micro': 0.9434306569343066,\n",
            " 'f1_weighted': 0.9446686952254557,\n",
            " 'precision_macro': 0.8759721901412653,\n",
            " 'precision_micro': 0.9434306569343066,\n",
            " 'precision_weighted': 0.9487059186752237,\n",
            " 'recall_macro': 0.9280589394677969,\n",
            " 'recall_micro': 0.9434306569343066,\n",
            " 'recall_weighted': 0.9434306569343066}\n",
            "Epoch: 0056, Train Loss: 0.14472, Time: 10.68452\n",
            "Val Loss: 0.30274\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9306569343065694,\n",
            " 'f1_macro': 0.8537485170549686,\n",
            " 'f1_micro': 0.9306569343065695,\n",
            " 'f1_weighted': 0.9322204322792974,\n",
            " 'precision_macro': 0.8412724229512083,\n",
            " 'precision_micro': 0.9306569343065694,\n",
            " 'precision_weighted': 0.9370569668710075,\n",
            " 'recall_macro': 0.8773058364281086,\n",
            " 'recall_micro': 0.9306569343065694,\n",
            " 'recall_weighted': 0.9306569343065694}\n",
            "Epoch: 0057, Train Loss: 0.14189, Time: 9.96000\n",
            "Val Loss: 0.31458\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9306569343065694,\n",
            " 'f1_macro': 0.83149562806026,\n",
            " 'f1_micro': 0.9306569343065695,\n",
            " 'f1_weighted': 0.9320357020651729,\n",
            " 'precision_macro': 0.8264419856837053,\n",
            " 'precision_micro': 0.9306569343065694,\n",
            " 'precision_weighted': 0.9364181466294248,\n",
            " 'recall_macro': 0.8461693408099057,\n",
            " 'recall_micro': 0.9306569343065694,\n",
            " 'recall_weighted': 0.9306569343065694}\n",
            "Epoch: 0058, Train Loss: 0.13698, Time: 10.66630\n",
            "Val Loss: 0.31469\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9379562043795621,\n",
            " 'f1_macro': 0.8542146904307346,\n",
            " 'f1_micro': 0.9379562043795621,\n",
            " 'f1_weighted': 0.9385877579594817,\n",
            " 'precision_macro': 0.8525247322122322,\n",
            " 'precision_micro': 0.9379562043795621,\n",
            " 'precision_weighted': 0.9417027341069313,\n",
            " 'recall_macro': 0.861573381213946,\n",
            " 'recall_micro': 0.9379562043795621,\n",
            " 'recall_weighted': 0.9379562043795621}\n",
            "Epoch: 0059, Train Loss: 0.13153, Time: 10.75512\n",
            "Val Loss: 0.29584\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9416058394160584,\n",
            " 'f1_macro': 0.881350963168887,\n",
            " 'f1_micro': 0.9416058394160584,\n",
            " 'f1_weighted': 0.9428445921439279,\n",
            " 'precision_macro': 0.8651751256507292,\n",
            " 'precision_micro': 0.9416058394160584,\n",
            " 'precision_weighted': 0.9468718560767234,\n",
            " 'recall_macro': 0.9172424411512987,\n",
            " 'recall_micro': 0.9416058394160584,\n",
            " 'recall_weighted': 0.9416058394160584}\n",
            "Epoch: 0060, Train Loss: 0.12776, Time: 10.78783\n",
            "Val Loss: 0.28868\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9397810218978102,\n",
            " 'f1_macro': 0.8700771874871688,\n",
            " 'f1_micro': 0.9397810218978102,\n",
            " 'f1_weighted': 0.940985963151425,\n",
            " 'precision_macro': 0.8644543753474019,\n",
            " 'precision_micro': 0.9397810218978102,\n",
            " 'precision_weighted': 0.9459022737695659,\n",
            " 'recall_macro': 0.8881937515843163,\n",
            " 'recall_micro': 0.9397810218978102,\n",
            " 'recall_weighted': 0.9397810218978102}\n",
            "Epoch: 0061, Train Loss: 0.12703, Time: 10.91908\n",
            "Val Loss: 0.26761\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9397810218978102,\n",
            " 'f1_macro': 0.8947378951327958,\n",
            " 'f1_micro': 0.9397810218978102,\n",
            " 'f1_weighted': 0.9405534663515268,\n",
            " 'precision_macro': 0.8846310061770588,\n",
            " 'precision_micro': 0.9397810218978102,\n",
            " 'precision_weighted': 0.9440476340615602,\n",
            " 'recall_macro': 0.90878731790959,\n",
            " 'recall_micro': 0.9397810218978102,\n",
            " 'recall_weighted': 0.9397810218978102}\n",
            "Epoch: 0062, Train Loss: 0.12459, Time: 10.48594\n",
            "Val Loss: 0.28519\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9416058394160584,\n",
            " 'f1_macro': 0.8712125629755887,\n",
            " 'f1_micro': 0.9416058394160584,\n",
            " 'f1_weighted': 0.9423039364238092,\n",
            " 'precision_macro': 0.8670741247057037,\n",
            " 'precision_micro': 0.9416058394160584,\n",
            " 'precision_weighted': 0.9450729954725345,\n",
            " 'recall_macro': 0.8817642619231194,\n",
            " 'recall_micro': 0.9416058394160584,\n",
            " 'recall_weighted': 0.9416058394160584}\n",
            "Epoch: 0063, Train Loss: 0.11753, Time: 10.06230\n",
            "Val Loss: 0.25408\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9452554744525548,\n",
            " 'f1_macro': 0.8901959439826659,\n",
            " 'f1_micro': 0.9452554744525548,\n",
            " 'f1_weighted': 0.9459511823640149,\n",
            " 'precision_macro': 0.8746965164268176,\n",
            " 'precision_micro': 0.9452554744525548,\n",
            " 'precision_weighted': 0.9489724701553315,\n",
            " 'recall_macro': 0.9187575926664502,\n",
            " 'recall_micro': 0.9452554744525548,\n",
            " 'recall_weighted': 0.9452554744525548}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_r8_presplit_0.8_0.1_0.1_2024-04-20T19-31-05.516628/trained_model_epoch_64.pt\n",
            "Epoch: 0064, Train Loss: 0.11820, Time: 10.64599\n",
            "Val Loss: 0.27417\n",
            "Val Results: ...\n",
            "{'accuracy': 0.9361313868613139,\n",
            " 'f1_macro': 0.854216361461416,\n",
            " 'f1_micro': 0.9361313868613139,\n",
            " 'f1_weighted': 0.9379927770094966,\n",
            " 'precision_macro': 0.8473836705734925,\n",
            " 'precision_micro': 0.9361313868613139,\n",
            " 'precision_weighted': 0.9434403419454604,\n",
            " 'recall_macro': 0.8739487373393021,\n",
            " 'recall_micro': 0.9361313868613139,\n",
            " 'recall_weighted': 0.9361313868613139}\n",
            "Test...\n",
            "{'accuracy': 0.9616263133851074,\n",
            " 'f1_macro': 0.8893745280034435,\n",
            " 'f1_micro': 0.9616263133851074,\n",
            " 'f1_weighted': 0.9619229289859961,\n",
            " 'precision_macro': 0.8748265944629132,\n",
            " 'precision_micro': 0.9616263133851074,\n",
            " 'precision_weighted': 0.9634436650018985,\n",
            " 'recall_macro': 0.9156410338505873,\n",
            " 'recall_micro': 0.9616263133851074,\n",
            " 'recall_weighted': 0.9616263133851074}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "\"\"\"\n",
        "Most Relevant\n",
        "\"\"\"\n",
        "\n",
        "debug = False\n",
        "gpu = -1\n",
        "use_comet_ml = False\n",
        "\n",
        "\"\"\"\n",
        "dataset:\n",
        " sentiment suffix for twitter means the negative classes of the original dataset are combined and the other classes are combined for sentiment analysis\n",
        " presplit suffix means training and test are predetermined in [dataset]_labels.txt\n",
        " small suffix means a very small dataset used for debugging\n",
        "\"\"\"\n",
        "random_seed = 123\n",
        "# dataset = 'r8_presplit'\n",
        "# dataset = 'ag_presplit'\n",
        "dataset = 'twitter_asian_prejudice'\n",
        "if 'twitter_asian_prejudice' in dataset:\n",
        "    if 'sentiment' in dataset:\n",
        "        num_labels = 2\n",
        "    else:\n",
        "        num_labels = 4\n",
        "elif 'r8' in dataset:\n",
        "    num_labels = 8\n",
        "\n",
        "\"\"\"\n",
        "Model. Pt1\n",
        "\"\"\"\n",
        "\n",
        "model = \"TextGNN\"\n",
        "\n",
        "model_params = {}\n",
        "use_edge_weights = False\n",
        "init_type = 'one_hot_init'\n",
        "if model == 'TextGNN':\n",
        "    pred_type = 'softmax'\n",
        "    node_embd_type = 'gcn'\n",
        "    layer_dim_list = [200, num_labels]\n",
        "    num_layers = len(layer_dim_list)\n",
        "    class_weights = True\n",
        "    dropout = True\n",
        "    s = 'TextGCN:pred_type={},node_embd_type={},num_layers={},layer_dim_list={},act={},' \\\n",
        "        'dropout={},class_weights={}'.format(\n",
        "        pred_type, node_embd_type, num_layers, \"_\".join([str(i) for i in layer_dim_list]), 'relu', dropout, class_weights\n",
        "    )\n",
        "    model_params = {\n",
        "        'pred_type': pred_type,\n",
        "        'node_embd':  node_embd_type,\n",
        "        'num_layers': num_layers,\n",
        "        'layer_dims': layer_dim_list,\n",
        "        'act': 'relu',\n",
        "        'class_weights': class_weights,\n",
        "        'dropout': dropout\n",
        "    }\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "print(\"{}: {}\\n\".format(model, model_params))\n",
        "\n",
        "\"\"\"\n",
        "Sampling\n",
        "\"\"\"\n",
        "word_window_size = 10\n",
        "validation_window_size = 5\n",
        "\n",
        "\"\"\"\n",
        "Validation\n",
        "\"\"\"\n",
        "validation_metric = \"accuracy\"  # Alternatively, \"f1_weighted\" or \"loss\"\n",
        "\n",
        "use_best_val_model_for_inference = True\n",
        "\n",
        "\"\"\"\n",
        "Evaluation.\n",
        "\"\"\"\n",
        "tvt_ratio = [0.8, 0.1, 0.1]\n",
        "tvt_list = [\"train\", \"test\", \"val\"]\n",
        "\n",
        "\"\"\"\n",
        "Optimization.\n",
        "\"\"\"\n",
        "\n",
        "lr = 1e-2\n",
        "\n",
        "device = 'cuda'#.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu'\n",
        "\n",
        "num_epochs = 2 if debug else 400\n",
        "\n",
        "\"\"\"\n",
        "Other info.\n",
        "\"\"\"\n",
        "# Assuming get_user() and get_host() are function calls that need to be defined or imported\n",
        "user = get_user()\n",
        "hostname = get_host()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEbqPvVekAco",
        "outputId": "85b7204e-4fac-4a4a-ebe7-0c46f8a61b96"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextGNN: {'pred_type': 'softmax', 'node_embd': 'gcn', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'twitter_asian_prejudice'\n",
        "num_experiments = 5\n",
        "random_seeds = [33, 15, 86, 109, 78]\n",
        "model = \"TextGNN\"\n",
        "all_experiment_results = []\n",
        "for exp in range(num_experiments):\n",
        "    random_seed = random_seeds[exp]\n",
        "    saver = Saver()\n",
        "    train_data, val_data, test_data, raw_doc_list = load_data()\n",
        "\n",
        "    saved_model, model = train(train_data, val_data, saver, False)\n",
        "    with torch.no_grad():\n",
        "        test_loss_model, preds_model = model(train_data.get_pyg_graph(device=device), test_data)\n",
        "    eval_res = eval(preds_model, test_data, True)\n",
        "    y_true = eval_res.pop('y_true')\n",
        "    y_pred = eval_res.pop('y_pred')\n",
        "    print(\"Test...experiment \", exp+1)\n",
        "    pprint(eval_res)\n",
        "    all_experiment_results.append(eval_res)\n",
        "    model = \"TextGNN\"\n",
        "\n",
        "# Calculate mean and standard deviation across experiments\n",
        "final_metrics = {key: [] for key in all_experiment_results[0]}\n",
        "for results in all_experiment_results:\n",
        "    for key in results:\n",
        "        final_metrics[key].append(results[key])\n",
        "\n",
        "for metric in final_metrics:\n",
        "    values = np.array(final_metrics[metric])\n",
        "    mean = values.mean()\n",
        "    std = values.std()\n",
        "    print(f'{metric}: Mean={mean:.6f}, Std={std:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os6eAct9DmA3",
        "outputId": "3a5f38de-5372-4ec9-bc9d-8fc36b156d9d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/twitter_asian_prejudice_train_80_val_10_test_10_seed_33_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Test...experiment  1\n",
            "{'accuracy': 0.7275,\n",
            " 'f1_macro': 0.5388396215146554,\n",
            " 'f1_micro': 0.7275000000000001,\n",
            " 'f1_weighted': 0.7259580163025291,\n",
            " 'precision_macro': 0.5765849791583935,\n",
            " 'precision_micro': 0.7275,\n",
            " 'precision_weighted': 0.7526247708699015,\n",
            " 'recall_macro': 0.5479557096231675,\n",
            " 'recall_micro': 0.7275,\n",
            " 'recall_weighted': 0.7275}\n",
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/twitter_asian_prejudice_train_80_val_10_test_10_seed_15_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Test...experiment  2\n",
            "{'accuracy': 0.6575,\n",
            " 'f1_macro': 0.49442946564006485,\n",
            " 'f1_micro': 0.6575,\n",
            " 'f1_weighted': 0.6752629715901738,\n",
            " 'precision_macro': 0.529340320833491,\n",
            " 'precision_micro': 0.6575,\n",
            " 'precision_weighted': 0.7633236488934455,\n",
            " 'recall_macro': 0.6102427991815175,\n",
            " 'recall_micro': 0.6575,\n",
            " 'recall_weighted': 0.6575}\n",
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/twitter_asian_prejudice_train_80_val_10_test_10_seed_86_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_3.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_4.pt\n",
            "Test...experiment  3\n",
            "{'accuracy': 0.73,\n",
            " 'f1_macro': 0.5455897991565489,\n",
            " 'f1_micro': 0.7299999999999999,\n",
            " 'f1_weighted': 0.7249912935860501,\n",
            " 'precision_macro': 0.547010313974402,\n",
            " 'precision_micro': 0.73,\n",
            " 'precision_weighted': 0.7252545045197715,\n",
            " 'recall_macro': 0.5537890173758897,\n",
            " 'recall_micro': 0.73,\n",
            " 'recall_weighted': 0.73}\n",
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/twitter_asian_prejudice_train_80_val_10_test_10_seed_109_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Test...experiment  4\n",
            "{'accuracy': 0.695,\n",
            " 'f1_macro': 0.5147632280525927,\n",
            " 'f1_micro': 0.695,\n",
            " 'f1_weighted': 0.7138191047953039,\n",
            " 'precision_macro': 0.5278422824728023,\n",
            " 'precision_micro': 0.695,\n",
            " 'precision_weighted': 0.770867632003588,\n",
            " 'recall_macro': 0.5938965745388338,\n",
            " 'recall_micro': 0.695,\n",
            " 'recall_weighted': 0.695}\n",
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/twitter_asian_prejudice_train_80_val_10_test_10_seed_78_window_size_10.klepto\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_5.pt\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_6.pt\n",
            "Test...experiment  5\n",
            "{'accuracy': 0.706,\n",
            " 'f1_macro': 0.5445015574045957,\n",
            " 'f1_micro': 0.706,\n",
            " 'f1_weighted': 0.7245062050371545,\n",
            " 'precision_macro': 0.5178230327850942,\n",
            " 'precision_micro': 0.706,\n",
            " 'precision_weighted': 0.7564433904893414,\n",
            " 'recall_macro': 0.6072821425719418,\n",
            " 'recall_micro': 0.706,\n",
            " 'recall_weighted': 0.706}\n",
            "accuracy: Mean=0.703200, Std=0.026353\n",
            "f1_weighted: Mean=0.712908, Std=0.019333\n",
            "f1_macro: Mean=0.527625, Std=0.020003\n",
            "f1_micro: Mean=0.703200, Std=0.026353\n",
            "precision_weighted: Mean=0.753703, Std=0.015525\n",
            "precision_macro: Mean=0.539720, Std=0.020690\n",
            "precision_micro: Mean=0.703200, Std=0.026353\n",
            "recall_weighted: Mean=0.703200, Std=0.026353\n",
            "recall_macro: Mean=0.582633, Std=0.026575\n",
            "recall_micro: Mean=0.703200, Std=0.026353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'twitter_asian_prejudice'\n",
        "saver = Saver()\n",
        "train_data, val_data, test_data, raw_doc_list = load_data()\n",
        "\n",
        "print(train_data.graph.shape)\n",
        "\n",
        "\n",
        "saved_model, model = train(train_data, val_data, saver)\n",
        "with torch.no_grad():\n",
        "    test_loss_model, preds_model = model(train_data.get_pyg_graph(device=device), test_data)\n",
        "eval_res = eval(preds_model, test_data, True)\n",
        "y_true = eval_res.pop('y_true')\n",
        "y_pred = eval_res.pop('y_pred')\n",
        "print(\"Test...\")\n",
        "pprint(eval_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eYDXaoTjZ-x",
        "outputId": "149450a6-2077-4e12-94f6-b6314ab1b53d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064\n",
            "Loaded from /content/drive/My Drive/CPSC_577_FP/save/split/twitter_asian_prejudice_train_80_val_10_test_10_seed_78_window_size_10.klepto\n",
            "(26057, 26057)\n",
            "Number params:  5212404\n",
            "Epoch: 0000, Train Loss: 1.38630, Time: 0.16604\n",
            "Val Loss: 1.38331\n",
            "Val Results: ...\n",
            "{'accuracy': 0.2575,\n",
            " 'f1_macro': 0.20450787229529688,\n",
            " 'f1_micro': 0.2575,\n",
            " 'f1_weighted': 0.3014090578145378,\n",
            " 'precision_macro': 0.37621645301725537,\n",
            " 'precision_micro': 0.2575,\n",
            " 'precision_weighted': 0.741202300371578,\n",
            " 'recall_macro': 0.3431496265713574,\n",
            " 'recall_micro': 0.2575,\n",
            " 'recall_weighted': 0.2575}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_1.pt\n",
            "Epoch: 0001, Train Loss: 1.38299, Time: 0.17097\n",
            "Val Loss: 1.37844\n",
            "Val Results: ...\n",
            "{'accuracy': 0.708,\n",
            " 'f1_macro': 0.41352078279280596,\n",
            " 'f1_micro': 0.708,\n",
            " 'f1_weighted': 0.6282614932596517,\n",
            " 'precision_macro': 0.7107036392824904,\n",
            " 'precision_micro': 0.708,\n",
            " 'precision_weighted': 0.7248921274748894,\n",
            " 'recall_macro': 0.38393854264852606,\n",
            " 'recall_micro': 0.708,\n",
            " 'recall_weighted': 0.708}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_2.pt\n",
            "Epoch: 0002, Train Loss: 1.37748, Time: 0.15318\n",
            "Val Loss: 1.37155\n",
            "Val Results: ...\n",
            "{'accuracy': 0.739,\n",
            " 'f1_macro': 0.5053807002685752,\n",
            " 'f1_micro': 0.739,\n",
            " 'f1_weighted': 0.6951916543270276,\n",
            " 'precision_macro': 0.6234810536426079,\n",
            " 'precision_micro': 0.739,\n",
            " 'precision_weighted': 0.7295847923342047,\n",
            " 'recall_macro': 0.514699878109117,\n",
            " 'recall_micro': 0.739,\n",
            " 'recall_weighted': 0.739}\n",
            "Trained model saved to /content/drive/My Drive/CPSC_577_FP/logs/TextGNN_twitter_asian_prejudice_0.8_0.1_0.1_2024-05-04T16-56-26.193064/trained_model_epoch_3.pt\n",
            "Epoch: 0003, Train Loss: 1.37031, Time: 0.15890\n",
            "Val Loss: 1.36327\n",
            "Val Results: ...\n",
            "{'accuracy': 0.728,\n",
            " 'f1_macro': 0.554893637411285,\n",
            " 'f1_micro': 0.728,\n",
            " 'f1_weighted': 0.7278799649985656,\n",
            " 'precision_macro': 0.5643092699057837,\n",
            " 'precision_micro': 0.728,\n",
            " 'precision_weighted': 0.7472964404051992,\n",
            " 'recall_macro': 0.6007510859712399,\n",
            " 'recall_micro': 0.728,\n",
            " 'recall_weighted': 0.728}\n",
            "Epoch: 0004, Train Loss: 1.36115, Time: 0.15440\n",
            "Val Loss: 1.35490\n",
            "Val Results: ...\n",
            "{'accuracy': 0.7205,\n",
            " 'f1_macro': 0.552965149135373,\n",
            " 'f1_micro': 0.7205,\n",
            " 'f1_weighted': 0.7247098137547247,\n",
            " 'precision_macro': 0.545143535003709,\n",
            " 'precision_micro': 0.7205,\n",
            " 'precision_weighted': 0.7420444733617478,\n",
            " 'recall_macro': 0.6078836368179164,\n",
            " 'recall_micro': 0.7205,\n",
            " 'recall_weighted': 0.7205}\n",
            "Epoch: 0005, Train Loss: 1.35117, Time: 0.15202\n",
            "Val Loss: 1.34467\n",
            "Val Results: ...\n",
            "{'accuracy': 0.7185,\n",
            " 'f1_macro': 0.5678982415656995,\n",
            " 'f1_micro': 0.7185,\n",
            " 'f1_weighted': 0.7294781538158877,\n",
            " 'precision_macro': 0.5470903990293843,\n",
            " 'precision_micro': 0.7185,\n",
            " 'precision_weighted': 0.7534691913123692,\n",
            " 'recall_macro': 0.6298664387363682,\n",
            " 'recall_micro': 0.7185,\n",
            " 'recall_weighted': 0.7185}\n",
            "Epoch: 0006, Train Loss: 1.34015, Time: 0.14668\n",
            "Val Loss: 1.33362\n",
            "Val Results: ...\n",
            "{'accuracy': 0.711,\n",
            " 'f1_macro': 0.5555464480418161,\n",
            " 'f1_micro': 0.7110000000000001,\n",
            " 'f1_weighted': 0.7234087977175677,\n",
            " 'precision_macro': 0.5478778097315586,\n",
            " 'precision_micro': 0.711,\n",
            " 'precision_weighted': 0.7560814984618242,\n",
            " 'recall_macro': 0.6228404337226671,\n",
            " 'recall_micro': 0.711,\n",
            " 'recall_weighted': 0.711}\n",
            "Epoch: 0007, Train Loss: 1.32824, Time: 0.15731\n",
            "Val Loss: 1.31937\n",
            "Val Results: ...\n",
            "{'accuracy': 0.714,\n",
            " 'f1_macro': 0.5565629747166624,\n",
            " 'f1_micro': 0.714,\n",
            " 'f1_weighted': 0.7243419773572098,\n",
            " 'precision_macro': 0.5414540710122886,\n",
            " 'precision_micro': 0.714,\n",
            " 'precision_weighted': 0.7487238447087924,\n",
            " 'recall_macro': 0.6166543711306203,\n",
            " 'recall_micro': 0.714,\n",
            " 'recall_weighted': 0.714}\n",
            "Test...\n",
            "{'accuracy': 0.7025,\n",
            " 'f1_macro': 0.5429893156801695,\n",
            " 'f1_micro': 0.7025,\n",
            " 'f1_weighted': 0.724033251327719,\n",
            " 'precision_macro': 0.521197650067256,\n",
            " 'precision_micro': 0.7025,\n",
            " 'precision_weighted': 0.7654861091847489,\n",
            " 'recall_macro': 0.6224415513808146,\n",
            " 'recall_micro': 0.7025,\n",
            " 'recall_weighted': 0.7025}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1CLFkSoL4lA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}