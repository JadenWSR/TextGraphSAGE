{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sshw0zx56XXi",
        "outputId": "0c7572af-ecbb-4ee9-a12b-4c41e4270123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "\n",
        "# Defining a simple LSTM Model for Text Classification\n",
        "class TextLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout):\n",
        "        super(TextLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        output, (hidden, _) = self.lstm(embedded)\n",
        "        hidden = self.dropout(hidden[-1])\n",
        "        return self.fc(hidden)\n"
      ],
      "metadata": {
        "id": "sZKHFaws6eJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a function to train the model\n",
        "# Function to train the model\n",
        "def train_model(model, iterator, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for texts, labels in iterator:\n",
        "        texts, labels = texts.to(device), labels.to(device)  # Move data to the device\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(texts)\n",
        "        loss = criterion(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in iterator:\n",
        "            texts, labels = texts.to(device), labels.to(device)  # Move data to the device\n",
        "            predictions = model(texts)\n",
        "            loss = criterion(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            all_preds.append(predictions)\n",
        "            all_labels.append(labels)\n",
        "    avg_loss = epoch_loss / len(iterator)\n",
        "    eval_metrics = eval_mod(torch.cat(all_preds), torch.cat(all_labels))\n",
        "    return avg_loss, eval_metrics\n",
        "\n",
        "def eval_mod(preds, labels):\n",
        "    y_true = labels\n",
        "    y_pred_label = torch.argmax(preds, dim=1)\n",
        "    y_pred_label = y_pred_label.cpu().numpy()  # Ensuring it's a numpy array\n",
        "    y_true = y_true.cpu().numpy()  # Ensuring labels are also numpy array\n",
        "\n",
        "    accuracy = metrics.accuracy_score(y_true, y_pred_label)\n",
        "    f1_weighted = metrics.f1_score(y_true, y_pred_label, average='weighted')\n",
        "    f1_macro = metrics.f1_score(y_true, y_pred_label, average='macro')\n",
        "    f1_micro = metrics.f1_score(y_true, y_pred_label, average='micro')\n",
        "    precision_weighted = metrics.precision_score(y_true, y_pred_label, average='weighted')\n",
        "    precision_macro = metrics.precision_score(y_true, y_pred_label, average='macro')\n",
        "    precision_micro = metrics.precision_score(y_true, y_pred_label, average='micro')\n",
        "    recall_weighted = metrics.recall_score(y_true, y_pred_label, average='weighted')\n",
        "    recall_macro = metrics.recall_score(y_true, y_pred_label, average='macro')\n",
        "    recall_micro = metrics.recall_score(y_true, y_pred_label, average='micro')\n",
        "\n",
        "    results = {\"accuracy\": accuracy,\n",
        "               \"f1_weighted\": f1_weighted,\n",
        "               \"f1_macro\": f1_macro,\n",
        "               \"f1_micro\": f1_micro,\n",
        "               \"precision_weighted\": precision_weighted,\n",
        "               \"precision_macro\": precision_macro,\n",
        "               \"precision_micro\": precision_micro,\n",
        "               \"recall_weighted\": recall_weighted,\n",
        "               \"recall_macro\": recall_macro,\n",
        "               \"recall_micro\": recall_micro\n",
        "               }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "S1699yEH7qjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def read_data(dataset_name, split_ratios=(0.8, 0.1, 0.1), seed=123):\n",
        "    data = {}\n",
        "    x, y = [], []\n",
        "\n",
        "    # Define dataset files based on the dataset name\n",
        "    if dataset_name == \"r8_presplit\":\n",
        "        sentences_file = \"drive/MyDrive/CPSC_577_FP/r8_sentences_clean.txt\"\n",
        "        labels_file = \"drive/MyDrive/CPSC_577_FP/r8_labels.txt\"\n",
        "        label_pos = 2\n",
        "    elif dataset_name == \"ag_presplit\":\n",
        "        sentences_file = \"drive/MyDrive/CPSC_577_FP/ag_sentences_clean.txt\"\n",
        "        labels_file = \"drive/MyDrive/CPSC_577_FP/ag_labels.txt\"\n",
        "        label_pos = 2\n",
        "    elif dataset_name == \"twitter_asian_prejudice\":\n",
        "        sentences_file = \"drive/MyDrive/CPSC_577_FP/twitter_asian_prejudice_sentences_clean.txt\"\n",
        "        labels_file = \"drive/MyDrive/CPSC_577_FP/twitter_asian_prejudice_labels.txt\"\n",
        "        label_pos = 0\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dataset name provided.\")\n",
        "\n",
        "    # Read sentence data\n",
        "    with open(sentences_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line[-1] == \"\\n\":\n",
        "                line = line[:-1]\n",
        "            x.append(line.split())\n",
        "\n",
        "    # Read label data\n",
        "    with open(labels_file, \"r\", encoding=\"utf-8\") as d:\n",
        "        for line in d:\n",
        "            if line[-1] == \"\\n\":\n",
        "                line = line[:-1]\n",
        "            y.append(line.split()[label_pos])\n",
        "\n",
        "    # Shuffle data using the provided seed\n",
        "    x, y = shuffle(x, y, random_state=seed)\n",
        "\n",
        "    # Calculate split indices based on split ratios\n",
        "    total_count = len(x)\n",
        "    train_end = int(split_ratios[0] * total_count)\n",
        "    valid_end = train_end + int(split_ratios[1] * total_count)\n",
        "\n",
        "    # Split data into training, validation, and test sets\n",
        "    data[\"train_x\"], data[\"train_y\"] = x[:train_end], y[:train_end]\n",
        "    data[\"valid_x\"], data[\"valid_y\"] = x[train_end:valid_end], y[train_end:valid_end]\n",
        "    data[\"test_x\"], data[\"test_y\"] = x[valid_end:], y[valid_end:]\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "eH0i1SyN7t2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_data_loader(texts, labels, batch_size, vocab, label_dict, max_len=100):\n",
        "    # Convert texts to integer sequences\n",
        "    processed_texts = [[vocab[word] if word in vocab else vocab[\"<UNK>\"] for word in text] for text in texts]\n",
        "\n",
        "    # Pad or truncate sequences\n",
        "    processed_texts = [text[:max_len] + [vocab[\"<PAD>\"]] * (max_len - len(text)) if len(text) < max_len else text[:max_len] for text in processed_texts]\n",
        "\n",
        "    # Convert labels to integers\n",
        "    processed_labels = [label_dict[label] for label in labels]\n",
        "\n",
        "    # Create tensors\n",
        "    texts_tensor = torch.tensor(processed_texts, dtype=torch.long)\n",
        "    labels_tensor = torch.tensor(processed_labels, dtype=torch.long)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = TensorDataset(texts_tensor, labels_tensor)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "jxWM0YAgBSyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89p99Pp4EnIB",
        "outputId": "99777720-8c7b-4c9a-cb26-16f698fbdaf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "def main():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    dataset_name = \"r8_presplit\"\n",
        "    # Parameters\n",
        "\n",
        "    embedding_dim = 64\n",
        "    hidden_dim = 50\n",
        "    output_dim = 8  # Adjusted to the dataset's specific output dimensions\n",
        "    num_layers = 2\n",
        "    dropout = 0\n",
        "    batch_size = 64\n",
        "    num_epochs = 100\n",
        "    patience = 10  # Early stopping patience\n",
        "    num_experiments = 5\n",
        "    random_seeds = [33, 15, 86, 109, 78]\n",
        "    all_experiment_results = []\n",
        "\n",
        "    for exp in range(num_experiments):\n",
        "        random_seed = random_seeds[exp]\n",
        "        # Load data\n",
        "\n",
        "        data = read_data(dataset_name, seed = random_seed)\n",
        "        vocab = {word: i for i, word in enumerate(set(sum(data['train_x'], [])))}  # Creating a simple vocab\n",
        "        vocab[\"<PAD>\"] = 0\n",
        "        vocab[\"<UNK>\"] = 1\n",
        "        label_dict = {label: i for i, label in enumerate(set(data['train_y']))}\n",
        "        vocab_size = len(vocab)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = create_data_loader(data['train_x'], data['train_y'], batch_size, vocab, label_dict)\n",
        "        valid_loader = create_data_loader(data['valid_x'], data['valid_y'], batch_size, vocab, label_dict)\n",
        "        test_loader = create_data_loader(data['test_x'], data['test_y'], batch_size, vocab, label_dict)\n",
        "\n",
        "\n",
        "        # Create model\n",
        "        model = TextLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout).to(device)\n",
        "        learning_rate = 0.1\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Early stopping mechanism\n",
        "        best_valid_loss = float('inf')\n",
        "        best_model_path = \"\"\n",
        "        epochs_no_improve = 0\n",
        "\n",
        "        # Training and validation\n",
        "        for epoch in range(num_epochs):\n",
        "            start_time = time.time()\n",
        "            train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
        "            train_time = time.time() - start_time\n",
        "            valid_loss, valid_results = evaluate_model(model, valid_loader, criterion, device)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_path = f'/content/drive/My Drive/CPSC_577_FP/logs/LSTM/model_best_{exp}.pt'\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= patience:\n",
        "                    print(\"Early stopping triggered on experiment\", exp+1)\n",
        "                    break\n",
        "        # Load the best model and evaluate on the test set\n",
        "        model.load_state_dict(torch.load(best_model_path))\n",
        "        test_loss, test_results = evaluate_model(model, test_loader, criterion, device)\n",
        "        all_experiment_results.append(test_results)\n",
        "\n",
        "    # Calculate mean and standard deviation across experiments\n",
        "    final_metrics = {key: [] for key in all_experiment_results[0]}\n",
        "    for results in all_experiment_results:\n",
        "        for key in results:\n",
        "            final_metrics[key].append(results[key])\n",
        "\n",
        "    for metric in final_metrics:\n",
        "        values = np.array(final_metrics[metric])\n",
        "        mean = values.mean()\n",
        "        std = values.std()\n",
        "        print(f'{metric}: Mean={mean:.4f}, Std={std:.4f}')"
      ],
      "metadata": {
        "id": "lsQesyZn-KAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-UuHmFBQxQd",
        "outputId": "7a3de727-f443-47a2-c098-0202a66add57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping triggered on experiment 1\n",
            "Early stopping triggered on experiment 2\n",
            "Early stopping triggered on experiment 3\n",
            "Early stopping triggered on experiment 4\n",
            "Early stopping triggered on experiment 5\n",
            "accuracy: Mean=0.8482, Std=0.0402\n",
            "f1_weighted: Mean=0.8321, Std=0.0495\n",
            "f1_macro: Mean=0.5028, Std=0.1018\n",
            "f1_micro: Mean=0.8482, Std=0.0402\n",
            "precision_weighted: Mean=0.8255, Std=0.0535\n",
            "precision_macro: Mean=0.5127, Std=0.1101\n",
            "precision_micro: Mean=0.8482, Std=0.0402\n",
            "recall_weighted: Mean=0.8482, Std=0.0402\n",
            "recall_macro: Mean=0.5179, Std=0.0940\n",
            "recall_micro: Mean=0.8482, Std=0.0402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main1():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    # Load data\n",
        "    dataset_name = \"twitter_asian_prejudice\"\n",
        "\n",
        "\n",
        "    # Parameters\n",
        "\n",
        "    embedding_dim = 64\n",
        "    hidden_dim = 50\n",
        "    output_dim = 5  # Adjusted to the dataset's specific output dimensions\n",
        "    num_layers = 2\n",
        "    dropout = 0\n",
        "    batch_size = 64\n",
        "    num_epochs = 50\n",
        "    patience = 10  # Early stopping patience\n",
        "    num_experiments = 5\n",
        "    random_seeds = [33, 15, 86, 109, 78]\n",
        "    all_experiment_results = []\n",
        "\n",
        "    for exp in range(num_experiments):\n",
        "        random_seed = random_seeds[exp]\n",
        "        data = read_data(dataset_name, seed= random_seed)\n",
        "        vocab = {word: i for i, word in enumerate(set(sum(data['train_x'], [])))}  # Creating a simple vocab\n",
        "        vocab[\"<PAD>\"] = 0\n",
        "        vocab[\"<UNK>\"] = 1\n",
        "        label_dict = {label: i for i, label in enumerate(set(data['train_y']))}\n",
        "        vocab_size = len(vocab)\n",
        "        # Create data loaders\n",
        "        train_loader = create_data_loader(data['train_x'], data['train_y'], batch_size, vocab, label_dict)\n",
        "        valid_loader = create_data_loader(data['valid_x'], data['valid_y'], batch_size, vocab, label_dict)\n",
        "        test_loader = create_data_loader(data['test_x'], data['test_y'], batch_size, vocab, label_dict)\n",
        "\n",
        "\n",
        "        # Create model\n",
        "        model = TextLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout).to(device)\n",
        "        learning_rate = 0.1\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Early stopping mechanism\n",
        "        best_valid_loss = float('inf')\n",
        "        best_model_path = \"\"\n",
        "        epochs_no_improve = 0\n",
        "\n",
        "        # Training and validation\n",
        "        for epoch in range(num_epochs):\n",
        "            start_time = time.time()\n",
        "            train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
        "            train_time = time.time() - start_time\n",
        "            valid_loss, valid_results = evaluate_model(model, valid_loader, criterion, device)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_path = f'/content/drive/My Drive/CPSC_577_FP/logs/LSTM/model_best_{exp}.pt'\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= patience:\n",
        "                    print(\"Early stopping triggered on experiment\", exp+1)\n",
        "                    break\n",
        "        # Load the best model and evaluate on the test set\n",
        "        model.load_state_dict(torch.load(best_model_path))\n",
        "        test_loss, test_results = evaluate_model(model, test_loader, criterion, device)\n",
        "        all_experiment_results.append(test_results)\n",
        "\n",
        "    # Calculate mean and standard deviation across experiments\n",
        "    final_metrics = {key: [] for key in all_experiment_results[0]}\n",
        "    for results in all_experiment_results:\n",
        "        for key in results:\n",
        "            final_metrics[key].append(results[key])\n",
        "\n",
        "    for metric in final_metrics:\n",
        "        values = np.array(final_metrics[metric])\n",
        "        mean = values.mean()\n",
        "        std = values.std()\n",
        "        print(f'{metric}: Mean={mean:.4f}, Std={std:.4f}')"
      ],
      "metadata": {
        "id": "LntjFutMAy8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "main1()  # Run your main function again"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_8hWHUJFUv5",
        "outputId": "4b9b1713-d0d9-4cc9-fea8-c4a6f93f26b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping triggered on experiment 1\n",
            "Early stopping triggered on experiment 2\n",
            "Early stopping triggered on experiment 3\n",
            "Early stopping triggered on experiment 4\n",
            "Early stopping triggered on experiment 5\n",
            "accuracy: Mean=0.6813, Std=0.0078\n",
            "f1_weighted: Mean=0.5522, Std=0.0100\n",
            "f1_macro: Mean=0.1621, Std=0.0011\n",
            "f1_micro: Mean=0.6813, Std=0.0078\n",
            "precision_weighted: Mean=0.4642, Std=0.0105\n",
            "precision_macro: Mean=0.1363, Std=0.0016\n",
            "precision_micro: Mean=0.6813, Std=0.0078\n",
            "recall_weighted: Mean=0.6813, Std=0.0078\n",
            "recall_macro: Mean=0.2000, Std=0.0000\n",
            "recall_micro: Mean=0.6813, Std=0.0078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcc8AHX5FwlA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}