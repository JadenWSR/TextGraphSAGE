{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/armancohan/cpsc477-internal/blob/main/hw3/part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH2YExQa16Gd"
   },
   "source": [
    "# CPSC 477/577 Project Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk0Ng1L3puxz"
   },
   "source": [
    "## Name and NetID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mtwa1W61px8Z"
   },
   "source": [
    "Group Member: Shurui Wang; Lang Ding; Weiyi You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfNL_UxjyA-A",
    "outputId": "e0a08c5a-da5e-4ee5-91b9-d072717eb3cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# do not run if you are using school cluster\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BmaY5OKzqnP",
    "outputId": "1214b137-d2b7-42e8-84c1-5719543dce4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: klepto in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (0.2.5)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from klepto) (0.3.4)\n",
      "Requirement already satisfied: dill>=0.3.8 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from klepto) (0.3.8)\n"
     ]
    }
   ],
   "source": [
    "# do not run if you are using school cluster\n",
    "!pip install klepto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2YXqk-1BzlJk"
   },
   "outputs": [],
   "source": [
    "# util.py\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "import klepto\n",
    "from os.path import dirname, abspath, join, expanduser, isfile, exists\n",
    "from os import environ, makedirs, getcwd\n",
    "import pytz\n",
    "import re\n",
    "from socket import gethostname\n",
    "\n",
    "def ensure_prefix(filepath):\n",
    "    # Define the base path prefix\n",
    "    prefix = '/gpfs/gibbs/project/cpsc477/cpsc477_sw2349/content/MyDrive/'\n",
    "\n",
    "    # Check if the prefix is already in the filepath\n",
    "    if not filepath.startswith(prefix):\n",
    "        # If not, add the prefix\n",
    "        filepath = join(prefix, filepath.lstrip('/'))  # Ensure no leading '/' to avoid treating it as absolute\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def get_root_path():\n",
    "    return getcwd()  # 获取并返回当前工作目录\n",
    "\n",
    "def get_data_path():\n",
    "    return join(get_root_path(), '/CPSC_577_FP')\n",
    "\n",
    "\n",
    "def get_corpus_path():\n",
    "    return join(get_data_path())\n",
    "\n",
    "\n",
    "def get_save_path():\n",
    "    return join(get_root_path(), '/CPSC_577_FP/save')\n",
    "\n",
    "\n",
    "def load(filepath, print_msg=True):\n",
    "    fp = proc_filepath(filepath)\n",
    "    if isfile(fp):\n",
    "        return load_klepto(fp, print_msg)\n",
    "    elif print_msg:\n",
    "        print('Trying to load but no file {}'.format(fp))\n",
    "\n",
    "\n",
    "def load_klepto(filepath, print_msg):\n",
    "    rtn = klepto.archives.file_archive(filepath)\n",
    "    rtn.load()\n",
    "    global logs\n",
    "    if logs:\n",
    "        if print_msg:\n",
    "            print('Loaded from {}'.format(filepath))\n",
    "    return rtn\n",
    "\n",
    "\n",
    "def save(obj, filepath, print_msg=True):\n",
    "    filepath = ensure_prefix(filepath)\n",
    "    fp = proc_filepath(filepath, ext='.klepto')\n",
    "    create_dir_if_not_exists(dirname(filepath))\n",
    "    save_klepto(obj, fp, print_msg)\n",
    "\n",
    "\n",
    "def create_dir_if_not_exists(dir):\n",
    "    if not exists(dir):\n",
    "        makedirs(dir)\n",
    "\n",
    "\n",
    "def save_klepto(dic, filepath, print_msg):\n",
    "    global logs\n",
    "    if logs:\n",
    "        print('filepath to save:', filepath)\n",
    "        if print_msg:\n",
    "            print('Saving to {}'.format(filepath))\n",
    "    klepto.archives.file_archive(filepath, dict=dic).dump()\n",
    "\n",
    "\n",
    "def proc_filepath(filepath, ext='.klepto'):\n",
    "    global logs\n",
    "    if logs:\n",
    "        print('filepath:', filepath)\n",
    "    filepath = ensure_prefix(filepath)\n",
    "    if logs:\n",
    "        print('filepath:', filepath)\n",
    "    if type(filepath) is not str:\n",
    "        raise RuntimeError('Did you pass a file path to this function?')\n",
    "    return append_ext_to_filepath(ext, filepath)\n",
    "\n",
    "\n",
    "def append_ext_to_filepath(ext, fp):\n",
    "    if not fp.endswith(ext):\n",
    "        fp += ext\n",
    "    return fp\n",
    "\n",
    "\n",
    "def parse_as_int_list(il):\n",
    "    rtn = []\n",
    "    for x in il.split('_'):\n",
    "        x = int(x)\n",
    "        rtn.append(x)\n",
    "    return rtn\n",
    "\n",
    "\n",
    "def get_user():\n",
    "    try:\n",
    "        home_user = expanduser(\"~\").split('/')[-1]\n",
    "    except:\n",
    "        home_user = 'user'\n",
    "    return home_user\n",
    "\n",
    "\n",
    "def get_host():\n",
    "    host = environ.get('HOSTNAME')\n",
    "    if host is not None:\n",
    "        return host\n",
    "    return gethostname()\n",
    "\n",
    "tstamp = None\n",
    "\n",
    "\n",
    "def get_ts():\n",
    "    global tstamp\n",
    "    if not tstamp:\n",
    "        tstamp = get_current_ts()\n",
    "    return tstamp\n",
    "\n",
    "\n",
    "def get_current_ts(zone='US/Pacific'):\n",
    "    return datetime.datetime.now(pytz.timezone(zone)).strftime(\n",
    "        '%Y-%m-%dT%H-%M-%S.%f')\n",
    "\n",
    "\n",
    "def sorted_nicely(l, reverse=False):\n",
    "    def tryint(s):\n",
    "        try:\n",
    "            return int(s)\n",
    "        except:\n",
    "            return s\n",
    "\n",
    "    def alphanum_key(s):\n",
    "        if type(s) is not str:\n",
    "            raise ValueError('{} must be a string in l: {}'.format(s, l))\n",
    "        return [tryint(c) for c in re.split('([0-9]+)', s)]\n",
    "\n",
    "    rtn = sorted(l, key=alphanum_key)\n",
    "    if reverse:\n",
    "        rtn = reversed(rtn)\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpZbzmjV0V1z",
    "outputId": "18057c7d-ebaf-4bd0-f132-d30b690454cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
      "Requirement already satisfied: torch-scatter in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (2.1.2+pt22cu121)\n",
      "Requirement already satisfied: torch-geometric in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (2.5.3)\n",
      "Requirement already satisfied: tqdm in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (1.12.0)\n",
      "Requirement already satisfied: fsspec in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (3.1.3)\n",
      "Requirement already satisfied: aiohttp in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (3.9.3)\n",
      "Requirement already satisfied: requests in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: scikit-learn in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (1.4.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from scikit-learn->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/cpsc477_sw2349/.conda/envs/cpsc577/lib/python3.12/site-packages (from scikit-learn->torch-geometric) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lh8ejyTPzxkG"
   },
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch_geometric.data import Data as PyGSingleGraphData\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, dict):\n",
    "        return {key: to_device(value, device) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "class TextDataset(object):\n",
    "    def __init__(self, name, sparse_graph, labels, vocab, word_id_map, docs_dict, loaded_dict, tvt='all',\n",
    "                 train_test_split=None):\n",
    "        if loaded_dict is not None:  # restore from content loaded from disk\n",
    "            self.__dict__ = loaded_dict\n",
    "            return\n",
    "        self.name = name\n",
    "        self.graph = sparse_graph\n",
    "        self.labels = labels\n",
    "        if 'twitter_asian_prejudice' in name:\n",
    "            if 'sentiment' not in name:\n",
    "                self.labels = ['discussion_of_eastasian_prejudice' if label =='counter_speech' else label for label in self.labels]\n",
    "            else:\n",
    "                if 'neutral' not in labels:\n",
    "                    sentiment_labels = []\n",
    "                    neutral_pos_labels = [\"none_of_the_above\", \"counter_speech\", \"discussion_of_eastasian_prejudice\"]\n",
    "                    for label in labels:\n",
    "                        if label in neutral_pos_labels:\n",
    "                            sentiment_labels.append(\"neutral\")\n",
    "                        else:\n",
    "                            sentiment_labels.append(\"negative\")\n",
    "                    self.labels = sentiment_labels\n",
    "        self.label_dict = {label: i for i, label in enumerate(list(set(self.labels)))}\n",
    "        self.label_inds = np.asarray([self.label_dict[label] for label in self.labels])\n",
    "        self.vocab = vocab\n",
    "        self.word_id_map = word_id_map\n",
    "        self.docs = docs_dict\n",
    "        self.node_ids = list(self.docs.keys())\n",
    "        self.tvt = tvt\n",
    "        self.train_test_split = train_test_split\n",
    "\n",
    "    def tvt_split(self, split_points, tvt_list, seed):\n",
    "        if self.train_test_split is None:\n",
    "            doc_id_chunks = self._chunk_doc_ids(split_points, seed)\n",
    "        else:\n",
    "            train_ids = []\n",
    "            test_ids = []\n",
    "            for k, v in self.train_test_split.items():\n",
    "                if v == 'test':\n",
    "                    test_ids.append(k)\n",
    "                elif v == 'train':\n",
    "                    train_ids.append(k)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            num_val = int(len(train_ids) * 0.1)\n",
    "            random.Random(seed).shuffle(train_ids)\n",
    "            val_ids = train_ids[:num_val]\n",
    "            train_ids = train_ids[num_val:]\n",
    "            doc_id_chunks = [train_ids, val_ids, test_ids]\n",
    "        sub_dataset = []\n",
    "        for i, chunk in enumerate(doc_id_chunks):\n",
    "            docs = {doc_id: self.docs[doc_id] for doc_id in chunk}\n",
    "            sub_dataset.append(TextDataset(self.name, self.graph, self.labels, self.vocab,\n",
    "                                           self.word_id_map, docs, None, tvt_list[i]))\n",
    "        return sub_dataset\n",
    "\n",
    "    def _chunk_doc_ids(self, split_points, seed):\n",
    "        ids = sorted(self.docs.keys())\n",
    "        id_chunks = self._chunk_list(ids, split_points, seed)\n",
    "        return id_chunks\n",
    "\n",
    "    def _chunk_list(self, li, split_points, seed):\n",
    "        rtn = []\n",
    "        random.Random(seed).shuffle(li)\n",
    "        left = 0\n",
    "        split_indices = [int(len(li) * sp) for sp in split_points]\n",
    "        for si in split_indices:\n",
    "            right = left + si\n",
    "            if type(right) is not int or right <= 0 or right >= len(li):\n",
    "                raise ValueError('Wrong split_points {}'.format(split_points))\n",
    "            take = li[left:right]\n",
    "            rtn.append(take)\n",
    "            left = right\n",
    "        # The last chunk is inferred.\n",
    "        rtn.append(li[left:])\n",
    "        return rtn\n",
    "\n",
    "    def init_node_feats(self, type, device):\n",
    "        if type == 'one_hot_init':\n",
    "            num_nodes = self.graph.shape[0]\n",
    "            identity = sp.identity(num_nodes)\n",
    "            ind0, ind1, values = sp.find(identity)\n",
    "            inds = np.stack((ind0, ind1), axis=0)\n",
    "            self.node_feats = torch.sparse_coo_tensor(inds, values, device=device,\n",
    "                                                      dtype=torch.float)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def get_pyg_graph(self, device):\n",
    "        if not hasattr(self, \"pyg_graph\"):\n",
    "            adj = self.graph  # Your scipy.sparse.csr_matrix\n",
    "            adj_coo = adj.tocoo()  # Convert to COO format, which is simple to work with\n",
    "\n",
    "            # Create edge index and edge weight tensors directly from the COO format\n",
    "            edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long).to(device)\n",
    "            edge_weight = torch.tensor(adj_coo.data, dtype=torch.float).to(device)\n",
    "\n",
    "            # Ensure node features are in a suitable format and on the correct device\n",
    "            if hasattr(self, 'node_feats'):\n",
    "                if isinstance(self.node_feats, np.ndarray):\n",
    "                    node_feats = torch.from_numpy(self.node_feats).float().to(device)\n",
    "                else:\n",
    "                    node_feats = self.node_feats.to(device)\n",
    "            else:\n",
    "                node_feats = torch.ones((adj.shape[0], 1), dtype=torch.float).to(device)\n",
    "\n",
    "            # Create PyG graph data object\n",
    "            self.pyg_graph = PyGSingleGraphData(x=node_feats, edge_index=edge_index, edge_attr=edge_weight, y=None)\n",
    "        return self.pyg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-8TYLALS0kHO"
   },
   "outputs": [],
   "source": [
    "# build_graph.py\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from os.path import join, exists\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def build_text_graph_dataset(dataset, window_size):\n",
    "    if \"small\" in dataset or \"presplit\" in dataset or 'sentiment' in dataset:\n",
    "        dataset_name = \"_\".join(dataset.split(\"_\")[:-1])\n",
    "    else:\n",
    "        dataset_name = dataset\n",
    "    clean_text_path = join('/gpfs/gibbs/project/cpsc477/cpsc477_sw2349/content/MyDrive/CPSC_577_FP', dataset_name + '_sentences_clean.txt')\n",
    "    labels_path = join('/gpfs/gibbs/project/cpsc477/cpsc477_sw2349/content/MyDrive/CPSC_577_FP', dataset_name + '_labels.txt')\n",
    "    labels = pd.read_csv(labels_path, header=None, sep='\\t')\n",
    "    doc_list = []\n",
    "    f = open(clean_text_path, 'rb')\n",
    "    for line in f.readlines():\n",
    "        doc_list.append(line.strip().decode())\n",
    "    f.close()\n",
    "    assert len(labels) == len(doc_list)\n",
    "    if 'presplit' not in dataset:\n",
    "        labels_list = labels.iloc[0:, 0].tolist()\n",
    "        split_dict = None\n",
    "    else:\n",
    "        labels_list = labels.iloc[0:, 2].tolist()\n",
    "        split = labels.iloc[0:, 1].tolist()\n",
    "        split_dict = {}\n",
    "        for i, v in enumerate(split):\n",
    "            split_dict[i] = v\n",
    "    if \"small\" in dataset:\n",
    "        doc_list = doc_list[:200]\n",
    "        labels_list = labels_list[:200]\n",
    "\n",
    "    word_freq = get_vocab(doc_list)\n",
    "    vocab = list(word_freq.keys())\n",
    "    if not exists(join('/gpfs/gibbs/project/cpsc477/cpsc477_sw2349/content/MyDrive/CPSC_577_FP', dataset + '_vocab.txt')):\n",
    "        vocab_str = '\\n'.join(vocab)\n",
    "        f = open(join('/gpfs/gibbs/project/cpsc477/cpsc477_sw2349/content/MyDrive/CPSC_577_FP', dataset + '_vocab.txt'), 'w')\n",
    "        f.write(vocab_str)\n",
    "        f.close()\n",
    "    words_in_docs, word_doc_freq = build_word_doc_edges(doc_list)\n",
    "    word_id_map = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    sparse_graph = build_edges(doc_list, word_id_map, vocab, word_doc_freq, window_size)\n",
    "    docs_dict = {i: doc for i, doc in enumerate(doc_list)}\n",
    "    return TextDataset(dataset, sparse_graph, labels_list, vocab, word_id_map, docs_dict, None,\n",
    "                       train_test_split=split_dict)\n",
    "\n",
    "\n",
    "def build_edges(doc_list, word_id_map, vocab, word_doc_freq, window_size=20):\n",
    "    # constructing all windows\n",
    "    windows = []\n",
    "    for doc_words in doc_list:\n",
    "        words = doc_words.split()\n",
    "        doc_length = len(words)\n",
    "        if doc_length <= window_size:\n",
    "            windows.append(words)\n",
    "        else:\n",
    "            for i in range(doc_length - window_size + 1):\n",
    "                window = words[i: i + window_size]\n",
    "                windows.append(window)\n",
    "    # constructing all single word frequency\n",
    "    word_window_freq = defaultdict(int)\n",
    "    for window in windows:\n",
    "        appeared = set()\n",
    "        for word in window:\n",
    "            if word not in appeared:\n",
    "                word_window_freq[word] += 1\n",
    "                appeared.add(word)\n",
    "    # constructing word pair count frequency\n",
    "    word_pair_count = defaultdict(int)\n",
    "    for window in tqdm(windows):\n",
    "        for i in range(1, len(window)):\n",
    "            for j in range(i):\n",
    "                word_i = window[i]\n",
    "                word_j = window[j]\n",
    "                word_i_id = word_id_map[word_i]\n",
    "                word_j_id = word_id_map[word_j]\n",
    "                if word_i_id == word_j_id:\n",
    "                    continue\n",
    "                word_pair_count[(word_i_id, word_j_id)] += 1\n",
    "                word_pair_count[(word_j_id, word_i_id)] += 1\n",
    "    row = []\n",
    "    col = []\n",
    "    weight = []\n",
    "\n",
    "    # pmi as weights\n",
    "    num_docs = len(doc_list)\n",
    "    num_window = len(windows)\n",
    "    for word_id_pair, count in tqdm(word_pair_count.items()):\n",
    "        i, j = word_id_pair[0], word_id_pair[1]\n",
    "        word_freq_i = word_window_freq[vocab[i]]\n",
    "        word_freq_j = word_window_freq[vocab[j]]\n",
    "        pmi = log((1.0 * count / num_window) /\n",
    "                  (1.0 * word_freq_i * word_freq_j / (num_window * num_window)))\n",
    "        if pmi <= 0:\n",
    "            continue\n",
    "        row.append(num_docs + i)\n",
    "        col.append(num_docs + j)\n",
    "        weight.append(pmi)\n",
    "\n",
    "    # frequency of document word pair\n",
    "    doc_word_freq = defaultdict(int)\n",
    "    for i, doc_words in enumerate(doc_list):\n",
    "        words = doc_words.split()\n",
    "        for word in words:\n",
    "            word_id = word_id_map[word]\n",
    "            doc_word_str = (i, word_id)\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "\n",
    "    for i, doc_words in enumerate(doc_list):\n",
    "        words = doc_words.split()\n",
    "        doc_word_set = set()\n",
    "        for word in words:\n",
    "            if word in doc_word_set:\n",
    "                continue\n",
    "            word_id = word_id_map[word]\n",
    "            freq = doc_word_freq[(i, word_id)]\n",
    "            row.append(i)\n",
    "            col.append(num_docs + word_id)\n",
    "            idf = log(1.0 * num_docs /\n",
    "                      word_doc_freq[vocab[word_id]])\n",
    "            weight.append(freq * idf)\n",
    "            doc_word_set.add(word)\n",
    "\n",
    "    # Adding self-loops to the graph\n",
    "    for i in range(num_docs + len(vocab)):\n",
    "        row.append(i)\n",
    "        col.append(i)\n",
    "        weight.append(1)  # Assigning a weight of 1 to self-loops\n",
    "\n",
    "    number_nodes = num_docs + len(vocab)\n",
    "    adj_mat = sp.csr_matrix((weight, (row, col)), shape=(number_nodes, number_nodes))\n",
    "    adj = adj_mat + adj_mat.T.multiply(adj_mat.T > adj_mat) - adj_mat.multiply(adj_mat.T > adj_mat)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def get_vocab(text_list):\n",
    "    word_freq = defaultdict(int)\n",
    "    for doc_words in text_list:\n",
    "        words = doc_words.split()\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def build_word_doc_edges(doc_list):\n",
    "    # build all docs that a word is contained in\n",
    "    words_in_docs = defaultdict(set)\n",
    "    for i, doc_words in enumerate(doc_list):\n",
    "        words = doc_words.split()\n",
    "        for word in words:\n",
    "            words_in_docs[word].add(i)\n",
    "\n",
    "    word_doc_freq = {}\n",
    "    for word, doc_list in words_in_docs.items():\n",
    "        word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "    return words_in_docs, word_doc_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XL-kBqQw0xV-"
   },
   "outputs": [],
   "source": [
    "# prep_data.py\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from os.path import join, exists\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_data(dataset):\n",
    "    clean_text_path = join('/gpfs/gibbs/project/cpsc477/cpsc477_sw2349/content/MyDrive/CPSC_577_FP', 'corpus', dataset + '_sentences_clean.txt')\n",
    "    if not exists(clean_text_path):\n",
    "        docs_list = []\n",
    "        old_name = dataset\n",
    "        if \"no_hashtag\" in dataset:\n",
    "            dataset = '_'.join(dataset.split('_')[:-2])\n",
    "        with open(join('/gpfs/gibbs/project/cpsc477/cpsc477_sw2349/content/MyDrive/CPSC_577_FP', 'corpus', dataset + '_sentences.txt')) as f:\n",
    "            for line in f.readlines():\n",
    "                docs_list.append(line.strip())\n",
    "        dataset = old_name\n",
    "        word_counts = defaultdict(int)\n",
    "        for doc in docs_list:\n",
    "            temp = clean_doc(doc, dataset)\n",
    "            words = temp.split()\n",
    "            for word in words:\n",
    "                word_counts[word] += 1\n",
    "        clean_docs = clean_documents(docs_list, word_counts, dataset)\n",
    "        corpus_str = '\\n'.join(clean_docs)\n",
    "        f = open(clean_text_path, 'w')\n",
    "        f.write(corpus_str)\n",
    "        f.close()\n",
    "    f = open(clean_text_path, 'r')\n",
    "    lines = f.readlines()\n",
    "    min_len = 10000\n",
    "    aver_len = 0\n",
    "    max_len = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        temp = line.split()\n",
    "        aver_len = aver_len + len(temp)\n",
    "        if len(temp) < min_len:\n",
    "            min_len = len(temp)\n",
    "        if len(temp) > max_len:\n",
    "            max_len = len(temp)\n",
    "    f.close()\n",
    "    aver_len = 1.0 * aver_len / len(lines)\n",
    "    print('min_len : ' + str(min_len))\n",
    "    print('max_len : ' + str(max_len))\n",
    "    print('average_len : ' + str(aver_len))\n",
    "\n",
    "\n",
    "def clean_documents(docs, word_counts, dataset):\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(stop_words)\n",
    "    ret = []\n",
    "    for doc in docs:\n",
    "        doc = clean_doc(doc, dataset)\n",
    "        words = doc.split()\n",
    "        words = [word for word in words if word not in stop_words and word_counts[word] >= 5]\n",
    "        doc = ' '.join(words).strip()\n",
    "        if doc != '':\n",
    "            ret.append(' '.join(words).strip())\n",
    "        else:\n",
    "            ret.append(' ')\n",
    "    return ret\n",
    "\n",
    "\n",
    "def clean_doc_ap(string):\n",
    "    string = re.sub(r\"http[s]?\\:\\/\\/.[a-zA-Z0-9\\.\\/\\_?=%&#\\-\\+!]+\", \" \", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9()_+,!?:\\'\\`]\", \" \", string)  # replace all non alpha numeric characters\n",
    "    string = re.sub(r\"(?<!HASHTAG)_\", \" \", string)\n",
    "    string = re.sub(r\"(?<!EASTASIA)\\+ | (?<!VIRUS)\\+\", \" \", string)\n",
    "    string = re.sub(r\"\\+\", \"_\", string)\n",
    "    string = re.sub(r\"HASHTAG_EASTASIA_VIRUS(?!(\\s))\", \"HASHTAG_EASTASIA_VIRUS \", string)\n",
    "    string = re.sub(r\"HASHTAG_EASTASIA(?!(\\s|_))\", \"HASHTAG_EASTASIA \", string)\n",
    "    string = re.sub(r\"HASHTAG_VIRUS(?!(\\s|_))\", \"HASHTAG_VIRUS \", string)\n",
    "    string = re.sub(r\"HASHTAG_VIRUS_OTHERCOUNTRY(?!(\\s))\", \"HASHTAG_VIRUS_OTHERCOUNTRY \", string)\n",
    "    string = re.sub(r\"HASHTAG(?!([\\s|_]))\", \"HASHTAG \", string)\n",
    "    if \"no_hashtag\" in dataset:\n",
    "        string = re.sub(r\"HASHTAG_EASTASIA_VIRUS\", \" \", string)\n",
    "        string = re.sub(r\"HASHTAG_EASTASIA\", \" \", string)\n",
    "        string = re.sub(r\"HASHTAG_VIRUS\", \" \", string)\n",
    "        string = re.sub(r\"HASHTAG_VIRUS_OTHERCOUNTRY\", \" \", string)\n",
    "        string = re.sub(r\"HASHTAG\", \" \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "def clean_doc(string, dataset):\n",
    "    if 'twitter_asian_prejudice' in dataset:\n",
    "        string = clean_doc_ap(string)\n",
    "    else:\n",
    "        pass\n",
    "    string = re.sub(r\"^\\\"\", \"\", string)\n",
    "    string = re.sub(r\"\\\"$\", \"\", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\"\\.\", \" \", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" \", string)\n",
    "    string = re.sub(r\"\\(\", \" \", string)\n",
    "    string = re.sub(r\"\\)\", \" \", string)\n",
    "    string = re.sub(r\"\\?\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jlvPXsBd8LUy"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "#from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class CustomSAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, bias=True):\n",
    "        super(CustomSAGEConv, self).__init__(aggr=None)  # Initialize without predefined aggregation\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.lin = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "            torch.nn.init.uniform_(self.bias, -0.01, 0.01)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # Add self-loops to the adjacency matrix\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Create the adjacency matrix from edge_index for dense operations\n",
    "        num_nodes = x.size(0)\n",
    "        adj = torch.zeros((num_nodes, num_nodes), device=x.device)\n",
    "        adj[edge_index[0], edge_index[1]] = 1  # Assuming unweighted for simplicity, use edge_weight if available\n",
    "\n",
    "        # Normalize adjacency matrix\n",
    "        if self.normalize:\n",
    "            row_sum = adj.sum(dim=1, keepdim=True)\n",
    "            d_inv_sqrt = row_sum.pow(-0.5)\n",
    "            d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0\n",
    "            adj = d_inv_sqrt * adj * d_inv_sqrt.transpose(0, 1)\n",
    "\n",
    "        # Dense matrix multiplication for aggregation\n",
    "        aggregated_features = torch.matmul(adj, x)\n",
    "\n",
    "        # Apply linear transformation\n",
    "        transformed_features = self.lin(aggregated_features)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            transformed_features += self.bias\n",
    "\n",
    "        return transformed_features\n",
    "\n",
    "\n",
    "class TextGNN(nn.Module):\n",
    "    def __init__(self, pred_type, node_embd_type, num_layers, layer_dim_list, act, bn, num_labels, class_weights, dropout):\n",
    "        super(TextGNN, self).__init__()\n",
    "        self.node_embd_type = node_embd_type\n",
    "        self.layer_dim_list = layer_dim_list\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        if pred_type == 'softmax':\n",
    "            assert layer_dim_list[-1] == num_labels\n",
    "        elif pred_type == 'mlp':\n",
    "            dims = self._calc_mlp_dims(layer_dim_list[-1], num_labels)\n",
    "            self.mlp = MLP(layer_dim_list[-1], num_labels, num_hidden_lyr=len(dims), hidden_channels=dims, bn=False)\n",
    "        self.pred_type = pred_type\n",
    "        assert len(layer_dim_list) == (num_layers + 1)\n",
    "        self.act = act\n",
    "        self.bn = bn\n",
    "        self.layers = self._create_node_embd_layers()\n",
    "        self.loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(self, pyg_graph, dataset):\n",
    "        acts = [pyg_graph.x]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ins = acts[-1]\n",
    "            outs = layer(ins, pyg_graph)\n",
    "            acts.append(outs)\n",
    "\n",
    "        return self._loss(acts[-1], dataset)\n",
    "\n",
    "    def _loss(self, ins, dataset):\n",
    "        pred_inds = dataset.node_ids\n",
    "        if self.pred_type == 'softmax':\n",
    "            y_preds = ins[pred_inds]\n",
    "        elif self.pred_type == 'mlp':\n",
    "            y_preds = self.mlp(ins[pred_inds])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        y_true = torch.tensor(dataset.label_inds[pred_inds], dtype=torch.long, device=device)\n",
    "        loss = self.loss(y_preds, y_true)\n",
    "        return loss, y_preds.cpu().detach().numpy()\n",
    "\n",
    "    def _create_node_embd_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            act = self.act if i < self.num_layers - 1 else 'identity'\n",
    "            layers.append(NodeEmbedding(\n",
    "                type=self.node_embd_type,\n",
    "                in_dim=self.layer_dim_list[i],\n",
    "                out_dim=self.layer_dim_list[i + 1],\n",
    "                act=act,\n",
    "                bn=self.bn,\n",
    "                dropout=self.dropout if i != 0 else False\n",
    "            ))\n",
    "        return layers\n",
    "\n",
    "    def _calc_mlp_dims(self, mlp_dim, output_dim=1):\n",
    "        dim = mlp_dim\n",
    "        dims = []\n",
    "        while dim > output_dim:\n",
    "            dim = dim // 2\n",
    "            dims.append(dim)\n",
    "        dims = dims[:-1]\n",
    "        return dims\n",
    "\n",
    "\n",
    "class NodeEmbedding(nn.Module):\n",
    "    def __init__(self, type, in_dim, out_dim, act, bn, dropout):\n",
    "        super(NodeEmbedding, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.type = type\n",
    "        if type == 'gcn':\n",
    "            self.conv = GCNConv(in_dim, out_dim)\n",
    "            self.act = create_act(act, out_dim)\n",
    "        elif type == 'gat':\n",
    "            self.conv = GATConv(in_dim, out_dim)\n",
    "            self.act = create_act(act, out_dim)\n",
    "        elif type == \"graphsage\" :\n",
    "            self.conv = CustomSAGEConv(in_dim, out_dim)\n",
    "            self.act = create_act(act, out_dim)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Unknown node embedding layer type {}'.format(type))\n",
    "        self.bn = bn\n",
    "        if self.bn:\n",
    "            self.bn = torch.nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = dropout\n",
    "        if dropout:\n",
    "            self.dropout = torch.nn.Dropout()\n",
    "\n",
    "    def forward(self, ins, pyg_graph):\n",
    "        if self.dropout:\n",
    "            ins = self.dropout(ins)\n",
    "\n",
    "        if self.type == 'gcn':\n",
    "            if use_edge_weights:\n",
    "                x = self.conv(ins, pyg_graph.edge_index, edge_weight=pyg_graph.edge_attr)\n",
    "            else:\n",
    "                if ins.is_sparse:\n",
    "                  ins = ins.to_dense()\n",
    "                if pyg_graph.edge_index.is_sparse:\n",
    "                  ins = ins.to_dense()\n",
    "                  # If your edge indices are in a tensor called 'edge_index'\n",
    "                pyg_graph.edge_index = pyg_graph.edge_index.to(device)\n",
    "\n",
    "                # Similarly for other tensors involved in computation like feature matrices\n",
    "                ins = ins.to(device)\n",
    "                x = self.conv(ins, pyg_graph.edge_index)\n",
    "        else:\n",
    "            x = self.conv(ins, pyg_graph.edge_index)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''mlp can specify number of hidden layers and hidden layer channels'''\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation_type='relu', num_hidden_lyr=2,\n",
    "                 hidden_channels=None, bn=False):\n",
    "        super().__init__()\n",
    "        self.out_dim = output_dim\n",
    "        if not hidden_channels:\n",
    "            hidden_channels = [input_dim for _ in range(num_hidden_lyr)]\n",
    "        elif len(hidden_channels) != num_hidden_lyr:\n",
    "            raise ValueError(\n",
    "                \"number of hidden layers should be the same as the lengh of hidden_channels\")\n",
    "        self.layer_channels = [input_dim] + hidden_channels + [output_dim]\n",
    "        self.activation = create_act(activation_type)\n",
    "        self.layers = nn.ModuleList(list(\n",
    "            map(self.weight_init, [nn.Linear(self.layer_channels[i], self.layer_channels[i + 1])\n",
    "                                   for i in range(len(self.layer_channels) - 1)])))\n",
    "        self.bn = bn\n",
    "        if self.bn:\n",
    "            self.bn = torch.nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def weight_init(self, m):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        return m\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_inputs = [x]\n",
    "        for layer in self.layers:\n",
    "            input = layer_inputs[-1]\n",
    "            if layer == self.layers[-1]:\n",
    "                layer_inputs.append(layer(input))\n",
    "            else:\n",
    "                layer_inputs.append(self.activation(layer(input)))\n",
    "        # model.store_layer_output(self, layer_inputs[-1])\n",
    "        if self.bn:\n",
    "            layer_inputs[-1] = self.bn(layer_inputs[-1])\n",
    "        return layer_inputs[-1]\n",
    "\n",
    "\n",
    "def create_act(act, num_parameters=None):\n",
    "    if act == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act == 'prelu':\n",
    "        return nn.PReLU(num_parameters)\n",
    "    elif act == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif act == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act == 'identity':\n",
    "        class Identity(nn.Module):\n",
    "            def forward(self, x):\n",
    "                return x\n",
    "\n",
    "        return Identity()\n",
    "    else:\n",
    "        raise ValueError('Unknown activation function {}'.format(act))\n",
    "\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\n",
    "    Classfication with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
    "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
    "            (default: :obj:`False`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
    "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}`.\n",
    "            (default: :obj:`False`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 improved=False,\n",
    "                 cached=False,\n",
    "                 bias=True):\n",
    "        super(GCNConv, self).__init__('add')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ),\n",
    "                                     dtype=dtype,\n",
    "                                     device=edge_index.device)\n",
    "        edge_weight = edge_weight.view(-1)\n",
    "        assert edge_weight.size(0) == edge_index.size(1)\n",
    "\n",
    "        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n",
    "        edge_index = add_self_loops(edge_index, num_nodes)\n",
    "        # Calling add_self_loops with optional edge_attr:\n",
    "        # edge_attr = torch.ones([edge_index.shape[1]], dtype=torch.float32)  # Assuming one attribute per edge\n",
    "\n",
    "        # edge_index, edge_attr = add_self_loops(edge_index, edge_attr=edge_attr if edge_attr is not None else None)\n",
    "\n",
    "        loop_weight = torch.full((num_nodes, ),\n",
    "                                 1 if not improved else 2,\n",
    "                                 dtype=edge_weight.dtype,\n",
    "                                 device=edge_weight.device)\n",
    "        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        if x.is_sparse:\n",
    "            x = torch.sparse.mm(x, self.weight)\n",
    "        else:\n",
    "            x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\n",
    "                                            self.improved, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n",
    "\n",
    "class GATConv(MessagePassing):\n",
    "    r\"\"\"The graph attentional operator from the `\"Graph Attention Networks\"\n",
    "    <https://arxiv.org/abs/1710.10903>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{j} +\n",
    "        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\n",
    "\n",
    "    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\n",
    "\n",
    "    .. math::\n",
    "        \\alpha_{i,j} =\n",
    "        \\frac{\n",
    "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n",
    "        \\right)\\right)}\n",
    "        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n",
    "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n",
    "        \\right)\\right)}.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        heads (int, optional): Number of multi-head-attentions. (default:\n",
    "            :obj:`1`)\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
    "        attentions are averaged instead of concatenated. (default: :obj:`True`)\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
    "            slope. (default: :obj:`0.2`)\n",
    "        dropout (float, optional): Dropout probability of the normalized\n",
    "            attention coefficients which exposes each node to a stochastically\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 heads=1,\n",
    "                 concat=True,\n",
    "                 negative_slope=0.2,\n",
    "                 dropout=0,\n",
    "                 bias=True):\n",
    "        super(GATConv, self).__init__('add')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.weight = Parameter(\n",
    "            torch.Tensor(in_channels, heads * out_channels))\n",
    "        self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\"\"\"\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        if x.is_sparse:\n",
    "            x = torch.sparse.mm(x, self.weight).view(-1, self.heads, self.out_channels)\n",
    "        else:\n",
    "            x = torch.matmul(x, self.weight).view(-1, self.heads, self.out_channels)\n",
    "        return self.propagate(edge_index, x=x, num_nodes=x.size(0))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index, num_nodes):\n",
    "        # Compute attention coefficients.\n",
    "        alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, edge_index[0], num_nodes)\n",
    "\n",
    "        # Sample attention coefficients stochastically.\n",
    "        if self.training and self.dropout > 0:\n",
    "            alpha = F.dropout(alpha, p=self.dropout, training=True)\n",
    "\n",
    "        return x_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.concat is True:\n",
    "            aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            aggr_out = aggr_out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3ymWi6av5m_B"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_model(dataset):\n",
    "    name = model  # Directly use the global variable 'model'\n",
    "    layer_info = model_params  # Directly use the global dictionary 'model_params'\n",
    "    if name in model_ctors:\n",
    "        return model_ctors[name](layer_info, dataset)\n",
    "    else:\n",
    "        raise ValueError(\"Model not implemented {}\".format(name))\n",
    "\n",
    "\n",
    "\n",
    "def create_text_gnn(layer_info, dataset):\n",
    "\n",
    "    lyr_dims = layer_info[\"layer_dims\"]\n",
    "    lyr_dims = [dataset.node_feats.shape[1]] + lyr_dims\n",
    "    weights = None\n",
    "    if layer_info[\"class_weights\"] == True:\n",
    "        counts = Counter(dataset.label_inds[dataset.node_ids])\n",
    "        weights = len(counts) * [0]\n",
    "        min_weight = min(counts.values())\n",
    "        for k, v in counts.items():\n",
    "            weights[k] = min_weight / float(v)\n",
    "        weights = torch.tensor(weights, device=device)\n",
    "\n",
    "    return TextGNN(\n",
    "        pred_type=layer_info[\"pred_type\"],\n",
    "        node_embd_type=layer_info[\"node_embd\"],\n",
    "        num_layers=int(layer_info[\"num_layers\"]),\n",
    "        layer_dim_list=lyr_dims,\n",
    "        act=layer_info[\"act\"],\n",
    "        bn=False,\n",
    "        num_labels=len(dataset.label_dict),\n",
    "        class_weights=weights,\n",
    "        dropout=layer_info[\"dropout\"]\n",
    "    )\n",
    "\n",
    "\n",
    "model_ctors = {\n",
    "    'TextGNN': create_text_gnn,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "prsKtlIV_HEc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def eval(preds, dataset, test=False):\n",
    "    y_true = dataset.label_inds[dataset.node_ids]\n",
    "    y_pred_label = np.asarray([np.argmax(pred) for pred in preds])\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred_label)\n",
    "    f1_weighted = metrics.f1_score(y_true, y_pred_label, average='weighted')\n",
    "    f1_macro = metrics.f1_score(y_true, y_pred_label, average='macro')\n",
    "    f1_micro = metrics.f1_score(y_true, y_pred_label, average='micro')\n",
    "    precision_weighted = metrics.precision_score(y_true, y_pred_label, average='weighted')\n",
    "    precision_macro = metrics.precision_score(y_true, y_pred_label, average='macro')\n",
    "    precision_micro = metrics.precision_score(y_true, y_pred_label, average='micro')\n",
    "    recall_weighted = metrics.recall_score(y_true, y_pred_label, average='weighted')\n",
    "    recall_macro = metrics.recall_score(y_true, y_pred_label, average='macro')\n",
    "    recall_micro = metrics.recall_score(y_true, y_pred_label, average='micro')\n",
    "    results = {\"accuracy\": accuracy,\n",
    "               \"f1_weighted\": f1_weighted,\n",
    "               \"f1_macro\": f1_macro,\n",
    "               \"f1_micro\": f1_micro,\n",
    "               \"precision_weighted\": precision_weighted,\n",
    "               \"precision_macro\": precision_macro,\n",
    "               \"precision_micro\": precision_micro,\n",
    "               \"recall_weighted\": recall_weighted,\n",
    "               \"recall_macro\": recall_macro,\n",
    "               \"recall_micro\": recall_micro\n",
    "               }\n",
    "    if test:\n",
    "        one_hot_true = np.zeros((y_true.size, len(dataset.label_dict)))\n",
    "        one_hot_true[np.arange(y_true.size), y_true] = 1\n",
    "        results[\"y_true\"] = one_hot_true\n",
    "        one_hot_pred = np.zeros((y_true.size, len(dataset.label_dict)))\n",
    "        one_hot_pred[np.arange(y_pred_label.size),y_pred_label] = 1\n",
    "        results[\"y_pred\"] = one_hot_pred\n",
    "    return results\n",
    "\n",
    "\n",
    "class MovingAverage(object):\n",
    "    def __init__(self, window, want_increase=True):\n",
    "        self.moving_avg = [float('-inf')] if want_increase else [float('inf')]\n",
    "        self.want_increase = want_increase\n",
    "        self.results = []\n",
    "        self.window = window\n",
    "\n",
    "    def add_to_moving_avg(self, x):\n",
    "        self.results.append(x)\n",
    "        if len(self.results) >= self.window:\n",
    "            next_val = sum(self.results[-self.window:]) / self.window\n",
    "            self.moving_avg.append(next_val)\n",
    "\n",
    "    def best_result(self, x):\n",
    "        if self.want_increase:\n",
    "            return (x - 1e-7) > max(self.results)\n",
    "        else:\n",
    "            return (x + 1e-7) < min(self.results)\n",
    "\n",
    "    def stop(self):\n",
    "        if len(self.moving_avg) < 2:\n",
    "            return False\n",
    "        if self.want_increase:\n",
    "            return (self.moving_avg[-1] + 1e-7) < self.moving_avg[-2]\n",
    "        else:\n",
    "            return (self.moving_avg[-2] + 1e-7) < self.moving_avg[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EuKe9AUyAEI4"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from os.path import join, getctime\n",
    "import torch\n",
    "\n",
    "class Saver(object):\n",
    "    def __init__(self):\n",
    "        global logs\n",
    "        model_str = self.get_model_str()\n",
    "        self.logdir = join(\n",
    "            '/home/cpsc477_sw2349/project/content/MyDrive/CPSC_577_FP/logs',  # 使用 Colab 的默认工作目录下的 logs 目录\n",
    "            '{}_{}'.format(model_str, get_ts()))\n",
    "        create_dir_if_not_exists(self.logdir)\n",
    "        self.model_info_f = self._open('model_info.txt')\n",
    "        if logs:\n",
    "            print('Logging to {}'.format(self.logdir))\n",
    "\n",
    "    def save_trained_model(self, trained_model, epoch=None):\n",
    "        epoch = \"_epoch_{}\".format(epoch) if epoch is not None else \"\"\n",
    "        p = join(self.logdir, 'trained_model{}.pt'.format(epoch))\n",
    "        torch.save(trained_model.state_dict(), p)\n",
    "        if logs:\n",
    "            print('Trained model saved to {}'.format(p))\n",
    "\n",
    "    def load_trained_model(self, train_data):\n",
    "        p = join(self.logdir, 'trained_model*')\n",
    "        files = glob.glob(p)\n",
    "        best_trained_model_path = max(files, key=getctime)\n",
    "        trained_model = create_model(train_data)\n",
    "        trained_model.load_state_dict(\n",
    "            torch.load(best_trained_model_path, map_location=device))\n",
    "        trained_model.to(device)\n",
    "        return trained_model\n",
    "\n",
    "    def get_model_str(self):\n",
    "        li = []\n",
    "        key_flags = [model, dataset, \"_\".join([str(i) for i in tvt_ratio])]\n",
    "        for f in key_flags:\n",
    "            li.append(str(f))\n",
    "        return '_'.join(li)\n",
    "\n",
    "    def _open(self, f):\n",
    "        return open(join(self.logdir, f), 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "31ixQVgSCIeK"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    global dataset\n",
    "    dir = join(get_save_path(), 'split')\n",
    "    dataset_name = dataset\n",
    "    train_ratio = int(tvt_ratio[0] * 100)\n",
    "    val_ratio = int(tvt_ratio[1] * 100)\n",
    "    test_ratio = 100 - train_ratio - val_ratio\n",
    "    if 'presplit' not in dataset_name:\n",
    "        save_fn = '{}_train_{}_val_{}_test_{}_seed_{}_window_size_{}_SAGE'.format(dataset_name, train_ratio,\n",
    "                                                              val_ratio, test_ratio,\n",
    "                                                              random_seed, word_window_size)\n",
    "    else:\n",
    "        save_fn = '{}_train_val_test_{}_window_size_{}_SAGE'.format(dataset_name, random_seed, word_window_size)\n",
    "    path = join(dir, save_fn)\n",
    "    rtn = load(path)\n",
    "    if rtn:\n",
    "        train_data, val_data, test_data = rtn['train_data'], rtn['val_data'], rtn['test_data']\n",
    "    else:\n",
    "        train_data, val_data, test_data = _load_tvt_data_helper()\n",
    "        save({'train_data': train_data, 'val_data': val_data, 'test_data': test_data}, path)\n",
    "    dataset = dataset\n",
    "    if \"small\" in dataset or \"presplit\" in dataset or 'sentiment' in dataset:\n",
    "        dataset_name = \"_\".join(dataset.split(\"_\")[:-1])\n",
    "    else:\n",
    "        dataset_name = dataset\n",
    "\n",
    "    orig_text_path = join(get_corpus_path(), dataset_name + \"_sentences.txt\")\n",
    "    orig_text_path = ensure_prefix(orig_text_path)\n",
    "    raw_doc_list = []\n",
    "    f = open(orig_text_path, 'rb')\n",
    "    for line in f.readlines():\n",
    "        raw_doc_list.append(line.strip().decode())\n",
    "    f.close()\n",
    "\n",
    "    return train_data, val_data, test_data, raw_doc_list\n",
    "\n",
    "\n",
    "def _load_tvt_data_helper():\n",
    "    global dataset\n",
    "    dir = join(get_save_path(), 'all')\n",
    "    path = join(dir, dataset + '_all_window_' + str(word_window_size))\n",
    "    rtn = load(path)\n",
    "    if rtn:\n",
    "        dataset = TextDataset(None, None, None, None, None, None, rtn)\n",
    "    else:\n",
    "        dataset = build_text_graph_dataset(dataset, word_window_size)\n",
    "        gc.collect()\n",
    "        save(dataset.__dict__, path)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = dataset.tvt_split(tvt_ratio[:2], tvt_list, random_seed)\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "omxsss62_tpo"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "\n",
    "def train(train_data, val_data, saver, logs = True):\n",
    "    train_data.init_node_feats(init_type, device)\n",
    "    val_data.init_node_feats(init_type, device)\n",
    "    model = create_model(train_data)\n",
    "    model = model.to(device)\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Number params: \", pytorch_total_params)\n",
    "    moving_avg = MovingAverage(validation_window_size, validation_metric != 'loss')\n",
    "    pyg_graph = train_data.get_pyg_graph(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        loss, preds_train = model(pyg_graph, train_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        with torch.no_grad():\n",
    "            val_loss, preds_val = model(pyg_graph, val_data)\n",
    "            val_loss = val_loss.item()\n",
    "            eval_res_val = eval(preds_val, val_data)\n",
    "            if logs:\n",
    "                print(\"Epoch: {:04d}, Train Loss: {:.5f}, Time: {:.5f}\".format(epoch, loss, time.time() - t))\n",
    "                print(\"Val Loss: {:.5f}\".format(val_loss))\n",
    "                print(\"Val Results: ...\")\n",
    "                pprint(eval_res_val)\n",
    "            eval_res_val[\"loss\"] = val_loss\n",
    "\n",
    "            if len(moving_avg.results) == 0 or moving_avg.best_result(eval_res_val[validation_metric]):\n",
    "                saver.save_trained_model(model, epoch + 1)\n",
    "            moving_avg.add_to_moving_avg(eval_res_val[validation_metric])\n",
    "            if moving_avg.stop():\n",
    "                break\n",
    "    best_model = saver.load_trained_model(train_data)\n",
    "    return best_model, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning R8 Dataset\n",
    "\n",
    "We perform hyperparameter tuning by grid search. \n",
    "Possible hyperparameters to tune are :\n",
    "- pred_type\n",
    "- Learning rate\n",
    "- activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with lr=0.01, pred_type=softmax, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "New best model with accuracy 0.944266788487894 and params (0.01, 'softmax', True, 'relu')\n",
      "Testing with lr=0.01, pred_type=softmax, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074408\n",
      "Testing with lr=0.01, pred_type=softmax, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.01, pred_type=softmax, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "New best model with accuracy 0.9470077661032434 and params (0.01, 'softmax', True, 'tanh')\n",
      "Testing with lr=0.01, pred_type=softmax, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "New best model with accuracy 0.9616263133851074 and params (0.01, 'softmax', False, 'relu')\n",
      "Testing with lr=0.01, pred_type=softmax, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074408\n",
      "New best model with accuracy 0.9661946094106898 and params (0.01, 'softmax', False, 'prelu')\n",
      "Testing with lr=0.01, pred_type=softmax, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.01, pred_type=softmax, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.01, pred_type=mlp, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.01, pred_type=mlp, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074480\n",
      "Testing with lr=0.01, pred_type=mlp, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.01, pred_type=mlp, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.01, pred_type=mlp, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.01, pred_type=mlp, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074480\n",
      "Testing with lr=0.01, pred_type=mlp, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.01, pred_type=mlp, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.001, pred_type=softmax, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.001, pred_type=softmax, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074408\n",
      "Testing with lr=0.001, pred_type=softmax, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.001, pred_type=softmax, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.001, pred_type=softmax, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.001, pred_type=softmax, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074408\n",
      "Testing with lr=0.001, pred_type=softmax, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.001, pred_type=softmax, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.001, pred_type=mlp, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.001, pred_type=mlp, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074480\n",
      "Testing with lr=0.001, pred_type=mlp, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.001, pred_type=mlp, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.001, pred_type=mlp, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.001, pred_type=mlp, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074480\n",
      "Testing with lr=0.001, pred_type=mlp, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.001, pred_type=mlp, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.0001, pred_type=softmax, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.0001, pred_type=softmax, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074408\n",
      "Testing with lr=0.0001, pred_type=softmax, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.0001, pred_type=softmax, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.0001, pred_type=softmax, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.0001, pred_type=softmax, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074408\n",
      "Testing with lr=0.0001, pred_type=softmax, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.0001, pred_type=softmax, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074208\n",
      "Testing with lr=0.0001, pred_type=mlp, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.0001, pred_type=mlp, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074480\n",
      "Testing with lr=0.0001, pred_type=mlp, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.0001, pred_type=mlp, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.0001, pred_type=mlp, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.0001, pred_type=mlp, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074480\n",
      "Testing with lr=0.0001, pred_type=mlp, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Testing with lr=0.0001, pred_type=mlp, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  3074280\n",
      "Best Parameters: (0.01, 'softmax', False, 'prelu')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "debug = False\n",
    "gpu = -1\n",
    "use_comet_ml = False\n",
    "\n",
    "random_seed = 123\n",
    "dataset = 'r8_presplit'\n",
    "\n",
    "if 'twitter_asian_prejudice' in dataset:\n",
    "    if 'sentiment' in dataset:\n",
    "        num_labels = 2\n",
    "    else:\n",
    "        num_labels = 4\n",
    "elif 'r8' in dataset:\n",
    "    num_labels = 8\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'lr': [1e-2, 1e-3, 1e-4],\n",
    "    'pred_type': ['softmax', 'mlp'],\n",
    "    'dropout': [True, False],\n",
    "    'act' : ['relu', 'prelu', 'sigmoid', 'tanh']\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sampling\n",
    "\"\"\"\n",
    "word_window_size = 10\n",
    "validation_window_size = 10\n",
    "\n",
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "validation_metric = \"accuracy\"  # Alternatively, \"f1_weighted\" or \"loss\"\n",
    "\n",
    "use_best_val_model_for_inference = True\n",
    "\n",
    "\"\"\"\n",
    "Evaluation.\n",
    "\"\"\"\n",
    "tvt_ratio = [0.8, 0.1, 0.1]\n",
    "tvt_list = [\"train\", \"test\", \"val\"]\n",
    "model = \"TextGNN\"\n",
    "device = 'cuda'#.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu'\n",
    "    \n",
    "num_epochs = 2 if debug else 400\n",
    "lr = 2e-2\n",
    "use_edge_weights = False\n",
    "init_type = 'one_hot_init'\n",
    "logs = False\n",
    "model_params = {}\n",
    "# Function to create a model with specified hyperparameters\n",
    "def run_experiment(Learning_Rate, Pred_Type, Dropout, Act):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    global lr, model, num_labels, dataset, use_edge_weights, init_type, model_params\n",
    "    lr = Learning_Rate\n",
    "\n",
    "    if model == 'TextGNN':\n",
    "        pred_type = Pred_Type\n",
    "        node_embd_type = 'graphsage'\n",
    "        layer_dim_list = [200, num_labels]\n",
    "        num_layers = len(layer_dim_list)\n",
    "        class_weights = True\n",
    "        dropout = Dropout\n",
    "        s = 'TextGraphSAGE:pred_type={},node_embd_type={},num_layers={},layer_dim_list={},act={},' \\\n",
    "            'dropout={},class_weights={}'.format(\n",
    "            pred_type, node_embd_type, num_layers, \"_\".join([str(i) for i in layer_dim_list]), Act, dropout, class_weights\n",
    "        )\n",
    "        model_params = {\n",
    "            'pred_type': pred_type,\n",
    "            'node_embd':  node_embd_type,\n",
    "            'num_layers': num_layers,\n",
    "            'layer_dims': layer_dim_list,\n",
    "            'act': Act,\n",
    "            'class_weights': class_weights,\n",
    "            'dropout': dropout\n",
    "        }\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    print(\"{}: {}\\n\".format(model, model_params))\n",
    "    saver = Saver()\n",
    "    train_data, val_data, test_data, raw_doc_list = load_data()\n",
    "    \n",
    "    saved_model, model = train(train_data, val_data, saver, False)\n",
    "    with torch.no_grad():\n",
    "        test_loss_model, preds_model = model(train_data.get_pyg_graph(device=device), test_data)\n",
    "\n",
    "    eval_res = eval(preds_model, test_data, True)\n",
    "    y_true = eval_res.pop('y_true')\n",
    "    y_pred = eval_res.pop('y_pred')\n",
    "    acc = eval_res['accuracy']\n",
    "    model = \"TextGNN\"\n",
    "    return acc\n",
    "\n",
    "# Perform grid search\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "results = defaultdict(list)\n",
    "\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "    Learning_Rate, Pred_Type, Dropout, Act = params\n",
    "    print(f\"Testing with lr={Learning_Rate}, pred_type={Pred_Type}, act={Act}, dropout={Dropout}\")\n",
    "    \n",
    "    acc = run_experiment(Learning_Rate, Pred_Type, Dropout, Act)\n",
    "    results[(Learning_Rate, Pred_Type, Dropout, Act)].append(acc)\n",
    "    \n",
    "    # Track the best parameters\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = (Learning_Rate, Pred_Type, Dropout, Act)\n",
    "        print(f\"New best model with accuracy {best_acc} and params {best_params}\")\n",
    "\n",
    "# Print best found parameters\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training and testing with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASbR0ZDG06yE",
    "outputId": "3e9bef98-ea8d-4d64-f6f6-af8f07733393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 8], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "\"\"\"\n",
    "Most Relevant\n",
    "\"\"\"\n",
    "\n",
    "debug = False\n",
    "gpu = -1\n",
    "use_comet_ml = False\n",
    "logs = False\n",
    "\n",
    "\"\"\"\n",
    "dataset:\n",
    " sentiment suffix for twitter means the negative classes of the original dataset are combined and the other classes are combined for sentiment analysis\n",
    " presplit suffix means training and test are predetermined in [dataset]_labels.txt\n",
    " small suffix means a very small dataset used for debugging\n",
    "\"\"\"\n",
    "random_seed = 123\n",
    "dataset = 'r8_presplit'\n",
    "# dataset = 'ag_presplit'\n",
    "\n",
    "if 'ag' in dataset:\n",
    "    num_labels = 4\n",
    "elif 'r8' in dataset:\n",
    "    num_labels = 8\n",
    "\n",
    "\"\"\"\n",
    "Model. Pt1\n",
    "\"\"\"\n",
    "\n",
    "model = \"TextGNN\"\n",
    "\n",
    "model_params = {}\n",
    "use_edge_weights = False\n",
    "init_type = 'one_hot_init'\n",
    "if model == 'TextGNN':\n",
    "    pred_type = 'softmax'\n",
    "    node_embd_type = 'graphsage'\n",
    "    layer_dim_list = [200, num_labels]\n",
    "    num_layers = len(layer_dim_list)\n",
    "    class_weights = True\n",
    "    dropout = False\n",
    "    s = 'TextGraphSAGE:pred_type={},node_embd_type={},num_layers={},layer_dim_list={},act={},' \\\n",
    "        'dropout={},class_weights={}'.format(\n",
    "        pred_type, node_embd_type, num_layers, \"_\".join([str(i) for i in layer_dim_list]), 'relu', dropout, class_weights\n",
    "    )\n",
    "    model_params = {\n",
    "        'pred_type': pred_type,\n",
    "        'node_embd':  node_embd_type,\n",
    "        'num_layers': num_layers,\n",
    "        'layer_dims': layer_dim_list,\n",
    "        'act': 'prelu',\n",
    "        'class_weights': class_weights,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "print(\"{}: {}\\n\".format(model, model_params))\n",
    "\n",
    "\"\"\"\n",
    "Sampling\n",
    "\"\"\"\n",
    "word_window_size = 10\n",
    "validation_window_size = 10\n",
    "\n",
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "validation_metric = \"accuracy\"  # Alternatively, \"f1_weighted\" or \"loss\"\n",
    "\n",
    "use_best_val_model_for_inference = True\n",
    "\n",
    "\"\"\"\n",
    "Evaluation.\n",
    "\"\"\"\n",
    "tvt_ratio = [0.8, 0.1, 0.1]\n",
    "tvt_list = [\"train\", \"test\", \"val\"]\n",
    "\n",
    "\"\"\"\n",
    "Optimization.\n",
    "\"\"\"\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "device = 'cuda'#.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu'\n",
    "\n",
    "num_epochs = 2 if debug else 400\n",
    "\n",
    "\"\"\"\n",
    "Other info.\n",
    "\"\"\"\n",
    "# Assuming get_user() and get_host() are function calls that need to be defined or imported\n",
    "user = get_user()\n",
    "hostname = get_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number params:  3074408\n",
      "Test...experiment  1\n",
      "{'accuracy': 0.9643672910004568,\n",
      " 'f1_macro': 0.9038464848725574,\n",
      " 'f1_micro': 0.9643672910004568,\n",
      " 'f1_weighted': 0.9645816313775437,\n",
      " 'precision_macro': 0.8904182462664343,\n",
      " 'precision_micro': 0.9643672910004568,\n",
      " 'precision_weighted': 0.9654787119766619,\n",
      " 'recall_macro': 0.9232344870902105,\n",
      " 'recall_micro': 0.9643672910004568,\n",
      " 'recall_weighted': 0.9643672910004568}\n",
      "Number params:  3074408\n",
      "Test...experiment  2\n",
      "{'accuracy': 0.9597989949748744,\n",
      " 'f1_macro': 0.8904713473916541,\n",
      " 'f1_micro': 0.9597989949748744,\n",
      " 'f1_weighted': 0.9602068516576928,\n",
      " 'precision_macro': 0.8693514559060807,\n",
      " 'precision_micro': 0.9597989949748744,\n",
      " 'precision_weighted': 0.9612201274827927,\n",
      " 'recall_macro': 0.9185654086724129,\n",
      " 'recall_micro': 0.9597989949748744,\n",
      " 'recall_weighted': 0.9597989949748744}\n",
      "Number params:  3074408\n",
      "Test...experiment  3\n",
      "{'accuracy': 0.9625399725902238,\n",
      " 'f1_macro': 0.9013959170621434,\n",
      " 'f1_micro': 0.9625399725902238,\n",
      " 'f1_weighted': 0.9628446002683398,\n",
      " 'precision_macro': 0.8829283048611056,\n",
      " 'precision_micro': 0.9625399725902238,\n",
      " 'precision_weighted': 0.9639584461811144,\n",
      " 'recall_macro': 0.9277812808212788,\n",
      " 'recall_micro': 0.9625399725902238,\n",
      " 'recall_weighted': 0.9625399725902238}\n",
      "Number params:  3074408\n",
      "Test...experiment  4\n",
      "{'accuracy': 0.964824120603015,\n",
      " 'f1_macro': 0.9064056295729248,\n",
      " 'f1_micro': 0.964824120603015,\n",
      " 'f1_weighted': 0.9651573182344916,\n",
      " 'precision_macro': 0.8831526904133009,\n",
      " 'precision_micro': 0.964824120603015,\n",
      " 'precision_weighted': 0.965978282729918,\n",
      " 'recall_macro': 0.9362539661164714,\n",
      " 'recall_micro': 0.964824120603015,\n",
      " 'recall_weighted': 0.964824120603015}\n",
      "Number params:  3074408\n",
      "Test...experiment  5\n",
      "{'accuracy': 0.9634536317953404,\n",
      " 'f1_macro': 0.8966655538152684,\n",
      " 'f1_micro': 0.9634536317953404,\n",
      " 'f1_weighted': 0.9638531836480979,\n",
      " 'precision_macro': 0.8783526130148263,\n",
      " 'precision_micro': 0.9634536317953404,\n",
      " 'precision_weighted': 0.9652230542790735,\n",
      " 'recall_macro': 0.9258372994498048,\n",
      " 'recall_micro': 0.9634536317953404,\n",
      " 'recall_weighted': 0.9634536317953404}\n",
      "accuracy: Mean=0.962997, Std=0.001781\n",
      "f1_weighted: Mean=0.963329, Std=0.001742\n",
      "f1_macro: Mean=0.899757, Std=0.005646\n",
      "f1_micro: Mean=0.962997, Std=0.001781\n",
      "precision_weighted: Mean=0.964372, Std=0.001711\n",
      "precision_macro: Mean=0.880841, Std=0.006923\n",
      "precision_micro: Mean=0.962997, Std=0.001781\n",
      "recall_weighted: Mean=0.962997, Std=0.001781\n",
      "recall_macro: Mean=0.926334, Std=0.005843\n",
      "recall_micro: Mean=0.962997, Std=0.001781\n"
     ]
    }
   ],
   "source": [
    "dataset = 'r8_presplit'\n",
    "num_experiments = 5\n",
    "random_seeds = [33, 15, 86, 109, 78]\n",
    "model = \"TextGNN\"\n",
    "all_experiment_results = []\n",
    "for exp in range(num_experiments):\n",
    "    random_seed = random_seeds[exp]\n",
    "    saver = Saver()\n",
    "    train_data, val_data, test_data, raw_doc_list = load_data()\n",
    "    \n",
    "    saved_model, model = train(train_data, val_data, saver, False)\n",
    "    with torch.no_grad():\n",
    "        test_loss_model, preds_model = model(train_data.get_pyg_graph(device=device), test_data)\n",
    "    eval_res = eval(preds_model, test_data, True)\n",
    "    y_true = eval_res.pop('y_true')\n",
    "    y_pred = eval_res.pop('y_pred')\n",
    "    print(\"Test...experiment \", exp+1)\n",
    "    pprint(eval_res)\n",
    "    all_experiment_results.append(eval_res)\n",
    "    model = \"TextGNN\"\n",
    "    \n",
    "# Calculate mean and standard deviation across experiments\n",
    "final_metrics = {key: [] for key in all_experiment_results[0]}\n",
    "for results in all_experiment_results:\n",
    "    for key in results:\n",
    "        final_metrics[key].append(results[key])\n",
    "\n",
    "for metric in final_metrics:\n",
    "    values = np.array(final_metrics[metric])\n",
    "    mean = values.mean()\n",
    "    std = values.std()\n",
    "    print(f'{metric}: Mean={mean:.6f}, Std={std:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Twitter Dataset\n",
    "\n",
    "We perform hyperparameter tuning by grid search. \n",
    "Possible hyperparameters to tune are :\n",
    "- pred_type\n",
    "- Learning rate\n",
    "- activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with lr=0.01, pred_type=softmax, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "New best model with accuracy 0.7325 and params (0.01, 'softmax', True, 'relu')\n",
      "Testing with lr=0.01, pred_type=softmax, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212604\n",
      "Testing with lr=0.01, pred_type=softmax, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.01, pred_type=softmax, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.01, pred_type=softmax, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "New best model with accuracy 0.733 and params (0.01, 'softmax', False, 'relu')\n",
      "Testing with lr=0.01, pred_type=softmax, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212604\n",
      "Testing with lr=0.01, pred_type=softmax, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.01, pred_type=softmax, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.01, pred_type=mlp, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.01, pred_type=mlp, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212624\n",
      "Testing with lr=0.01, pred_type=mlp, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.01, pred_type=mlp, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.01, pred_type=mlp, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.01, pred_type=mlp, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212624\n",
      "Testing with lr=0.01, pred_type=mlp, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.01, pred_type=mlp, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.001, pred_type=softmax, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.001, pred_type=softmax, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212604\n",
      "Testing with lr=0.001, pred_type=softmax, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.001, pred_type=softmax, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.001, pred_type=softmax, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.001, pred_type=softmax, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212604\n",
      "Testing with lr=0.001, pred_type=softmax, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.001, pred_type=softmax, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "New best model with accuracy 0.7525 and params (0.001, 'softmax', False, 'tanh')\n",
      "Testing with lr=0.001, pred_type=mlp, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.001, pred_type=mlp, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212624\n",
      "Testing with lr=0.001, pred_type=mlp, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.001, pred_type=mlp, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.001, pred_type=mlp, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.001, pred_type=mlp, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212624\n",
      "Testing with lr=0.001, pred_type=mlp, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.001, pred_type=mlp, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.0001, pred_type=softmax, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.0001, pred_type=softmax, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212604\n",
      "Testing with lr=0.0001, pred_type=softmax, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.0001, pred_type=softmax, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.0001, pred_type=softmax, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.0001, pred_type=softmax, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212604\n",
      "Testing with lr=0.0001, pred_type=softmax, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.0001, pred_type=softmax, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212404\n",
      "Testing with lr=0.0001, pred_type=mlp, act=relu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.0001, pred_type=mlp, act=prelu, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212624\n",
      "Testing with lr=0.0001, pred_type=mlp, act=sigmoid, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.0001, pred_type=mlp, act=tanh, dropout=True\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': True}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.0001, pred_type=mlp, act=relu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'relu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.0001, pred_type=mlp, act=prelu, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'prelu', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212624\n",
      "Testing with lr=0.0001, pred_type=mlp, act=sigmoid, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'sigmoid', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Testing with lr=0.0001, pred_type=mlp, act=tanh, dropout=False\n",
      "TextGNN: {'pred_type': 'mlp', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n",
      "Number params:  5212424\n",
      "Best Parameters: (0.001, 'softmax', False, 'tanh')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "debug = False\n",
    "gpu = -1\n",
    "use_comet_ml = False\n",
    "\n",
    "random_seed = 123\n",
    "dataset = 'twitter_asian_prejudice'\n",
    "\n",
    "if 'twitter_asian_prejudice' in dataset:\n",
    "    if 'sentiment' in dataset:\n",
    "        num_labels = 2\n",
    "    else:\n",
    "        num_labels = 4\n",
    "elif 'r8' in dataset:\n",
    "    num_labels = 8\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'lr': [1e-2, 1e-3, 1e-4],\n",
    "    'pred_type': ['softmax', 'mlp'],\n",
    "    'dropout': [True, False],\n",
    "    'act' : ['relu', 'prelu', 'sigmoid', 'tanh']\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sampling\n",
    "\"\"\"\n",
    "word_window_size = 10\n",
    "validation_window_size = 10\n",
    "\n",
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "validation_metric = \"accuracy\"  # Alternatively, \"f1_weighted\" or \"loss\"\n",
    "\n",
    "use_best_val_model_for_inference = True\n",
    "\n",
    "\"\"\"\n",
    "Evaluation.\n",
    "\"\"\"\n",
    "tvt_ratio = [0.8, 0.1, 0.1]\n",
    "tvt_list = [\"train\", \"test\", \"val\"]\n",
    "model = \"TextGNN\"\n",
    "device = 'cuda'#.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu'\n",
    "    \n",
    "num_epochs = 2 if debug else 400\n",
    "lr = 2e-2\n",
    "use_edge_weights = False\n",
    "init_type = 'one_hot_init'\n",
    "logs = False\n",
    "model_params = {}\n",
    "# Function to create a model with specified hyperparameters\n",
    "def run_experiment(Learning_Rate, Pred_Type, Dropout, Act):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    global lr, model, num_labels, dataset, use_edge_weights, init_type, model_params\n",
    "    lr = Learning_Rate\n",
    "\n",
    "    if model == 'TextGNN':\n",
    "        pred_type = Pred_Type\n",
    "        node_embd_type = 'graphsage'\n",
    "        layer_dim_list = [200, num_labels]\n",
    "        num_layers = len(layer_dim_list)\n",
    "        class_weights = True\n",
    "        dropout = Dropout\n",
    "        s = 'TextGraphSAGE:pred_type={},node_embd_type={},num_layers={},layer_dim_list={},act={},' \\\n",
    "            'dropout={},class_weights={}'.format(\n",
    "            pred_type, node_embd_type, num_layers, \"_\".join([str(i) for i in layer_dim_list]), Act, dropout, class_weights\n",
    "        )\n",
    "        model_params = {\n",
    "            'pred_type': pred_type,\n",
    "            'node_embd':  node_embd_type,\n",
    "            'num_layers': num_layers,\n",
    "            'layer_dims': layer_dim_list,\n",
    "            'act': Act,\n",
    "            'class_weights': class_weights,\n",
    "            'dropout': dropout\n",
    "        }\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    print(\"{}: {}\\n\".format(model, model_params))\n",
    "    saver = Saver()\n",
    "    train_data, val_data, test_data, raw_doc_list = load_data()\n",
    "    \n",
    "    saved_model, model = train(train_data, val_data, saver, False)\n",
    "    with torch.no_grad():\n",
    "        test_loss_model, preds_model = model(train_data.get_pyg_graph(device=device), test_data)\n",
    "\n",
    "    eval_res = eval(preds_model, test_data, True)\n",
    "    y_true = eval_res.pop('y_true')\n",
    "    y_pred = eval_res.pop('y_pred')\n",
    "    acc = eval_res['accuracy']\n",
    "    model = \"TextGNN\"\n",
    "    return acc\n",
    "\n",
    "# Perform grid search\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "results = defaultdict(list)\n",
    "\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "    Learning_Rate, Pred_Type, Dropout, Act = params\n",
    "    print(f\"Testing with lr={Learning_Rate}, pred_type={Pred_Type}, act={Act}, dropout={Dropout}\")\n",
    "    \n",
    "    acc = run_experiment(Learning_Rate, Pred_Type, Dropout, Act)\n",
    "    results[(Learning_Rate, Pred_Type, Dropout, Act)].append(acc)\n",
    "    \n",
    "    # Track the best parameters\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = (Learning_Rate, Pred_Type, Dropout, Act)\n",
    "        print(f\"New best model with accuracy {best_acc} and params {best_params}\")\n",
    "\n",
    "# Print best found parameters\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEbqPvVekAco",
    "outputId": "4c78995e-7a06-4727-c7f4-7373a0393264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextGNN: {'pred_type': 'softmax', 'node_embd': 'graphsage', 'num_layers': 2, 'layer_dims': [200, 4], 'act': 'tanh', 'class_weights': True, 'dropout': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "\"\"\"\n",
    "Most Relevant\n",
    "\"\"\"\n",
    "\n",
    "debug = False\n",
    "gpu = -1\n",
    "use_comet_ml = False\n",
    "\n",
    "\"\"\"\n",
    "dataset:\n",
    " sentiment suffix for twitter means the negative classes of the original dataset are combined and the other classes are combined for sentiment analysis\n",
    " presplit suffix means training and test are predetermined in [dataset]_labels.txt\n",
    " small suffix means a very small dataset used for debugging\n",
    "\"\"\"\n",
    "random_seed = 123\n",
    "# dataset = 'r8_presplit'\n",
    "# dataset = 'ag_presplit'\n",
    "dataset = 'twitter_asian_prejudice'\n",
    "if 'twitter_asian_prejudice' in dataset:\n",
    "    if 'sentiment' in dataset:\n",
    "        num_labels = 2\n",
    "    else:\n",
    "        num_labels = 4\n",
    "elif 'r8' in dataset:\n",
    "    num_labels = 8\n",
    "\n",
    "\"\"\"\n",
    "Model. Pt1\n",
    "\"\"\"\n",
    "\n",
    "model = \"TextGNN\"\n",
    "\n",
    "model_params = {}\n",
    "use_edge_weights = False\n",
    "init_type = 'one_hot_init'\n",
    "if model == 'TextGNN':\n",
    "    pred_type = 'softmax'\n",
    "    node_embd_type = 'graphsage'\n",
    "    layer_dim_list = [200, num_labels]\n",
    "    num_layers = len(layer_dim_list)\n",
    "    class_weights = True\n",
    "    dropout = False\n",
    "    s = 'TextGraphSAGE:pred_type={},node_embd_type={},num_layers={},layer_dim_list={},act={},' \\\n",
    "        'dropout={},class_weights={}'.format(\n",
    "        pred_type, node_embd_type, num_layers, \"_\".join([str(i) for i in layer_dim_list]), 'tanh', dropout, class_weights\n",
    "    )\n",
    "    model_params = {\n",
    "        'pred_type': pred_type,\n",
    "        'node_embd':  node_embd_type,\n",
    "        'num_layers': num_layers,\n",
    "        'layer_dims': layer_dim_list,\n",
    "        'act': 'tanh',\n",
    "        'class_weights': class_weights,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "print(\"{}: {}\\n\".format(model, model_params))\n",
    "\n",
    "\"\"\"\n",
    "Sampling\n",
    "\"\"\"\n",
    "word_window_size = 10\n",
    "validation_window_size = 10\n",
    "\n",
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "validation_metric = \"accuracy\"  # Alternatively, \"f1_weighted\" or \"loss\"\n",
    "\n",
    "use_best_val_model_for_inference = True\n",
    "\n",
    "\"\"\"\n",
    "Evaluation.\n",
    "\"\"\"\n",
    "tvt_ratio = [0.8, 0.1, 0.1]\n",
    "tvt_list = [\"train\", \"test\", \"val\"]\n",
    "\n",
    "\"\"\"\n",
    "Optimization.\n",
    "\"\"\"\n",
    "logs = False\n",
    "lr = 0.001\n",
    "\n",
    "device = 'cuda:{}'.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu'\n",
    "\n",
    "num_epochs = 2 if debug else 400\n",
    "\n",
    "\"\"\"\n",
    "Other info.\n",
    "\"\"\"\n",
    "# Assuming get_user() and get_host() are function calls that need to be defined or imported\n",
    "user = get_user()\n",
    "hostname = get_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number params:  5212404\n",
      "Test...experiment  1\n",
      "{'accuracy': 0.7335,\n",
      " 'f1_macro': 0.5417735238436896,\n",
      " 'f1_micro': 0.7335,\n",
      " 'f1_weighted': 0.7302325299057705,\n",
      " 'precision_macro': 0.5471763390681791,\n",
      " 'precision_micro': 0.7335,\n",
      " 'precision_weighted': 0.7389061471583507,\n",
      " 'recall_macro': 0.5672478591335415,\n",
      " 'recall_micro': 0.7335,\n",
      " 'recall_weighted': 0.7335}\n",
      "Number params:  5212404\n",
      "Test...experiment  2\n",
      "{'accuracy': 0.7365,\n",
      " 'f1_macro': 0.47897767657709145,\n",
      " 'f1_micro': 0.7365,\n",
      " 'f1_weighted': 0.6887778509339222,\n",
      " 'precision_macro': 0.600049244491693,\n",
      " 'precision_micro': 0.7365,\n",
      " 'precision_weighted': 0.7063421654396634,\n",
      " 'recall_macro': 0.44994194208189886,\n",
      " 'recall_micro': 0.7365,\n",
      " 'recall_weighted': 0.7365}\n",
      "Number params:  5212404\n",
      "Test...experiment  3\n",
      "{'accuracy': 0.7235,\n",
      " 'f1_macro': 0.4726740510532783,\n",
      " 'f1_micro': 0.7235,\n",
      " 'f1_weighted': 0.6706321914311322,\n",
      " 'precision_macro': 0.6029520359105459,\n",
      " 'precision_micro': 0.7235,\n",
      " 'precision_weighted': 0.7133096712524563,\n",
      " 'recall_macro': 0.45450266372448006,\n",
      " 'recall_micro': 0.7235,\n",
      " 'recall_weighted': 0.7235}\n",
      "Number params:  5212404\n",
      "Test...experiment  4\n",
      "{'accuracy': 0.7515,\n",
      " 'f1_macro': 0.544318940669958,\n",
      " 'f1_micro': 0.7515,\n",
      " 'f1_weighted': 0.7417580160124135,\n",
      " 'precision_macro': 0.5596713271043889,\n",
      " 'precision_micro': 0.7515,\n",
      " 'precision_weighted': 0.7494737968109157,\n",
      " 'recall_macro': 0.5678189719734403,\n",
      " 'recall_micro': 0.7515,\n",
      " 'recall_weighted': 0.7515}\n",
      "Number params:  5212404\n",
      "Test...experiment  5\n",
      "{'accuracy': 0.7345,\n",
      " 'f1_macro': 0.5473562818667363,\n",
      " 'f1_micro': 0.7345,\n",
      " 'f1_weighted': 0.7350717869376625,\n",
      " 'precision_macro': 0.5560410280591104,\n",
      " 'precision_micro': 0.7345,\n",
      " 'precision_weighted': 0.7595393778526859,\n",
      " 'recall_macro': 0.5966352495013774,\n",
      " 'recall_micro': 0.7345,\n",
      " 'recall_weighted': 0.7345}\n",
      "accuracy: Mean=0.735900, Std=0.009002\n",
      "f1_weighted: Mean=0.713294, Std=0.028258\n",
      "f1_macro: Mean=0.517020, Std=0.033740\n",
      "f1_micro: Mean=0.735900, Std=0.009002\n",
      "precision_weighted: Mean=0.733514, Std=0.020531\n",
      "precision_macro: Mean=0.573178, Std=0.023498\n",
      "precision_micro: Mean=0.735900, Std=0.009002\n",
      "recall_weighted: Mean=0.735900, Std=0.009002\n",
      "recall_macro: Mean=0.527229, Std=0.062175\n",
      "recall_micro: Mean=0.735900, Std=0.009002\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "dataset = 'twitter_asian_prejudice'\n",
    "num_experiments = 5\n",
    "random_seeds = [33, 15, 86, 109, 78]\n",
    "model = \"TextGNN\"\n",
    "all_experiment_results = []\n",
    "for exp in range(num_experiments):\n",
    "    random_seed = random_seeds[exp]\n",
    "    saver = Saver()\n",
    "    train_data, val_data, test_data, raw_doc_list = load_data()\n",
    "    \n",
    "    saved_model, model = train(train_data, val_data, saver, False)\n",
    "    with torch.no_grad():\n",
    "        test_loss_model, preds_model = model(train_data.get_pyg_graph(device=device), test_data)\n",
    "    eval_res = eval(preds_model, test_data, True)\n",
    "    y_true = eval_res.pop('y_true')\n",
    "    y_pred = eval_res.pop('y_pred')\n",
    "    print(\"Test...experiment \", exp+1)\n",
    "    pprint(eval_res)\n",
    "    all_experiment_results.append(eval_res)\n",
    "    model = \"TextGNN\"\n",
    "    \n",
    "# Calculate mean and standard deviation across experiments\n",
    "final_metrics = {key: [] for key in all_experiment_results[0]}\n",
    "for results in all_experiment_results:\n",
    "    for key in results:\n",
    "        final_metrics[key].append(results[key])\n",
    "\n",
    "for metric in final_metrics:\n",
    "    values = np.array(final_metrics[metric])\n",
    "    mean = values.mean()\n",
    "    std = values.std()\n",
    "    print(f'{metric}: Mean={mean:.6f}, Std={std:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
